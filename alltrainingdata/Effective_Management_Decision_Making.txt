Effective	Management	Decision Making
An	Introduction Ian	Pownall

Download	free	books	at

Ian Pownall

Effective Management Decision Making
An Introduction

2
Download free eBooks at bookboon.com

Effective Management Decision Making: An Introduction © 2012 Ian Pownall & bookboon.com ISBN 978-87-403-0120-5

3
Download free eBooks at bookboon.com

Effective Management Decision Making

Contents

Contents
	Introduction	 	 1.1	 1.2	 1.3	 1.4	 1.5	 1.7	 	 2.1	 2.2	 2.3	 2.4	 2.5	 2.6	 2.7	 Chapter 1	 Effective Management Decision Making: Introduction	 The Duality of Decision Making?	 Types of Business and Management Decisions	 Who is involved in Decision Making?- The Decision Body	 The three phased model	 Key terms and glossary	 Chapter 2	 Developing rational models with qualitative methods and analysis: Data forecasting	 Simple Averaging Forecasting	 Moving Averages	 Exponential Smoothing Data Forecasting	 Errors, accuracy and confidence	 Causal forecasting (explanatory forecasting)	 Non-linear Forecasting and multiple regression– Curve fitting	 9 10 10 10 14 17 26 27 27 29 29 30 32 35 41 43 60

1.6	Summary	

In the past four years we have drilled

81,000 km
That’s more than twice around the world.
Who are we?
We are the world’s leading oilfield services company. Working globally—often in remote and challenging locations—we invent, design, engineer, manufacture, apply, and maintain technology to help customers find and produce oil and gas safely.

Who are we looking for?
We offer countless opportunities in the following domains: n Engineering, Research, and Operations n Geoscience and Petrotechnical n Commercial and Business If you are a self-motivated graduate looking for a dynamic career, apply to join our team.

What will you be?

careers.slb.com

4
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making 2.8	 2.9	 2.11	 	 3.1	 3.2	 3.3	 3.4	 3.5	 3.6	 3.8	 3.9	 	 4.1	 4.2	 4.3	 4.4	 Multiple regression and partial regression analysis	 Complete worked non linear forecasting example with seasonality	 Key Terms and glossary	 Chapter 3	 Developing rational models with quantitative methods and analysis: probabilities	 The Decision tree – a map of the solution set	 Decision Analysis	 More on probabilities: Expected Monetary Values (EMVs)	 Revising problem information – Bayes Theorem	 The Value of Sample and Perfect Information 	 Key Terms and glossary	 Chapter closing question	 Chapter 4	 Developing rational models with quantitative methods and analysis:  Distribution Functions and Queuing Theory	 The mathematical function: discrete and continuous variables	 The discrete distribution – the binomial function	 Extending binomial number sequences	 129 129 132 137 71 73 83 83 100 100 102 105 112 117 122 125 126 127 129

Contents

2.10	Summary	

3.7	Summary	

5
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making 4.5	 4.6	 4.7	 4.8	 4.9	 4.10	 4.11	 4.12	 4.13	 4.14	 4.15	 4.16	 4.18	 	 The Poisson sequence	 Queuing Theory	 Examples of poisson problems	 More examples of poisson problems:	 Queuing Theory – Modelling reality	 Defining the queuing characteristics	 Example of M/M/1 system	 Queuing cost analysis of M/M/1 models	 M/M/k queues	 The economic analysis of queues	 Other queuing systems and different waiting line structures	 Example of an arbitrary service time queue	 Key terms and glossary	 Chapter 5	 Irrationality in Management Decision Making	 5.2	 5.3	 5.4	 The Monte Carlo Simulation	 Systems thinking about decisions	 Checkland’s Soft System Methodology (SSM) – Mode 1	 138 143 150 151 152 155 157 159 160 162 164 165 167 167 176 176 179 182 184

Contents

4.17	Summary	

5.1	 Developing holistic models with qualitative methods of decision analysis:

Find and follow us: http://twitter.com/bioradlscareers www.linkedin.com/groupsDirectory, search for Bio-Rad Life Sciences Careers http://bio-radlifesciencescareersblog.blogspot.com

John Randall, PhD Senior Marketing Manager, Bio-Plex Business Unit

Bio-Rad is a longtime leader in the life science research industry and has been voted one of the Best Places to Work by our employees in the San Francisco Bay Area. Bring out your best in one of our many positions in research and development, sales, marketing, operations, and software development. Opportunities await — share your passion at Bio-Rad!

www.bio-rad.com/careers

6
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making 5.5	Summary	 5.6	 	 6.1	 6.2	 6.3	 6.5	 	 7.1	 7.2	 7.3	 7.4	 7.5	 7.6	 Key terms and glossary	 Chapter 6	 The individual in decision making: Heuristics in Management Decision Making:	 More on the cognitive view – schematas and heuristics	 More on heuristics	 Key terms and glossary	 Chapter 7	 Group decision making – is it really better?	 Group communication	 Convergent thinking emergence in groups- The Abilene Paradox	 Convergent thinking emergence in groups- The Groupthink phenomenon	 Futures forecasting and decision making	 195 196 198 198 206 209 214 214 215 219 220 221 223 226 230 230 231

Contents

6.4	Summary	

The role of groups in decision making and long term decision making methods and analysis	  215

7.7	Summary	 7.8	Glossary	 	References	

678'<)25<2850$67(5©6'(*5((
&KDOPHUV 8QLYHUVLW\ RI 7HFKQRORJ\ FRQGXFWV UHVHDUFK DQG HGXFDWLRQ LQ HQJLQHHU LQJ DQG QDWXUDO VFLHQFHV DUFKLWHFWXUH WHFKQRORJ\UHODWHG PDWKHPDWLFDO VFLHQFHV DQG QDXWLFDO VFLHQFHV %HKLQG DOO WKDW &KDOPHUV DFFRPSOLVKHV WKH DLP SHUVLVWV IRU FRQWULEXWLQJ WR D VXVWDLQDEOH IXWXUH ¤ ERWK QDWLRQDOO\ DQG JOREDOO\ 9LVLW XV RQ &KDOPHUVVH RU 1H[W 6WRS &KDOPHUV RQ IDFHERRN

7
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Contents

Acknowledgements Many thanks to Christine for her tireless proof reading of the academic register and use of the English language in this text.

8
Download free eBooks at bookboon.com

Effective Management Decision Making

Introduction

Introduction
This short text is the output of a desire to produce a helpful additional source for my students and from that, perhaps be of use to other similar students and managers of this subject area. After several years of working with classes on Management Decision Making, the need for a short and focused integrative text was clear to me. There are many excellent texts on both the qualitative and quantitative aspects of decision making, but few which address both. Where feasible, however, I have made significant reference to recommended texts on these areas throughout the chapters, although this duality problem was the primary reason for this text. Chapter 1 opens with a short narrative on this issue. A second reason for the text, was to try to produce a relatively short essay that would convey important and relevant knowledge to its readers, in a language and manner that would make it accessible to those students who were less comfortable with mathematics. At times therefore, the language and writing style is deliberately parochial. One fundamental objective when writing these materials was not to seek to replace either a quantitative text on Management Science or a qualitative text on Judgement and Systems Analysis – but to offer a helpful guide around these topics. This text therefore has a simple structure and focuses upon those areas of personal interest and which have formed the core of my taught classes; indeed most chapter materials are derived from my lecture notes suitably expanded and with further reference to several key texts. There is therefore no attempt to offer an inclusive coverage of the range of materials associated with the general topic of Management Decision Making. This text is intended to complement the student’s wider reading and should be used in conjunction with more developed materials. The key areas focused upon in this introductory material are then: The nature of decision making and modelling, data forecasting, probabilities and probability functions in decision making, systems analysis of decision making (in particular Soft Systems Methodology), individuals and cognition in decision making, the group in decision making and finally consideration to non quantitative long term forecasting for decision making. It is my hope, that this text may offer help to those students of this topic who maybe struggling with a fundamental understanding of issues and if this is achieved once per reader, then the text has served a good purpose. This text covers seven key topic areas which are broadly referred to in the relevant chapter headings: 1)	 Introduction and an overview of the breadth of the topic: Modelling decisions (Chapter 1) 2)	 Developing rational models with quantitative methods and analysis: Data Forecasting (Chapter 2) 3)	 Developing rational models with quantitative methods and analysis: Probabilities (Chapter 3) 4)	 Developing rational models with quantitative methods and analysis: Probability distribution and queuing theory (Chapter 4) 5)	 Developing holistic models with qualitative methods and analysis: Soft Systems Methodologies (Mode 1 and Mode 2) (Chapter 5) 6)	 The role of the individual in decision making: Heuristics (Chapter 6) 7)	 The role of groups in decision making (Chapter 7) Key activities and exercises are embedded within the chapters to enhance your learning and, as appropriate, key skills development. The chapters should preferentially be read in order, although they will standalone if you wish to dip into the materials.

9
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 1:

Chapter 1
1.1	 Effective Management Decision Making: Introduction
Management decision making is a seemingly simple title for a text or for study as a Business Management student or manager. After all, we all make decisions every moment of our lives, from the trivial topics of deciding ‘what shall we eat tonight?’ to more difficult decisions about ‘where shall I study for my degree?’. We tend to believe we make such decisions in an entirely rational and logical manner and after considering the varying advantages and disadvantages of those outcomes. Indeed, selecting options from a range of actions is at the heart of decision making and is probably one of the defining characteristics of being an effective manager. However, if you start to question the motivations and reasons for decisions taken, you begin to realise that trying to understand why a given action was chosen over another and whether it was a ‘good’ or ‘bad’ decision is actually a complex and difficult task. This questioning highlights the inherent difficulties in identifying clear and agreed criteria against which an ‘effective’ decision can be judged independently. If you are a student, think about the decision you made about in choosing which university or college to study with. If you are a manager then consider your chosen career path - what criteria did you use to make this decision? Why did you choose these criteria? Did you evaluate the advantages and disadvantages of all those criteria and their impact upon all possible choices of universities (or careers)? How important was the influence of your family or friends? Did you question any assumptions about those universities (or career paths)? And so forth… You soon realise that despite the fact decisions are made by individuals and groups regularly, understanding them and anticipating them is not an easy task. This text aims to give you an understanding of the reflective skills necessary for effective decision making, and also an insight into how to better manage those with whom you work and live, in both a qualitative context (trying better to understand people) and a quantitative context (trying better to work with data and numbers). It is based upon several years of devising and delivering a Decision Making course for final year students in varying Business Degree programmes and in trying to grapple with the inherent duality of the topic for students.

1.2	

The Duality of Decision Making?

It should have become clear from reading the first page that the topic of decision making has two distinctive foundations – a quantitative and a qualitative focus. This is indicative of a relatively young management discipline and one that has deep roots in operations research and statistical analyses (Harrison, 1999). This is also reflected in the range of texts written on this topic but which generally are either of a quantitative or qualitative nature. A few authors have tried to integrate and popularise the two foundations, but these materials are not easily accessible. Some of the better known teaching texts on this integration are noted as: •	 Jennings D. and Wattam S.,(1998), “Decision Making: An integrated Approach’, Prentice-Hall •	 Teale, M., Dispenza, V., Flynn J. and Currie D.,(2002),’Management Decision Making: Towards an Integrative Approach’, FT-Prentice Hall.

10
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 1:

One of the ways in which management decision making has been analysed, is to consider process concerns. For example, Harrison (1999) and Olson (2001) outline several perspectives on management decision making which reflect different priorities within the processes of making decisions (as an individual judgement). These are: •	 An integrative perspective (or rational normative) which argues that an effective decision is constructed from the successful performance of each step in the overall process (a belief which is used to frame qualitative discussions in this text) •	 An interdisciplinary perspective which looks to both behavioural and quantitative disciplines to understand and explain decision making (this is also a focus in this text) •	 An interlocking perspective which recognises that the engagement of one perspective to view decision making (such as a cognitive focus where individuals have bounded rationality (see Chapter 5)) necessarily limits the use of other perspectives (such as quantitative methods) •	 An interrelational perspective (or a cause-effect view), where decisions taken are interrelated across organisational events, in pursuit of an organisational goal. As an example, we note shortly in Box 1.2, the interrelational decision making of Nokia. These perspectives are also reflected in the large variety of texts and materials that are available. For example, a quick review of Amazon’s inventory of existing (and future unpublished) titles on ‘Management Decision Making’ in the ‘Management’ category notes the following publications themes:

Figure 1.1: A count of the themed titles in Management Decision Making Source: Amazon (2011)

11
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 1:

If these themes are then grouped into their quantitative and qualitative foci, the following summary figure is generated:

Figure 1.2: Volume of titles that focus upon either qualitative or quantitative management decision making Source: Amazon (2011)

The qualitative focus in Decision Making then dominates the focus of published text materials, despite the original quantitative roots to the discipline. However, from a systems perspective (see Checkland, 1981: 1990 for example), which seeks to view the holistic nature of a problem (the problem domain), we know the quantitative focus is also important and should not be ignored in the development of management skills in decision making– so that a problem can be fully understood too. Therefore this text also considers (some of) this breadth to present a fuller picture to the reader. Fuller and Mansour (2003) citing Lane et al (1993) present an overview of this breadth and outline 13 distinctive quantitative decision making methods that have evolved in the operations management and research literature. These are: 1)	 Decision Analysis* 2)	 Linear programming models 3)	 Game Theory models 4)	 Simulation models* 5)	 Network optimization models 6)	 Project management models 7)	 Inventory models 8)	 Queuing models* 9)	 Dynamic programming 10)	 Integer programme 11)	 Non linear programming models* 12)	 Forecasting models* 13)	 Markov Decision models* The (*) denotes there is a focus for these methods, in this text.

12
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 1:

A difficulty for students when faced with these methods, is being easily able to relate them to a business context and to view them as management tools (rather than in the abstract mathematical sense (Fuller & Mansour, 2003)). Certainly this reflects personal experience, having observed this with many students over the years. Part of this difficulty, is that whereas these methods are solution oriented with specific techniques and skills to deploy, rarely are actual business problems so neatly prescribed and packaged, especially in smaller organisations. Effective decision making is also therefore about being able to adapt and reflect upon the process and tools chosen to aid the decision making process. A student or manager who is able to adapt a modelling method to address a management decision, is exhibiting problem solving, judgement and foresight skills. This does of course not necessarily mean that the solution is or will be correct, but demonstrates that the manager is not a slave to a dogmatic use of a given method. Used in this way, these methods may also help improve the clarity of a problem and may also lead to further qualitative analysis prior to an effective decision being reached. Harrison (1999) argues that the scope of decision making begins with the individual (chapter 6), which follows from the preceding discussion too. Individuals can then work together in groups and/or teams, depending upon the context of the organisation and its micro and macro environment. In this text we focus upon both the individual (chapter 6) and the group dynamic of decision making in particular (chapter 7). In the context of the former, we will discuss the inherent bias and limits of individuals involved in decision making, whilst in the latter we discuss how a group dynamic can also strongly influence the independence of both the decision making process and its outcomes. Organisational influences are apparent in both individual and group decision making activities, as they evolve and change in an interdependent fashion with them. As Cohen et al (1972) asserts the convergence of necessary resources, individuals and information to resolve a problem or choose between process outcomes is rarely optimal in organisations - this is described by their Garbage Can model. In this model –typical of organised anarchies - the availability of solutions, their selection and implementation to resolve problems increasingly reflects the vagaries of the availability of resources and their analysis. Complexity and ambiguity increase to an extent that it can result in the breakdown of a guiding and structuring rationality and decisions are taken which can, upon fuller and richer reflection, be seen to be very poor (Langley et al,1995). As an example, Hollinger et al (2007) discuss how the newly appointed CEO of the Alcatel-Lucent group merger, who, through her absence at subsequent important decision making meetings, resulted in actions being agreed which further exasperated the recent merger’s corporate position and shareholder belief in the weak value of that entity. Decisions were not optimal in their corporate context, undermining the strength of the organisation and ultimately of the CEO (see Box 1.1)

13
Download free eBooks at bookboon.com

Effective Management Decision Making Box 1.1 : Alacatel –Lucent and anarchic decision making(2006-2008)

Chapter 1:

An expected appearance by the newly appointed American CEO of the merged Alcatel-Lucent company – Ms Pat Russo - , was marred by cat calls, whistles and foghorns from angry French employees, upset at job cuts organised by the outgoing CEO at the Alcatel-Lucent corporate meeting in June 2007. The response by the work force towards the CEO, shaped the CEOs subsequent engagement with corporate dialogue and communications within the organisation and especially in employment relationships. This is very much contra to the usual French cultural context of making decisions transparent through dialogue within different levels of the organisation. In addition, whilst approval for difficult job cuts within the organisation was given by Pat, she was also critiqued for not being visible and forceful enough to push the job cuts through and show conviction with her strategic focus for the organisation. Solutions were chosen and implemented to address and deliver upon the corporate merger aims, but arguably without the necessary individuals being involved at the appropriate time. This has negatively then affected confidence in the organisation and the merger. Perhaps more worryingly, the foundation for the merger as a solution to address problems of increasing competitive strength in China and rising to the increased challenges of large European firms such as Ericsson – overlooked other non-addressed technological weaknesses (especially in mobile infrastructure). Anarchic decision making as described by the Garbage Can model is therefore seemingly apparent in the manner of engagement of the CEO, the omissions in the analysis of the competitive positioning of the merged organisation and that internal departments in the merged organisation were also found to be bidding against themselves in the same contract tenders. Furthermore, part of the difficulty in making and communicating decisions within the merged organisation, has been that the senior management team and their decision context, has had to reflect an apparent ‘merger of equals’ despite the reported observations, that Alcatel defacto acquired Lucent. Both Pat Russo and the (non executive Chairman) Serge Tchuruk subsequently resigned in late 2008, following further profit warnings and a disappointing corporate performance of a 60% fall in the value of Alcatel-Lucent stock in 2008. Sources: Adapted from Aston (2008): Hollinger et al (2007)

1.3	

Types of Business and Management Decisions

Organisational decisions can have different characteristics, which shape how they can be understood and resolved by managers. Structured Decisions – are decisions where the aim is clear (i.e. the purpose of the decision to be taken is unambiguous, easily defined and understood). Structured decisions therefore follow a series of logical and rational steps in a clear progressive order. This is often labelled as a normative method of decision making (Jennings & Wattam, 1998) or a Rational model (or RAT model) (Lee & Cummins, 2004). For example, an organisation decides it needs to know more about Company X. To compile this information, it may decide to consult newspaper archives, or conduct market research. In other words, it deploys known and tested methods to progress the problem so that the organisational decision makers are then able to make a decision regarding preferred outcomes. Equally, as a manager of an organisation, there might be a need to schedule the work rota for the next 6 months to ensure sufficient resources are allocated to different jobs. In such cases, information will be available, on hand and manageable. These are structured decisions where the aim is clear and there are varying and well understood methods open to address the aim.

14
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 1:

Unstructured Decisions – by contrast, for individuals and organisations, are decisions which are unclear, ambiguous and poorly understood by participants. It may be very difficult to compare outcomes and their relevant benefit for individuals, the value of required information to resolve the problem or opportunity, may be difficult to assess. For example, Nintendo would have faced many uncertainties in their launch of the Wii console, which would have included high levels of ambiguity about the key market focus and its social impact. We know latterly of course, that this was in fact significant for its success as it broadened the socio-demographic base of gamers significantly. Programmable decisions are types of structured decisions which follow clear, delineated steps and procedures. They can be repetitive and routine (Harrison, 1999). Similarly, a non –programmed decision for an organisation can be said to occur where there are no existing procedures or practices in place to resolve the problem or address the opportunity. Sufficient reoccurrence of non programmed decision outcomes, can of course then generate a programmed organisational response to given situational stimuli. For example, when Honda first entered the US marketplace in 1958 with their 4 motorcycle types (which differed primarily by engine size), the different and changed uses of their vehicles by American buyers – who had long open roads which could be travelled at high speed, was in marked contrast to the congested Japanese and Far Eastern cities and road network. This created a problem with no programmed response by the organisation. The larger engine motorcycles had been the focus of sales attention by Honda (given the presumed market for this type of vehicle) but their extensive and unpredicted use in the American marketplace resulted in unforeseen mechanical failings. With no existing policies or practices to address the problem, new practices were developed (in this situation – shipping the faulty engines back to Japan and using the smaller engine motorbikes) (Pascale, 1988). Latterly, the smaller engined motorbikes proved to be a great unexpected commercial success and laid the foundation for Honda’s subsequent market dominance. Market evaluation and product development emerged as stronger factors of the decision to enter new markets.

Linköping University – innovative, highly ranked, European
Interested in Engineering and its various branches? Kickstart your career with an English-taught master’s degree.

Click here!

15
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Chapter 1:

Decisions also occur at different levels in an organisation – and those decisions can also be of different types. For example, strategic decisions are generally concerned with the most appropriate use of organisational resources for a given preferred competitive goal. They usually have some form of structure (i.e. the organisation may know what resources it has or can access through an internal resource audit), but also will carry uncertainties (such as the assumptions about how the buying behaviour of the customer might develop over time). Changes in these assumptions might then change what is produced, where, at what price and for whom, for the organisation. Tactical decisions are the actions which follow (and are required to be achieved) the strategic decision. We might say that whilst a strategic decision determines what the organisational purpose is or could be, the tactical decisions follow in determining what needs to be done to achieve this goal. For example, an organisation might decide strategically, that entering the Indian market with an existing product is appropriate, whilst the tactical decision might be to decide between an export focused approach or local (in country) manufacture by building a new factory or finding local production partners. Operational decisions finally, are short term and responsive actions. For example we could consider the hierarchy of decision making here as (and which continues from the preceding discussion): •	 Strategic Decision (or a Corporate Strategy) – such as for example, the decision to enter the Indian market to support organisational sales growth of 5% per annum •	 Tactical Decision (or a Business Strategy) – such as for example, to decide between export led market expansion or locally producing the product •	 Operational Decision – such as for example the decision to hire more expatriates (or local staff) to deliver and manage either the export or local production operations. Box 1.2 presents a summary of recent strategic decision making taken at Nokia to illustrate this hierarchy of organisational decision making and levels of management decision making. Box 1.2: Taking big decisions at Nokia Stephen Elop, who joined Nokia as President and CEO in September 2010 from the senior staff of Microsoft – is faced with a significant amount of uncertainty and ambiguity in determining the future strategy of the company. With a rapidly declining market share in developed markets (where Google’s Android and Apples iPhone have heralded the invasion of SmartPhones) and a weakening competitive position in emerging markets (such as India), the decisions he is taking are significant for the survival of the organisation. In February 2011, he issued his famous ‘burning platform’ memo and canvassed 3 questions to all Nokia employees: The ‘burning memo’ document presented a clear and simple analogy for Nokia employees: “There is a pertinent story about a man who was working on an oil platform in the North Sea. He woke up one night from a loud explosion, which suddenly set his entire oil platform on fire. In mere moments, he was surrounded by flames. Through the smoke and heat, he barely made his way out of the chaos to the platform’s edge. When he looked down over the edge, all he could see were the dark, cold, foreboding Atlantic waters. “As the fire approached him, the man had mere seconds to react. He could stand on the platform, and inevitably be consumed by the burning flames. Or, he could plunge 30 meters in to the freezing waters. The man was standing upon a ‘burning platform’, and he needed to make a choice.

16
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 1:

“He decided to jump. It was unexpected. In ordinary circumstances, the man would never consider plunging into icy waters. But these were not ordinary times - his platform was on fire. The man survived the fall and the waters. After he was rescued, he noted that a ‘burning platform’ caused a radical change in his behaviour. “ (Hill, 2011:14). The three questions posed to Nokia employees were: “What do you think I need to change?” “What do you think I need not or should not change?” “What are you afraid I’m going to miss?” Mr.Elop subsequently announced a strategic decision to create a joint venture with Microsoft and adopt their Windows operating system to power their new smartphones.. It offers tactically both leveraged branding for Microsoft and Nokia in both developed and emerging markets but moreover still requires Nokia to make the operational decision of maintaining their support for their inhouse Symbian and Meego platforms, to finance the transition of the company. A large challenge given the extensive history of organic and in house technological development and the corporate culture this has developed. You might wish to consider these questions for discussion and further research: 1)	 What were the decisions taken by Stephen and what were the defining features of those decisions? 2)	 What was the process Stephen used and why, to take those decisions? 3)	 What are the risks associated with these decisions?

1.4	

Who is involved in Decision Making?- The Decision Body

It is a common belief, when trying to understand decision making, to view it as equivalent to problem solving (Harrison, 1999). However, it must be remembered that decisions are often taken without a clear problem being resolved or driving the decision making process. For example, whilst Stephen Olap (see Box 1.2) may have been trying to solve a strategic problem with Nokia (a declining competitive market share in that instance), the mechanisms and actions (tactical and operational decisions) that resulted from that decision, were not necessarily being driven by that problem (as other choices could have been made). Clearly decision making and problem solving are related, but they are not interchangeable terms. Decision making occurs when a judgement must be made between the merits and demerits of different choice processes, whereas problem solving generates the choice processes in the first place (Harrison, 1999). In presenting an overview of decision making, we can also then begin to consider the antecedent environmental factors that shape decision making.

17
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 1:

Teale et al (2003) propose that decisions, of the type discussed, are made by a decision body in organisations. These are the individuals, collective groupings and other stakeholder entities that actively shape the decision making process. Some care should be taken regarding who or what may constitute a valid stakeholder for an organisational decision. Whilst Freeman (1984) originally identified organisational stakeholders as all those who are affected by an organisational decision, this latterly has come to be viewed as too large to be a useful (or a managerially practical) definition. Mitchell et al (1997) and Escoubes (1999) offer more focused insights on who might be a stakeholder and therefore constitute an active member of the organisational decision body. We can start this deepening of our analysis by recognising that decision making implicitly exhibits the power to select from solutions, often regardless of other actor preferences and influences. Mitchell et al (1997) then propose that stakeholders can be identified through three interdependent features of influence: 1)	 Their level of power and authority – for example how easy is it for a stakeholder to influence a firm’s decisions (consider the different likely power relations for a firm when working with a single customer vs a regulatory authority or as reported by Pickard (2007) the conflict between resource constrained local authorities and the legal team of property developers who are by comparison much better resourced and able to exert significant influence upon the outcomes of for example, planning applications)

18
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Chapter 1:

2)	 Their level of legitimacy – what is the social and moral authority of the stakeholder when using its influence to shape a firm’s decisions (consider a pressure group lobbying on behalf of homeless people vs Rotarian society lobbying on behalf of improved parking spaces or as reported by Wolf (2004) on geopolitical changes in the legitimacy of decision making and the controversial changes to US foreign policy especially under the Presidency of George Bush (Jnr), viewed by some observers as resulting in a decline in the legitimacy of the US to effect global geopolitical changes. Its decline heralds from a perceived weakening of international law, a decline in the acceptance of consensual decision making and moderation and its weakened position with regards to an ideology of preserving the peace). 3)	 Their level of urgency – what is the stakeholder’s level of immediate implication in the firm’s activities (consider a local archeaology groups’ protests about developing an historic property vs developer’s desires for access to the land or as reported by Bing & Dyer (2008), the changing economic face of China and growing wealth is helping both official and unofficial local representation groups to challenge the development of nuclear power and energy stations). A stakeholder which combines all three attributes can be said to have a high level of saliency for the decision body. However, it is important to also note the dynamic nature of the decision body. It is not a static or passive collection of individual(s) and/ or group(s), but a body that changes and evolves through new knowledge of the problem or decision to be made, or through the problem itself changing. Escoubes (1999) succinctly reflects on this by also recommending that appropriate stakeholders for the decision body can be identified by: 1)	 A regular analysis of who stakeholders are and might be (for example monitoring market trends, new technologies and their development, changes to the regulatory environment of the organisation – all of which might be typical of the triggers which change the relevance of a given stakeholder through the Mitchell et al (1997) criteria discussed earlier). 2)	 Selecting those stakeholders critical to the organisation and problem at hand – which stakeholders evidence a high level of saliency for the decision body. 3)	 Consulting with these stakeholders – to identify their needs/wants 4)	 Assessing how compatible they are with the firm’s preferred decision outcome 5)	 Establishing appropriate systems to meet those needs which fit with organisational and stakeholder needs This focus however only includes the human input. We cannot neglect the non-human input in the decision body, which may constitute the relevant data, information and knowledge. The decision context is therefore the environment within which the decision body acts. It captures the situational context, pressures and expectancies that have shaped the decision body and the processes and forms of outcomes. To illustrate the competing tension and issues in management decision making with multiple stakeholders, Activity 1.1 applies a simple version of the Mitchell & Escoubes framework: Activity 1.1: Consider the following scenario: An international firm is considering the most acceptable method of introducing, building and managing a new windfarm site, on an elevated area, to the north of your town and which would be visible from most parts of that town. Amongst the range of possible stakeholder’s which would constitute the decision body, you have identified the following as potentially important in the decision making process associated with the windfarm.

19
Download free eBooks at bookboon.com

Effective Management Decision Making •	 Stakeholder 1: Local residents pressure group •	 Stakeholder 2: Local Authority

Chapter 1:

The decision context includes the following situational factors that might be deemed appropriate evaluative features: Nature = The alignment of the windfarm to the purpose of the organisational stakeholder and its core interests in this firm’s decision Identity = The extent to which the stakeholder is associated with the culture and context of the area in which the decision is to be taken (spatially) Scope = The extent of the breadth of interests the stakeholder has in such firm decisions Now – working with a partner, consider the cross impact of each situational factor with the power, urgency and legitimacy of each stakeholder as you perceive them. Score each criteria against each feature to determine the most important stakeholder in this decision making activity. Use the following scale to help your decision making: •	 0-4 – no clear influence on the decision making of the firm •	 5 – influence (but neither strong / nor weak) •	 6-10 - a clear influence on the decision making of the firm
Stakeholder criteria NATURE POWER LEGITIMACY URGENCY Totals Stakeholder feature IDENTITY SCOPE

Q1) What does this activity and your findings tell you about management decision making here? Q2) What features and factors would add a temporal focus to the decision making here?

Models of Decision Making
The normative model of decision making (also described as the rational model (RAT)), offers a starting point to try to understand the process of decision making through the decision body and context. It remains an important foundation for a variety of social science and humanistic disciplines including leadership studies (Vroom & Yetton, 1973), economics and rational choice theory (Levi, 1997: Scott, 2000). This model assumes that all relevant and pertinent information is available to the decision body in a supportive (and unconstrained context), to allow optimal decisions to be taken, through a consideration of all potential outcomes (which themselves can be known and understood in advance) (Lee and Cummins, 2004). In the case of economics for example, this may be a comparative cost –benefit analysis. Key stages in this approach might then be described as:

20
Download free eBooks at bookboon.com

Effective Management Decision Making •	 Define the problem (what is it that needs to be determined?) •	 Determine the evaluative criteria (efficiency? efficacy? morality?)

Chapter 1:

•	 Identify all possible solutions (the range of actions which result in the achievement of the problem aim) •	 Judge the achievement of the outcomes of these solutions against preferred criteria and problem aim (which solution works best by the relevant criteria) •	 Choose the optimal solution Figure 1.4: Key RAT elements of decision making Such models can though be as simple as the testing of outcomes against a preferred goal (i.e. consider again the question of which travel option to take to get to the University or to work, which ensures you are able to be in class before class starts) (Baron, 2004) or can be involved with multiple evaluative criteria being used. As Baron (2004) further notes decisions are taken to achieve preferred goals according to decision body values. In some cases, they may also be subservient to other decisions taken – and in those cases we can focus upon decision analysis and probability outcomes (see Chapter 3). We can also consider the act of ‘non decision making’ that is often exhibited by individuals and organisations, as part of this text of decision making. Non Decision making as defined by Lukes (1974) refers to the control of the agenda for discussion regarding an issue or problem. It is a form of power and decision making influence that denies discussion to participants who are unaware of their decision making constraints.

21
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Chapter 1:

Whilst appealing as a model of decision making which articulates clearly delineated stages and direction, the RAT model does raises significant concerns - particularly regarding the nature of rationality for individuals. There is the assumption that there is a single best outcome, that the decision body is able to make a decision and select that optimal outcome when in practice, the availability of necessary information and understanding of outcomes is very difficult to gather and/ or determine. A further assumption is that the decision body possess the necessary judgemental and interpretative skills to be able to analyse and use available data. Ahmed & Shepherd (2010) give the illustration that creative managers, who are able to generate novel solutions to problems, do so through a combination of situational pressures (the problem to be addressed), experience and skills. Therefore the rational model seems to be an inadequate explanation of how the decision body can take decisions because of their varying contexts and individual interpretations. Before considering the complementary discussion on non-normative / non rational models further – it is helpful to illustrate the differences between alternative decision bodies through an example. Pirsig (1974) wrote an influential and well known (novel) text entitled ‘Zen and the Art of Motorcyle Maintenance’, which gives an account of the author and his dual philosopher identitys’ cognitive journey across Northern America. Amongst many issues discussed, the author identifies different forms of rationality in individuals (in this context between travelling companions on the cross country journey (John and the author)). Consider the two quotes below: 1)	 “This old engine has a nickels-and-dimes sound to it. As if there were a lot of loose change flying around inside it. Sounds awful, but its just normal valve clatter. Once you get used to the sound and learn to expect it, you automatically hear any difference. If you don’t hear any, that’s good. I tried to get John interested in that sound once, but it was hopeless. All he heard was noise and all he saw was the machine and me with greasy tools in my hands, nothing else. That didn’t work. He didn’t really see what was going on and was not interested to find out. He isn’t so interested in what things mean as in what they are.” (Pirsig, 1974: 59) 2)	 “When he brought his motorcycle over I got my wrenches out but noticed that no amount of tightening would stop the slippage [of the handlebars], because the ends of the collars were pinched shut. “You’re going to have to shim those out” I said “What’s a shim?” “It’s a flat thin strip of metal. You just slip it around the handlebar under the collar and it will open the collar up, so it can be tightened again” “Oh” he said. He was getting interested. ”Good. Where do you buy them?” “I’ve got come right here”, I said gleefully, holding up a can of beer in my hand. He didn’t understand for a moment. Then he said, ”What, the can?” But to my surprise, he didn’t see the cleverness of this at all. In fact he got haughty about the whole thing” (Pirsig, 1974: 60)

22
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 1:

In both quotations, two forms of rationality, or how individuals perceive the values of their environment differently are presented. Pirsig himself described these as classic and romantic views of rationality – where the classic view sees the world (and problems therein) mechanistically whereas the romantic view sees the world (and problems therein) aesthetically. Reconciling such divergent forms of rationality therefore is an aim of understanding decision making. The RAT model is limited and focused upon one form of rationality, whereas its opposite on the decision making spectrum, focuses upon for example human values, emotions and bias (see for example heuristics) (Lee and Cummins, 2004: Tversky and Kahneman, 1974:81 see chapter 6). Heuristic decision making methods are non optimal, but focus instead upon how decisions are made when specifically the decision body lacks depth and detailed information pertaining to the problem at hand. We will consider these in more detail later in Chapter 6. Perhaps most famously, Herbert Simon in 1951 introduced the concept of bounded rationality. When we consider the rationality of individuals, we can understand this either through a normative approach (the structured and process oriented mode discussed earlier) or we can adopt a descriptive approach (from which latterly has emerged the work on heuristics. Rationality in both cases describes the subsequent behaviour of individuals, in different decision contexts, to achieve their preferred goals. Bounded rationality articulates the view that individuals are limited information processors with constrained abilities and access to information. Decisions are therefore by definition, suboptimal, but also will vary between individuals in the same decision context. As a human (and management) topic, this is of great interest – with for example studies on serial and portfolio entrepreneurs and in what ways are their behaviours different from nascent or non entrepreneurs (for a detailed exploration see the work of Carland et al, 1997: Westhead & Wright, 1998: McGrath & MacMillan, 2000). An increased focus upon cognitive theories of decision making is also evident in recent management studies (Rogoff et al, 2004: Mitchel et al ,2007). Let’s explore this issue a little bit more with Activity 1.2. Do you think for example that the decision making cognitive processes are different for entrepreneurs and non entrepreneurs? Mitchell et al (2002:4) defined entrepreneurial cognitions to be: “the knowledge structures that people use to make assessments, judgments or decisions involving opportunity evaluation and venture creation and growth”. Rogof et al (2004) have explored the extent to which entrepreneurs and non entrepreneurs (in their case they focused upon pharmacists in New Jersey) attribute their commercial success (or failure) to factors of their environment (over which they have no control (an actor-observer bias)) or to internal factors of skill and effort (to which they have varying levels of control and failure is therefore externalised (a self serving attribution bias)). They concluded that entrepreneurs were more likely to judge their success as a result of individual efforts and controllable factors on their environment, than non entrepreneurs – although gender variations and the impact of experience was also a notable factor upon the attribution bias of the entrepreneur. A simplified version of their data collection questionnaire method is given in Activity 1.2 below and some sample answer data from MBA cohorts from Muscat and Singapore has been provided for comparative discussion.

23
Download free eBooks at bookboon.com

Effective Management Decision Making Activity 1.2: Entrepreneurial Decision Making

Chapter 1:

The following questions can be answered and scored individually, although it is more interesting and useful to gather collective responses (from your class). As you answer, make a note of your score (per question) so that you can then work out your means for questions which have a self serving attribution bias and those for an actor-observer bias). IN answering the questions – choose from the range of 1-4 so that: 1=Strongly Agree, 2=Agree, 3=Disagree, 4=Strongly Disagree. 1.	 Do you feel individual characteristics contribute to business success? 2.	 Do you feel management issues (e.g. Effective organisation, skills) contribute to business? 3.	 Do you feel financing issues contribute to business success? 4.	 Do you feel marketing activities contribute to business success? 5.	 Do you feel HR issues contribute to business success? 6.	 Do you feel economic conditions contribute to business success? 7.	 Do you feel competition contributes to business success? 8.	 Do you feel regulations contribute to business success? 9.	 Do you feel technology contributes to business success? 10.	Do you feel environmental factors contribute to business success? 11.	Do you feel individual characteristics impede business success? 12.	Do you feel management issues impede business success? 13.	Do you feel financing issues impede to business success? 14.	Do you feel marketing activities impede business success? 15.	Do you feel HR issues impede business success? 16.	Do you feel economic conditions impede business success? 17.	Do you feel competition impedes business success? 18.	Do you feel regulations impede business success? 19.	Do you feel technology impedes business success? 20.	Do you feel environmental factors impede business success? Determine your means for your answers for the following question combinations:•	 An I(Internal Attribution) to success for questions 1,2,4,5,11,13,14 •	 An E(external Attribution) to success for questions 3,6,7,8,910,12,15,16,17,18,19,20 Compare your mean with the collective means for questions noted and then reflect on them with the data given below. Depending upon the E/NE you will be able to compare your mean with the class mean and the extent to which you could attribute your entrepreneurial success/failures. A low score for I suggests you are more likely to attribute your success to factors over which you have control whereas a low E score suggests you are more likely to attribute your success to factors over which you have limited control. Comparative data from two MBA cohorts (25 in each grouping) from Singapore (November 2009) and Muscat (December 2010) –gave the following data, which suggests that Muscat students were slightly more likely to view their environment as having a more significant role in shaping their (potential) entrepreneurial success than Singaporeans. 24
Download free eBooks at bookboon.com

Effective Management Decision Making Singapore: (I) mean = 1.94 and (E) mean = 2.081 Muscat:(I) mean = 1.946 and (E) mean = 1.989

Chapter 1:

We have noted that Ahmed and Shepherd (2010) have also proposed that entrepreneurial creativity requires the confluence of individual skills, sector knowledge and an understanding of a problem and opportunity. The decision context then seems very important for an entrepreneurial decision and actions to be taken. It is also interesting to note from Gillson & Shalley (2004) that individuals who are placed in a team situation with the expectation of taking creative decisions, are able to fulfil this expectation more so than if this expectation was not made. So, the decision body, the decision context and purpose are multilayered concepts which can accentuate different individual and situational factors. The RAT model offers a starting point to begin to understand the processes of decision making but is neither sufficiently holistic for the purposes of this work, nor does it reflect the reality of human decision making. It lacks the breadth of possible modes of decision making that individuals can engage with (Langley et al, 1995). To identify a better starting point – and one which allows for and can integrate more decision making factors, the three phased model can be adopted (see Jennings and Wattam (1998) for example).

26 destinations 4 continents
Bartending is your ticket to the world

GET STARTED

25
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Chapter 1:

1.5	

The three phased model

In moving beyond the RAT model (see figure 1.4), the three phased model (Simon, 1960 cited by Langley et al 1995: Jennings & Wattam, 1998), is comprised of – problem identification (intelligence – identifying issues that require improvement and decisions to be made), solution development (design – inventing, developing and analysing possible courses of action) and solution selection (choice – selecting from the available and presented solutions). Clearly, the latter two parts of this model refer to the choices made by managers – and hence this is the decision activity. The Problem Identification (PI) refers to evaluating the information and knowledge about a problem or opportunity and in doing so seeking to add structure and clarity to the subsequent decision making stages. We also recognise that solution development and solution selection are not going to be the separate cognitive processes they are presented as in the RAT model. It is more practical to recognise that they overlap and can occur as simultaneous processes.
Zd ϯWŚĂƐĞ ,ĞƵƌŝƐƚŝĐƐ ŶĂƌĐŚŝĐĂů ĞĐŝƐŝŽŶ DĂŬŝŶŐ

^ĞƋƵĞŶƚŝĂů ĞĐŝƐŝŽŶ DĂŬŝŶŐ

Figure 1.5 – Moving beyond RAT

In this sense, the phased model rejects any kind of optimally economic outcome (that the best returning decision can be made (in whatever is of value to the decision maker)), but does retain some cognitive structure and order as to how decisions are made and can be evaluated (Langley et al, 1995). Clearly, there are still concerns about to what extent this phased approach is also valid cognitively (for example, where an organisation may lack a clear objective that informs the preferred range of solutions from which to judge and select), but it is a popular way of giving structure to the evaluation of the decision making process. The further to the right on figure 1.5, the greater the focus upon what has been termed procedural rationality (Lee & Cummins, 2004) – in other words, that human decision making becomes one shaped both by cognitive processes (from the 3 phased approach) but also by contextual and situational pressures. Generally, business decisions are assumed to reflect key attributes of the RAT model – that individuals are driven and motivated by self interest and that systems operate best when they have minimal external direction and control (Olson, 2001). As individual interests often vary and may not converge, governance systems are created (such as establishing budget holders and committees maintaining oversight over some organisational function)- which then add a cost to that organisation. The pursuit and belief in RAT models of decision making therefore incurs a competitive cost for organisations unless of course an alternative and localised form of rationality can emerge in and between organisations (such as ‘strong’ trust) which negates the need for overt governance control procedures and policies (Barney, 2007). Critiques of the RAT model are commonly known but feature the following observations (see Olson(2001) for a fuller discussion): •	 Not all decision variables can be controlled by the manager •	 That manager’s decision making preferences for chosen solutions cannot be understood by examining those solutions alone

26
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 1:

•	 Manager’s welfare and hence their choices made cannot be divorced from the welfare of others (see figure 1.5 and noting that taking decisions that are too self interested (at the expense of others) will constrain future selection options as the ‘others’ seek to then constrain the manager and their decision making options). •	 Individuals and managers can and do exhibit altruism •	 The value attached to choosing between preferred outcomes and actions is not consistent between different managers and individuals. •	 That the individual is not necessarily the best unit of analysis for decision making or the determination of decision making (we explore group decision making for example later in Chapter 7). •	 Organisations do not function rationally as decisions are not economically optomized. So, the further to the right we travel in Figure 1.5, the greater the divergence from a normative rationality we observe in effective decision making. Later in the text we consider how rationality changes to become highly interdependent upon others when cohesive groups emerge in highly pressurized and often political contexts (chapter 7). We also consider how differing situations and contexts give rise to different dominant rationalities for decision making – such as the take the best (TTB) model of forced choice, the QuickEst model of value estimation or the categorization by elimination model (Lee & Cummins, 2004) (see chapter 6 for a fuller discussion). Finally, Lee & Cummins (2004) seek to unify the spectrum of rationalities in figure 1.5, by adopting one rationality which evidences different threshold levels of evidence to support decision making (where for example, the RAT model requires all available evidence to be sampled). To illustrate the importance of both the RAT model and other forms of rationality, Walker and Knox (1997) consider how consumers make buying behaviour decisions when purchasing different types of goods. The research explored what factors shaped an intention to buy newspapers, kitchen towels and breakfast cereals. The findings suggest that the greater the level of personal involvement and preference with the good, the more this shapes the decision processes to choose one good over another. Thus for newspapers, where personal enjoyment and content were identified as important, individuals will expend effort in locating their preferred product type. This was not observed with the kitchen towels or breakfast cereals, where despite a preference (or brand) being identified, the dominant rationality was not RAT based (i.e. users did not evaluate all the local offerings from good providers) – but more reflected a TTB heuristic (or a satisficing outcome).

1.6	Summary
This opening chapter has outlined the context of this text and the breadth and diversity of the discipline of management decision making. Importantly, there is an integral duality in the decision making process firstly – between the science and art perspectives, which was latterly explored through a consideration of the different types of rationality which have been observed and explored in decision making. This has extended from a normative rational view to an anarchic and heuristic view. In the next chapter, the discussion begins to consider decision analysis and positivistic methods of decision making.

1.7	

Key terms and glossary

Problem Domain – The scope of issues to be considered to resolve a problem so as to be able to then make a judgement. Structured Decisions - Decisions where the aim is clear so that the purpose of the decision to be taken is unambiguous, easily defined and understood.

27
Download free eBooks at bookboon.com

Effective Management Decision Making Unstructured Decisions – Decisions where the aim is ambiguous, opague and hard to understand. Programmed Decisions - Decisions which follow clear, delineated steps and procedures.

Chapter 1:

Non Programmed Decisions – Decisions where there are no existing procedures or practices in place to resolve the problem or address the opportunity. Strategic Decisions – Decisions concerned with the overall direction and goal of an organisation Tactical Decisions – Decisions concerned with actions which follow (and are required to be achieved) the strategic decision Operational Decisions – Decisions with are concerned with functional activities that are necessary to be undertaken by an organisation Decision Body – Describes the decision makers (those who have an influence upon the exercise of judgement between competing solutions to a problem). Decision Context – Describes the situational factors shaping and affecting the decision body Problem Identification – Is a key step in resolving a problem to be able to exercise judgement of identifying all relevant and necessary information and data pertaining to that perceived problem. Procedural Rationality – Describes how decision making is shaped both by cognitive processes, contextual and situational pressures.

28
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

Chapter 2
2.1	 Developing rational models with qualitative methods and analysis: Data forecasting
Chapter 1 introduced the broad themes of this text and the duality of decision making. This chapter encompasses the RAT end of the decision making spectrum of decision models (Figure 1.5 Chapter 1) by focusing upon data forecasting. In essence, data forecasting is based upon using historic data to understand and predict future data. There is therefore great reliance upon measured outcomes which are then blended with some analyst subjectivity in choosing how to model with that data. When you come across the phrase – data forecasting – from a decision making perspective, it usually refers to time series data (or other sequentially presented and gathered data). Within this data may also be other influences (such as a recurring trend or a seasonality influence). Clearly, the conditions which generated that data (whether as sales by an organisation over a 2 year timeframe or the rate of change of innovations in a given product for example), are important in our confidence that whatever model we develop, will be robust and a reliable guide to future data from that context. Within our analyses therefore we must also be concerned with the reliability, validity and verifiability of our forecasts, which requires a consideration of the stability and longevity of the assumptions we made about that context. Hyndman (2009:1) describes the role and function of forecasting as: “Forecasting should be an integral part of the decision-making activities of management, as it can play an important role in many areas of a company. Modern organisations require short-medium- and long-term forecasts, depending on the specific application. Short-term forecasts are needed for scheduling of personnel, production and transportation. As part of the scheduling process, forecasts of demand are often also required. Medium-term forecasts are needed to determine future resource requirements in order to purchase raw materials, hire personnel, or buy machinery and equipment. Long-term forecasts are used in strategic planning. Such decisions must take account of market opportunities, environmental factors and internal resources” In general, the methods presented in this chapter are focused upon short and medium term forecasting for managers and moreover this text adopts the view of using projective forecasting for short term analyses and causal forecasting for medium term analyses (these terms are discussed shortly). Longer horizon forecasting is outlined in chapter 7. However, data forecasting is not just restricted to developing quantitative models (see chapter 3 for a further narrative on modelling), which might naturally be assumed. Data forecasting can be both qualitative and quantitative. In the case of the former, it can be interpretivist and subjectivist. This includes decision making methods such as the study of heuristics (strategic, biological and behavioural decision making), variations on Delphi Decision Making and other futures analyses (such as FAR (Field Anomaly Relaxation) from studies of strategy, market research methods, cross impact analyses and historical analogy (to name a few)). These will be discussed in more detail in Chapter 7.

29
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

Returning now though to the quantitative focus on data forecasting, we can differentiate between projective methods of data forecasting (which are concerned with short term forecasts of the order of a few days or a couple of weeks (for example, the restocking decisions of independent small grocery / convenience stores)) and causal forecasting (or explanatory forecasting) which is concerned with longer future forecasting and which rather than rely upon the absolute data to guide future decisions, is focused upon the relationships between the absolute data, which can be argued to more robust and stable. In this chapter, we explore varying data forecasting methods, from simple averaging, through data smoothing methods, linear and non linear regression and data decomposition. Multiple regression (with multiple (non related) independent variables) will be presented in outline, although effective solutions to such problems are more easily undertaken by using appropriate software.

2.2	

Simple Averaging Forecasting

Time series data is typically sourced and presented in a chronological order. If the units of the progression are unclear, then they may have to be transformed into a more appropriate format. In using such data to predict future trends and future data, key questions to consider in their interpretation are whether such data would be representative of all trends in that data, whether the model chosen to forecast future data will be also be able to reflect short term preferences and whether the environment is stable (and to what extent) to support future forecasts.

.

30
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making Forecasting methods are generally understood as comprising three generic types of modelling:

Chapter 2

1)	 Smoothing – projective forecasting based upon the most recent historical data, to predict short term future data. Typical methods of decision making include simple averaging, moving averages and one variable exponential smoothing. 2)	 Trend analysis – can be projective and/or causal forecasting which considers both the recent historic data and immediate future forecast, to generate the next future forecasts and modelling. Typical methods of decision making include two variable exponential smoothing and trend smoothing. 3)	 Trend analysis with seasonal and/or cyclical influence – is usually focused upon classical data decomposition and can encompass both linear changes in data and non linear changes in change, to generate complex forecasting models. The first smoothing method of simple averaging allows a manager or analyst to use historic time series data to determine the next data in that time sequence. For example consider the two sequences below:
Series 1 Series 2 98 140 100 166 98 118 104 32 100 44

Both series 1 and series 2, have the same average (of 100) – but clearly from the range of data presented, you would have more confidence with this forecast for series 1 data – why? The variance of series 1 is small compared with Series 2 and hence the environment which generated this data is seemingly more stable and hence, predictable. We therefore have more confidence in our future forecast for Series 1. Consider for example – averaging is simply described as: ∑ (x1+x2+x3...xn) n

F(t+1)

=

Where F(t+1)= future forecast in time period (t+1) t= time (assumed to be current) xn= data for ith period (where i=1 to n) n= number of data points in the averaging calculation The variance in series 1 and series 2 is defined as the average of the squared differences from the mean, or: ∑ (xi-xm)2 n

Variance

=

Where xm = mean of time series data sampled xi= ith data point in the time series data.

31
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

Working through series 1 and series 2 – the variance for series 1 is 4.8 whilst that for series 2 is 2808! Aside from this problem with both this method and these two data series, other concerns focus upon the response of this method to changes in data (i.e. averaging over a large number of data values (a large n) will mean that the next forecast at (t+1) will be slow to respond to changes in that historic data). Also there are potential trends arising from other factors shaping the data (as well as how rapid those trends change) and noise in the data (which is hard to eliminate).

2.3	

Moving Averages

Clearly using simple averaging and including ALL the data in that sampling can generate significant problems in terms of forecasting responsiveness and accuracy (with large variances). One immediate improvement is to sample some, but not all the available data in the time series dataset. The choice of how many historic data points are considered in the moving average forecast (N) can be chosen depending upon the stability of the environment of the data and sometimes, reflect a regular period in the data (i.e. the data may evidence a cyclical trend in the data and an effective choice of N can help ‘deseasonalize’ that data).The ‘moving average’ has the simple formula of: F (t+1) = [D(t) + D(t-1) +… D(110) + D(109) + D(108)…]/N Or (say for N=3) – F(t+1) =(D(t)+D(t-1)+D(t-2))/N Where F(t) = forecast of a data value at time t Where D(t) = data actually observed at time t Clearly a moving average projective forecast for time period (t+1) is more responsive to changes in the historic data (and for example, a smaller value of N increases the sensitivity of the response to changes in that data). This method also allows the manager / analyst to also ignore some data. Conventions diverge on how to represent moving averages – within datasets. One approach is to recognise that as an average, this forecast should be placed at the mid-point of those data points sampled. Alternatively, the moving average forecast should be placed at the next point in time (i.e. t+1). For example:

32
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

DŽŶƚŚ ^ĂůĞƐ ϯDD ϰϱϬ :ĂŶƵĂƌǇ  ϰϰϬ &ĞďƌƵĂƌǇ  ϰϲϬ DĂƌĐŚ  Ɖƌŝů ϰϭϬ ϰϱϬ DĂǇ ϯϴϬ ϰϯϲ͘ϲϳ :ƵŶĞ ϰϬϬ ϰϭϲ͘ϲϳ :ƵůǇ ϯϳϬ ϯϵϲ͘ϲϳ ƵŐƵƐƚ ϯϲϬ ϯϴϯ͘ϯϯ ^ĞƉƚĞŵďĞƌ ϰϭϬ ϯϳϲ͘ϲϳ KĐƚŽďĞƌ ϰϱϬ ϯϴϬ EŽǀĞŵďĞƌ ϰϳϬ ϰϬϲ͘ϲϳ ĞĐĞŵďĞƌ ϰϵϬ ϰϰϯ͘ϯϯ :ĂŶƵĂƌǇ ϰϲϬ ϰϳϬ &ŽƌĞĐĂƐƚ ϰϳϯ͘ϯϯ 

ϲDD ϭϮDD             ϰϮϯ͘ϯϯ  ϰϭϬ  ϯϵϲ͘ϲϳ  ϯϴϴ͘ϯϯ  ϯϵϱ  ϰϭϬ  ϰϮϱ ϰϮϰ͘ϭϳ ϰϰϬ ϰϮϱ

Table 2.1: Simple Comparative Moving Averages

Think Umeå. Get a Master’s degree!
• modern campus • world class research • 31 000 students • top class teachers • ranked nr 1 by international students Master’s programmes: • Architecture • Industrial Design • Science • Engineering

Sweden www.teknat.umu.se/english

33
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Chapter 2

MS Excel offers additional tools and statistical functions to aid the analysis of data. These functions are accessed through the ‘Data Analysis’ Excel Add In. For MS Office 2007 for example, this is added through the clicking the Office Icon (top LH corner of the screen), selecting excel ‘options’, then highlighting the radio button for ‘Analysis Toolpak’, followed by selecting ‘Go’. Next select ‘Analysis Toolpak’ and click OK again. You will then find ‘Data Analysis’ under the Data menu tab. Under this Data Analysis tool, there are a range of additional statistical functions available for use. This includes ‘moving averages’.



Populating the relevant cell entries in the Moving average dialog box (below) is straightforward. Where the input range is the original historic time series data, the interval represents the number of data points over which you wish to average, and the output range is the cells into which you wish the moving average calculations to be placed. You can also choose to plot a chart of the moving average output and calculate the standard error (which is the difference between the forecast moving average and the actual data observed for that time period).



34
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

2.4	

Exponential Smoothing Data Forecasting

Exponential Smoothing refers to a forecasting method that considers a different weighting given to both the most recent forecast and the most recent historic data. It is a form of moving average forecasting but offers greater responsiveness and noise reduction. In this sense, it reflects the ‘exponential curve’ although the exponential function itself is not part of this analytical model.

Figure 2.1: Representation of varying weighting of data

Much fewer data points are needed to support the next period forecasting compared with simple averaging or moving averages. The calculation used is: New Forecast = (a fraction of the most recent actual data)+(1-the fraction chosen) x most recent old forecast made This can be written as: F t = α A t-1+ (1- α) Ft-1 F t+1 = α A t+ (1- α) Ft Where: α = weighted smoothing constant ( 0< α <1). F t = forecast for time period t F t+1 = forecast for time period (t+1) A t-1= observed historic actual data for time period (t-1) A t = observed data for time period t Hence the selection of the smoothing constant, can make the forecast for the next period of time more or less responsive to changes in the actual historic data observed – i.e. a large value for α makes the next forecast very responsive to changes in that observed data, whilst a small value for α, makes the forecast relatively unresponsive, so minor variations are ‘smoothed’ out of the forecast. It is convention to set the first forecast Ft to be equal to the most recent data At, to commence the forecasting process.

35
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

For example, compare the two forecasts below (of airline passengers over a period of fifty years or so) to see the difference between a large and small value of α. (The ‘square’ forecasts have been offset by one period to allow visual comparison of their values with the actual data then observed).

ĐƚƵĂůĂƚĂ

&ŽƌĞĐĂƐƚ

                  )LJXUH)RUHFDVWVZLWKDYDOXHIRUĮ    
Figure 2.2 :Forecasts with a value for α=0.9

How could you take your studies to new heights?
By thinking about things that nobody has ever thought about before By writing a dissertation about the highest building on earth With an internship about natural hazards at popular tourist destinations By discussing with doctors, engineers and seismologists By all of the above

From climate change to space travel – as one of the leading reinsurers, we examine risks of all kinds and insure against them. Learn with us how you can drive projects of global significance forwards. Profit from the know-how and network of our staff. Lay the foundation stone for your professional career, while still at university. Find out how you can get involved at Munich Re as a student at munichre.com/career.

36
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Chapter 2



Figure 2.3: Forecasts with a value for α=0.1

Clearly, when figure 2.3 is compared with figure 2.2, the magnitude of the responsiveness of the next period forecasts is dampened and only broad trend movements in the data are captured. The selection of α for forecasts is generally subjective (and usually chosen between 0.3 and 0.5) – but does depend upon the context within which the data is being analysed. The exponential smoothing method as presented is not sufficient to be able to reflect in its forecasts any sustained underlying trends in the observed data. It is responsive, to a greater or lesser degree to the most recent change in the observed data – but is not able to continually reflect and model sustained increases or decreases in that data in its forecasts. As a forecasting method therefore, it has value say for a small business wishing to determine what quantity of stock to buy based upon what was sold last week, but in terms of trying to model how many sales of that same product may occur in a year’s time, the modelling method is unable to project that far ahead with much confidence. As a medium to long term forecasting method in this form, the method then has limitations – although it can be amended to address and recognise underlying methods. To address this weakness we can use a method called Holt’s Method (or Holt’s Linear Exponential Smoothing (LES)) (named after its inventor C. Holt in 1959) (Southampton University, 2011: Lotfi & Pegels, 1996). This is also sometimes called double smoothing (as we are now concerned both with the most recent observed movement in the data and the underlying trend in that data (i.e. a series of ‘external pressures’ which are acting to continually drive observed data down or up)). Holt’s method is valid only though if you believe the trend shaping your observed data – is following a linear relationship – i.e. proportional changes to inputs result in proportional changes in outputs (if you double your sales team, you double their sales performance). Holt’s method introduces a new exponential constant, generally labelled as β. It has the formula: H t+m = F t+1 + mT t+1

37
Download free eBooks at bookboon.com

Effective Management Decision Making Where: H t+m = is the forecast for time (t+m) = Ft+1+mTt+1 T t+1= β(F t+1 - Ft) + (1- β)Tt F t+1 = α Dt + (1- α)(Ft+Tt)

Chapter 2

Hence α and β – are the TWO smoothing constants, m= number of periods in the forecast (i.e. number or periods which reflect the trend (just as N was in moving averages)). Also as before to start the forecasts – set F(t)=D(t) and now T(t)=0 – with typical trial values for α and β (and starting with m=1). In this way, forecasts which follow both model any underlying trend and can also be more or less responsive to observed data fluctuations about that underlying trend. The data presented in table 2.2 (and previously used in figures 2.2 and 2.3) below is a sample of collected data for cumulative air passengers since 1960. Here the actual date (in the left hand column) has been transformed into a cumulative quarter count – to allow the forecasting method to work and treat the date as the ‘x’ variable in a linear equation (we will discuss the nature of linear equations shortly). In this example both α and β have been arbitrarily set to 0.3, t is set to zero initially and m to 1 initially. Constructing the equations above into excel generates the forecasts below.
Quarter 1.00 2.00 3.00 4.00 5.00 6.00 7.00 8.00 9.00 10.00 11.00 12.00 13.00 14.00 15.00 16.00 17.00 18.00 AIR passengers (000s) 112.00 118.00 132.00 129.00 121.00 135.00 148.00 148.00 136.00 119.00 104.00 118.00 115.00 126.00 141.00 135.00 125.00 149.00
Table 2.2

α =0.3 0.00 112.00 116.20 127.26 128.48 123.24 131.47 143.04 146.51 139.15 125.05 110.31 115.69 115.21 122.76 135.53 135.16 128.05

T(t) 0.00 33.60 24.78 20.66 14.83 8.81 8.64 9.52 7.70 3.18 -2.00 -5.82 -2.46 -1.87 0.96 4.50 3.04 -0.01

H(t+m) 0.00 145.60 140.98 147.92 143.31 132.05 140.11 152.56 154.22 142.34 123.04 104.49 113.23 113.34 123.72 140.03 138.20 128.04

The data in table 2.2 can then be plotted (where more x data (quarters) and y (passenger data) have been added). The close fit of the forecast to the observed data is very clear by inspection of figure 2.4.

38
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

ĐƚƵĂůĂƚĂ


Figure 2.4

Scholarships

Open your mind to new opportunities

With 31,000 students, Linnaeus University is one of the larger universities in Sweden. We are a modern university, known for our strong international profile. Every year more than 1,600 international students from all over the world choose to enjoy the friendly atmosphere and active student life at Linnaeus University. Welcome to join us!

Bachelor programmes in Business & Economics | Computer Science/IT | Design | Mathematics Master programmes in Business & Economics | Behavioural Sciences | Computer Science/IT | Cultural Studies & Social Sciences | Design | Mathematics | Natural Sciences | Technology & Engineering Summer Academy courses

39
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Chapter 2

A third iteration of exponential smoothing has been developed – sometimes called triple smoothing – and also known as the Holts – Winter method (again after its founders from the 1950s) which not only accounts for underlying trends in the observed data but can also respond to cyclical changes in data (i.e. it can model data which is apparently rising and falling with a regular and observable period). The Holts – Winter method has the following construction (Lotfi & Regels, 1996) – for a forecast Wt+m at period (t+m): Wt+m = (Ft+mTt).St (NB. The ‘.’ Symbol denotes multiplication) Where Ft is the smoothed value at time t, and is found by: F t = α A t/ St-m+ (1- α) (Ft-1 +Tt-1) St is the seasonality estimate at time t and is found by: St= β A t/ Ft + (1- β) St-m Tt is the trend estimate at time t found by: Tt = γ(Ft-Ft-1) +(1-γ)Tt-1 So – as before, and building upon the preceding models: α = simple smoothing constant β = smoothing constant for trend γ = smoothing constant for seasonality Let’s consider an example using the following data:
Time (t) 1 2 3 4 5 Observed data (D) 15.0 12.1 8.2 6.3 4.2

As before, to begin we arbitrarily select constant values between 0 and 1 and that the first forecasts reflect the most recent actual observed data:

40
Download free eBooks at bookboon.com

Effective Management Decision Making α = 0.4, β = 0.3, γ = 0.1 m=4 (assumed gradient estimation)

Chapter 2

As before, to commence the forecasting series, we set previous initial estimate of Tt =0, F0=D0=15 and all previous S values to 1 (S-3=S-2=S-1=S0=1). Then for t=1, we can generate the following:
Time (t) 1 2 3 5 6 7     Observed data (D) 15 12.1 8.2

F 15 13.84 11.5144      

S 1 0.962283 0.913646      

T 0 -0.116 0.633495      

W (final)       15 12.8715 12.83524

Table 2.3 : Forecasted data

Once the correct formula have been set up in the cells of excel, it becomes relatively easy to copy and paste, to populate the forecasted values.

2.5	

Errors, accuracy and confidence

Before moving on to consider further forecasting methods, clearly the manager and student of business, needs some understanding of which is the better forecasting method to use and why. The answer to this need is both quantitative AND qualitative. Take note that it is relatively easy to use the simple error equation below to determine the ‘best’ forecasting method by comparing the errors between forecasts made and data observed at any given point in the past – but that does not necessarily mean that the better forecasting method is the one with lower overall errors. Scope for individual managerial judgement and interpretation of the context in which decisions are taken, may not support or fit with a simple mathematical argument. So- we can broadly define ‘errors’ as the difference between the actual data observed and the forecast made for the same time period (t). This can be described simply as: E(t)= D(t)-F(t) The error at a given point time t is the difference between the observed data and forecast made for that same point in time, t. It is obvious therefore that the mean error of a forecasting model will be: Mean error = Σ E (t)/ n OR: = Σ [D(t) – F(t)]/n

41
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

Whilst this is an intuitive understanding of E(t) – clearly is has difficulties as it ignores the sign of any difference between the forecast and observed data for time period t. When we consider E(t) we are primarily concerned with the magnitude of difference between the forecast and the observed data. Hence it is more helpful to consider the modulus of the error as: Mean absolute deviation error = Σ E (t)/ n OR: = Σ |[D(t) – F(t)] |/n OR: Mean Square Error = Σ ([D(t) – F(t)] )2/n Modulus refers to the mathematical operation of ignoring the sign of an integer and focusing only upon its magnitude. We will consider more on errors and confidence in your modelling shortly.

Cyber Crime Innovation

Web-enabled Applications

Are you ready to do what matters when it comes to Technology?

42
Download free eBooks at bookboon.com

Data Analytics

Implementation

Big Data

.NET Implementation

Click on the ad to read more

IT Consultancy

Technology

Information Management

Social Business

Technology Advisory

Enterprise Application

Java

SAP

Cloud Computing

CRM

Enterprise Content Management SQL End-to-End Solution

Effective Management Decision Making

Chapter 2

2.6	

Causal forecasting (explanatory forecasting)

Unlike the projective forecasting methods presented so far, causal forecasting is concerned with the trends and relationships between the observed data – more than the actual data value per se. These methods therefore are reliant upon arguably, more modeling skill in identifying potential relationships in observed data (or expected relationships in data) which can then be used to model and forecast. We start this process by considering generically, what type of relationships shape our observed data. For example we have already met the idea of there being a trend in data (here called T) and that data may also be shaped by some external regularity (i.e. a series of external factors which are observed through their regular and repeated pattern of impacts on observed data - here called S – for seasonality (but do not think this ONLY means the natural seasons!)). We also usually identify an underlying trend (here called U) to denote the commencement of our data values of interest and ‘random’ noise (here called R) – which reflects minor variations in observed data subjected to a myriad of market forces which act without coherency. Hence we need to consider 4 interdependent ‘forces’ acting upon our data as: U(t) - Underlying trend (where has our data begun?) T(t) - Trend (how does each data relate to every other on a macro level) S(t) - Seasonal index (how does the data relate to each other on a micro level) R(t) - Random noise or error (E) The data observed for a given business activity (i.e. sales performance) is then argued to be determined by the interaction of these 4 factors. If we can find mathematical ways of relating these 4 factors together that seem to replicate the observed data, and which evidence low values for R(t) or E(t), then we have a potentially valuable method of forecasting into the future. It is important to note that as this approach is NOT concerned with the actual data to determine future forecasts, only the derived and proposed relationships between the data, then these methods offer forecasting that can extend into the medium term (as long as you believe the relationships between U,T,R and S remain valid assumptions to hold). The use of this approach to modelling and forecasting is sometimes called ‘classical decomposition’ – in the sense that as the modeller, you are seeking to identify and determine the different relationships between U, T, R and S. We start this process by plotting the observed data we have obtained against (usually) time. This is helpful as it provides the modeller with a visual reference as to whether variations in the observed data seem to resemble known mathematical equations. For example:

43
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

d

^ Z   h
Figure 2.5

In figure 2.5, the four relationships of U,T, S and R are shown (note – R is a scaled up view of one of the peaks, showing the minor variations in the data that form the apparent plot). S represents the seasonal trend in the data (where between data points A and B the pattern of the data is repeated). Hence if there were 7 data points that constructed the pattern between A and B, we could say that this data has a seasonality with a period of 7. Before we go further with this discussion – it is appropriate to (re)introduce linear equations and their graphical interpretation to you. Linear relationships between two variables (say x and y) simply mean that if you double x, you double y or if you halve x you halve y. A change in one variable results in a proportionate change in the other variable. In this way, usually we can say that y is the dependent variable (as it depends on x) and x is the independent variable. Where we are concerned with data forecasting of course, the x variable is usually time (t). So, back to basics and consider this dataset:
T y 1 10 2 20 3 30 4.5 45 5 50 6 60 7.5 75 8 80 9 90

44
Download free eBooks at bookboon.com

Effective Management Decision Making A plot of this data yields the graph below:

Chapter 2


Figure 2.6

This graph is not surprising given the data. Can we quantify the relationship between t and y here?

AXA Global Graduate Program
Find out more and apply

45
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Chapter 2

If we look at the data again, we note that y is a multiple of t (i.e. if you divide y by t you always get 10 as your answer). Hence we could describe the whole range of data presented as y=10t. It is not hard to imagine then that if y was increasing at a steeper rate we could result in a dataset described by y=10t+7 or if y was increasing at a slower rate, y=10t-7 and so forth. Now the value (or coefficient) that precedes the independent variable (t) here is also known as the gradient of the dataset (or the gradient of the line it ends up plotting). The gradient is a word that means the ‘rate of change in one dependent variable when compared with its independent variable’. Hence we could also rewrite our equation as say: (proportionate change in y (the dependent variable)) t +7 (proportionate change in t (dependent variable))

Y=10t+7 or y

=

This is usually written in generic form as Y=mx+C or in some texts (and here) as y=bx+a. These are called linear equations because of this proportional relationship between the variables (which means their plots are always straightlines). We can now re-consider figure 2.5 and note that T (the trend) is described by the gradient ‘b’ in a standard linear equation and U, the underlying trend (i.e. where our data starts is described by a (or C)). If you are not sure of this just consider what would happen when we set t=0 (i.e. at the start of the time for our linear equation) - clearly y=7 (or C or a). If we believe therefore that in a given range of data presented to us, to expect to see a proportional change between two variables (e.g. sales team size vs sales team performance), then we can think about using a linear equation to model the relationship between our 2 variables (here in the case of sales team (the independent variable) and sales performance (the dependent variable)). All we would need to do, would be to find the right ‘best fit’ equation which gives the lowest errors (see the earlier discussion on errors) to describe this relationship. Now if we return to our earlier discussion, we described how seasonality ‘S’ was represented by a regular repeated movement of data that has a defined period. This means that whilst a plot of sales team vs sales performance may have a trend and an underlying trend, it may also show up and down movements around that trend (T). So for some data points, the actual observed data may be higher than a trend line plotted through the data, whilst other data points would be lower than a trend line plotted through the data. From a modelling perspective and in simple terms, if we want to use a linear equation to describe the data’s movements in the past so we can use those same relationships in the future, we must modify the linear equation at the right point in the seasonal period. Clearly, the seasonality – or the movement in the data around the trend (T) can be constant (as it seems to be in figure 2.5) or could be increasing or decreasing with the passage of time. It is important to always determine this early in data forecasting if you wish to use this method as it will have an important bearing upon the accuracy of your subsequent model. When the seasonal movement in the data is described as being constant – this is called an additive model- and where the movement in the seasonal data is described as increasing/decreasing with the passage of time – this is called a multiplicative model.

46
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

These labels are self explanatory when you consider them. A constant movement in seasonal data (S) about a trend (T) really means that if you were to find the linear question which describes that trend T in the data – your best forecast needs to either add a little or take a little off (i.e. add a negative value) to that trend forecast – at the right point in time – to be accurate and reflect what you feel is a description of how the data relationships seem. Hence this is the additive model. An increasing or decreasing movement in the seasonal data (S) about a trend (T) similarly means you need to amend the forecasted value using your linear equation by a proportional amount at the right point in time. This is described as the multiplicative model as rather than add a fixed value (whether +ve or –ve to your trend based forecast) you instead take a % of the trend based forecast (so if you had the model y=10t+7 then for t=1, y would be 17), but if you thought this forecast at this time was influenced by seasonality, you might modify that forecasted value of 17 by 25% to give 4.25 as your final forecast for when t=1 (or equally you might determine that at that time t, seasonality meant you needed to modify 17 by say 125%, to give 21.25 as your final forecast). These two generic models are usually described by the formulations: Additive models Y=U+T+S+R Multiplicative models Y=U.T.S.R (and where usually we ignore R – and treat it is random noise in our data). Having reviewed the principle of the methodology – whereby the best fit linear line equation is identified (to determine U and T), any corrections to the values of this line for a given point in time, t, are then undertaken in an appropriate fashion (to determine S), to reach a final forecast – we can now work through a simple example using the following steps: 1.	 Inspect your data and the context of the data – Is it additive or multiplicative (plot the data it if in doubt)? If still unsure – you can make your forecasts using both models and then determine the MSE (choosing the best fit) – you will determine U 2.	 Calculate the Trend (T) – using Ordinary Least Squares regression (OLS) 3.	 Estimate values for each Nth period using regression formula 4.	 Determine the percentage or absolute variation of each period’s values from the estimates 5.	 Average these variations / smooth them – to determine S (R is generally ignored) 6.	 Forecast for n periods ahead based on T*U*S or T+S+U

The Ordinary Least Squares Regression method
The final method to develop skills in use is the (ordinary) least squares (OLS) regression method. This can be completed long hand or by using an appropriate software package – (excel is appropriate for the simpler versions of these problems). OLS is a mathematical method for determining the best fit linear line for a series of data points (note – you need to be confident you have identified proportional relationships between your dependent and independent variables. We will see later how we can ‘bend’ this requirement and force data to be interpreted in a linear fashion (even when the data may not be linear at all)). OLS determines the starting point and gradient of the best fit, by determining the errors and summing them, of the data points from the means of the data points (for x and y). The resultant value for the gradient is that which gives the lowest error and once this has been found, the intercept on the y axis – can be determined. This has the formulation:

47
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

(Remember y=bx+a as the general equation of a linear line (where a is the intercept on the y axis and b is the gradient of the line). By definition : (1) Σy = an +b Σx And Σxy=aΣx + bΣx if multiplied through by Σx
2

If a is substituted, you can derive: (2) b =( (nΣxy – ΣxΣy) / (nΣx2 – ΣxΣx)) And hence substituting this equation for b into (1) you can prove that (3) a= (Σy/n) – b(Σx/n) (or in other words that the intercept is the mean of the original y values less the mean of the x values multiplied by the gradient). So – if you wanted to work through a time series dataset to determine the appropriate equation for a best fit line, you could simply use Excel and construct the following:
&ŽƌĞĐĂƐƚƐƵƐŝŶŐǇсďǆнĂ;ŚĞƌĞĂƐǇсϭ͘ϴϰǆнϮϴ͘ϳϯͿ KƌŝŐŝŶĂůĂƚĂ
; 4XDUWHU                < 6DOHV                   /($67 648$5(6 1  ;<                   6TXDUHG;                  5HJUHVVLRQ (VWLPDWHGVDOHV                     E  D   

^hDD ŽůƵŵŶƐ

 

E


Figure 2.7: Longhand construction of OLS calculations

Values for a and b have been determined by constructing the equations for them, in the cells of the excel worksheet (i.e. equations (2) and (3)). The figures in bold italic in the regression estimated sales column (on the right of figure 2.7) are the true forecasts made only using the developed model of y=1.84x+28.73 (i.e. forecasts made when x=17,18,19 and 20). Clearly, once the cell formula have been set up and providing the assumptions which have been determined to underpin the relationship between the dependent and independent variable – remain valid, you can ‘click and drag’ the final forecasting column as far into the future (by increasing the x column) and the final forecasting column. A simple plot of this forecast and the original data shows the following:

48
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

ϯ Ϯ ϭ ϰ


Figure 2.8 : Original data plotted

49
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Chapter 2


Figure 2.9: Original data plotted and best fit linear line plotted

Clearly in figure 2.9, whilst the best fit line seems appropriately placed, if this was only used to determine the forecasts for time period 20 say (x=20), you would generate a poor value with high errors (remember this is the difference between the forecasted value and the actual data you would then see). The original data in figure 2.8 clearly shows it has seasonality and that this seasonality (the variation (rise and fall) from the mean of the y values (sales)) is perhaps changing in amplitude as time progresses (as we increase x). Moreover, if you inspect figure 2.8 you can see that the seasonality seems to have a period of 4 – i.e. 4 datapoints (1, 2, 3 and 4) are covered by each rise and fall in the data, before the pattern repeats itself. So whilst the solution presented in figure 2.9 reflects U and T, we also need to include S, to derive a complete final forecast. If we then extend our table in figure 2.7, to generate figure 2.10:

50
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

dŚŝƐŝƐƚŚĞǀĂůƵĞŽĨ ĐƚƵĂůͬ&ŽƌĞĐĂƐƚ ǆϭϬϬй;ŚĞƌĞ͗ϮϬͬϯϬ͘ϱϳͿ ʹǁŚŝĐŚƚŚĞŶĐŽŶƚŝŶƵĞƐ ƚŚƌŽƵŐŚƚŚĞĐŽůƵŵŶ

5HJUHVVLRQ (VWLPDWHGVDOHV      

3HUFHQWDJH IRUHFDVW FRUUHFWLRQ                

$YHUDJHRI VHDVRQDOYDULDWLRQV    

6HDVRQDOO\ DGMXVWHGIRUHFDVW                    

dŚŝƐŝƐƚŚĞĂǀĞƌĂŐĞŽĨƚŚĞ ƉĞƌĐĞŶƚĂŐĞĨŽƌĞĐĂƐƚĐŽƌƌĞĐƚŝŽŶƐ ŽǀĞƌƚŚĞƉĞƌŝŽĚŽĨƚŚĞƐĞĂƐŽŶĂůŝƚǇ ;ŝ͘Ğ͘;ϲϱ͘ϰϯнϵϴ͘ϳϰнϭϴϭ͘ϬϯнϴϬ͘ϯϲͿͬ ϰͿ

             

dŚŝƐŝƐƚŚĞĨŝŶĂůĨŽƌĞĐĂƐƚŽĨhΎdΎ^ŽĨ ϱϱ͘ϳϭйŽĨϯϬ͘ϱϳсϭϳ͘Ϭϯ 

Figure 2.10 : Final forecast using U*T*S

In figure 2.10, we have the trend forecasts derived from the linear equation of the best fit line, and this is then compared with the actual historic data (the original y column data). As we believe that seasonality is not constant over time, but seems to have a slow increase, we have determined the percentage difference between this trend forecast and that original data (i.e. Actual / Forecast x 100%). This calculation creates the ‘percentage forecast correction column’. We have noted that the seasonality for this data has a period of 4, hence 4 datapoints construct the same pattern in the data. It just so happens that the first data point on our plot in figure 2.8 is the start of this pattern of the data – but be careful to note that this is not always the case. If we wanted to determine therefore the forecast for the next time in which we expect to see our data return to this point in the data pattern of the seasonality (i.e. the 5th data point in figure 2.8), we would need to only consider 65.43% of the trend forecast for that point in time (i.e. when x=5). In doing so we are determining (T*U)*S i.e. we have modified the original trend forecast by an appropriate seasonal percentage (sometimes you might see this written as a seasonal index). Whilst this gives a much better forecast – we can improve this method a little bit more. We can improve this method by recognising that we have 16 datapoints here and at least 4 sets of datapoints give values for the same period (i.e. the same point in the pattern) of the seasonality. Therefore if we were to average those datavalues to determine the percentage forecast correction – for the correct point in the period of the seasonality, we help to further eliminate minor variations in the observed data and improve our final forecast. This is what is done in the column of average seasonal variations and explains why there are only 4 values in this column (as you only need 4 averaged values derived from averaging the percentage forecast correction to cover all the period of the seasonality). With these averaged seasonal variation values, you simply multiply it by the trend forecast, to determine the final forecast (described as the seasonally adjusted forecast in figure 2.10).

51
Download free eBooks at bookboon.com

Effective Management Decision Making So – to recap, the 5th value in the seasonally adjusted forecast (21.13) has been derived from the following: 1.	 The Trend forecast for x=5 (the 5th Quarter) was 37.93 (this includes T and U components) 2.	 The percentage forecast correction for x=5 (the 5th Quarter) was 21/37.93x100% = 55.35%

Chapter 2

3.	 As this correction value of 55.35% occurs at the first point in the seasonal cycle (which has a period of 4) we can identify and average ALL the other forecast corrections which occur at this point in the seasonal cycle (i.e. (65.43+55.35+50.78+51.27)/4=55.71% 4.	 As we are concerned with the seasonally adjusted forecast for the 5th datapoint when the seasonal cycle is just beginning to repeat (i.e. the modification to 37.93 (which is T and U)) – we know that we can multiply therefore 37.93x55.71% = 21.13. This is the final forecast (and reflects T*U*S). 5.	 For all other forecasts – and especially those for x>16, we simply extend the x column (Quarters), to determine the trend forecast and then modify it by the appropriate seasonal percentage value. So – it is clear that the decomposition method, utilizing an additive or multiplicative approach to manage seasonality, provides significant scope and flexibility to model a variety of univariate data (with one dependent variable). However, there are two questions that then arise which need further consideration. 1.	 Can we develop greater understanding about the errors in our models – in particular how good is our model? Or in other words, how much variance have we been able to explain by our simplifying assumptions used to generate our assumed mathematical relationships? 2.	 And what can we do, if these approaches only seem to generate poor or inappropriate modelling interpretations of the data we have gathered? We can take these questions in turn and explore another statistical relationship – namely Pearson’s Coefficient (of correlation) r and the Coefficient of Determination (Pearson’s Value) or R2. We will examine this mechanistically first, before introducing some excel shorthand to determine its value more efficiently. Pearson’s Coefficient (r) is a measure of the linear association between two variables (and by implication how accurately you can predict one from knowing the other). As we are at this time considering only linear (assumed) relationships, we would expect r to have the range of: -1<r<1 or in other words that if a positive proportional change in our variable was exactly matched by the same proportional change in the other variable, their ratio would be 1, and similarly -1, if there was a matched negative proportional change. As we are only concerned here with the independent variable x and the dependent variable y, r would be a measure of the strength of the linear relationship between them. (nΣxy – ΣxΣy) √((nΣx - (Σx)2).(nΣy2-(Σy)2))
2

r

=

Similarly, R2 is the measurement of explained variance – i.e. how much variance in our model is able to explain the observed variance in the actual data. Hence it is found by: Σ (Ye- mean of y)2 Σ(y-Mean of y)2

R2

=

52
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

Hence, Ye is the modelling forecast for (time) position t, ‘mean of y’ is the arithmetic average of the observed dependent data (usually y or equivalent). In essence the calculation for R2 is the ratio of the summed difference between the mean of the model forecast data and the average of the data and the difference between the observed and the average of that data. Clearly as the value of Ye tends towards the value of y, the ratio will tend towards 1. So if we determined the calculation for R2 as being 0.75, we could state that 75% of the observed variance in the data (from the mean) has been ‘explained’ by our model. Equally this also means that 25% of observed variance has not been captured by our chosen model of the data relationships. Clearly, we would like a value of R2 to be as near as possible to 1 (note - you can obtain values >1 for this ratio – it is the magnitude of difference from 1 that is important). If the determined value of R2 is significantly far from 1, say >0.5 difference, then we have a poor fitting chosen model and need to review our methodology. We can either determine R2 by a simple calculation (using the formula given), or use Excel’s functionality to determine this value and aid our understanding of what we might have missed in setting up our forecasting model. We introduced earlier, the Data Analysis add-in for Excel when discussing moving averages and exponential smoothing. The Add-in also has a regression function which can be used to determine both linear and non linear best fit equations, to aid your modelling. Let’s take a simple problem first and then develop it further. Question: Q) A specialist has advised that the number of FTEs( Full time employees) in a hospital can be estimated by 	 counting the number of beds in the hospital ( a common measure of hospital size). A researcher decided to develop a regression model in an attempt to predict the number of FTEs of a hospital by the number of beds. 12 hospitals were surveyed and the following data obtained. These are presented in sequence (by number of beds):
Number of beds 23 29 29 35 42 46 50 54 64 66 76 78
Figure 2.11

FTEs 69 95 102 118 126 125 138 178 156 184 176 225

53
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

We need to find the appropriate regression equation to test the state hypothesis and consider the errors in that resulting equation. From Data – Analysis – we select ‘Regression’ –

I’M WITH ZF. ENGINEER AND EASY RIDER.
www.im-with-zf.com

CH ARLES JENKIN

S

Scan the code and find out more about me and what I do at ZF:

Quality Engineer ZF Friedrichshafen

AG

54
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making Which then opens the following dialog box as:

Chapter 2

ϭ

Ϯ

ϯ

ϰ

We populate the relevant cell entries in the dialog box (follow the numbered arrows above) as: 1)	 X range – is your independent variable data (here the number of beds). Place the cursor in the dialog box and highlight ALL the cells (i.e. the cell range) you wish to include as your X data (this would be here the values from 23 through to 78). 2)	 Y range – is your dependent variable data (here the number of FTEs). Place the cursor in the dialog box and highlight ALL the cells (i.e. the cell range) you wish to include as your X data (this would be here the values from 69 through to 225). 3)	 Select ‘output range’ and then select a blank cell on the worksheet from which Excel will put the statistical calculations 4)	 You can also check the residual and residual plots boxes. Residual calculations are the Errors (i.e. E(t)). This is the difference between the observed data and the forecast data using the model structure you have told excel to use. In this example we have assumed a simple linear relationship as the plot of beds vs FTEs is shown below (and as we have been asked to test the hypothesis that there is a linear relationship between the two sets of data). A residual plot will be therefore, a plot of the errors of the forecast. It is helpful for a manager (and modeller) as it will identify if there are any trends in the errors.

55
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2


Figure 2.12

In other words, a residual plot should only exhibit apparently random errors. If we were to see a plot of errors with an upwards trend or downwards trend or some apparent pattern in the data – we could be concerned that our chosen mathematical relationship model, has not sufficiently captured the relationships in that data. We should then look for a better model and review the context within which the data is valid (as this usually gives strong guidance on what types of relationship we could see and expect). So- returning to our example, if we populate the cell boxes, click ‘OK’ – the result is the following:

6800$5<287387 5HJUHVVLRQ6WDWLVWLFV 0XOWLSOH5  56TXDUH  $GMXVWHG5 6TXDUH  6WDQGDUG(UURU  2EVHUYDWLRQV 
$129$  5HJUHVVLRQ 5HVLGXDO 7RWDO GI    66    06 )   

ŽĞĨĨŝĐŝĞŶƚŽĨĞƚĞƌŵŝŶĂƚŝŽŶ


^ŝŵƉůĞƌƌŽƌ

6LJQLILFDQFH ) (

 ,QWHUFHSW ;9DULDEOH

&RHIILFLHQWV  

6WDQGDUG 8SSHU (UURU W6WDW 3YDOXH /RZHU         (  

/RZHU   

8SSHU   

Ă;ŝŶƚĞƌĐĞƉƚŽŶz;&dͿʹh ĐŽŵƉŽŶĞŶƚ ďǀĂůƵĞ;ŐƌĂĚŝĞŶƚͿʹd ĐŽŵƉŽŶĞŶƚ

56
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

Observation 1 2 3 4 5 6 7 8 9 10 11 12

Predicted Y 82.23706148 95.62608545 95.62608545 109.0151094 124.6356374 133.5616534 142.4876693 151.4136853 173.7287253 178.1917332 200.5067732 204.9697812

Residuals -13.2371 -0.62609 6.373915 8.984891 1.364363 -8.56165 -4.48767 26.58631 -17.7287 5.808267 -24.5068 20.03022

In the output from excel is a range of data, some of which has been highlighted. For completeness, Cameron (2009) offers insight on the remaining information as follows: •	 Adjusted R2 – is the coefficient of determination when multiple independent variables have been used in the forecast (hence you’ll only need to refer to this if for example you have y, and variables for x1, x2, x3 etc). •	 Standard Error – is the sample estimate of the standard deviation of the error •	 Observations – number of data points (observations) used in the forecasting For the ANOVA table (the analysis of variance in the data), breaks down the sum of squares into its components, so that: Total sums of squares = Residual (or error) sum of squares + Regression (or explained) sum of squares. Thus Σ i (yi – mean of y)2 = Σ i (yi – forecast of yi)2 + Σ i (forecast of yi – mean of y)2 The F column- overall F-test of H0 (this is the so called null hypothesis where there are no expected other independent variables affecting the observed data (i.e. variables for x2 = 0 and variables for x3 = 0)): variable for x2 = 0 and variable for x3 = 0 versus Ha: at least one of variables of x2 and x3 does not equal zero (these are the possible independent variables). An F-test is a statistical measurement of the extent to which a data set exhibits key expected relationships. Hence in this analysis, an F test is the ratio between explained variance and unexplained variance. The next column labelled significance F has the associated P-value (which is a statistical measurement of the confidence you have in a given analysis (i.e. in this example that there is a simple linear relationship apparently determining the dependent data)). As per statistical convention as this value is <0.05, we accept it with 95% confidence. Finally, the remaining data table contains information relevant as:

57
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

•	 Coefficient - gives the least squares estimates of the independent variable (remember this can be for a single value of x or xi. •	 Standard error - gives the standard errors (i.e. the estimated standard deviation) of the least squares estimates for the independent variables. •	 T Stat- gives the computed t-statistic for H0: variable for xi = 0 against Ha: variable for xi ≠ 0. A t test is a statistical relationship to test if the mean of data follows an expected normal distribution. •	 P-value- gives the p-value for test of H0: variable for xi = 0 against Ha: variable for xi ≠ 0. See the earlier discussion on the p value. •	 Lower 95% and Upper 95% - are the values that define a 95% confidence interval for xi variables.



If it really matters, make it happen – with a career at Siemens.

siemens.com/careers

58
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Chapter 2

The plot above is therefore a graph of the errors – i.e. the difference between the forecast made using the equation generated by excel to fit the ‘best fit line’. It does not look like there is anything other than random noise shaping the errors in this plot. Another example would be:
\ [







ƌƌŽƌŽĨƚŚĞ


&ŽƌĞĐĂƐƚ





       

Figure 2.13

The error of the forecast is as shown in Figure 2.13. If we did another forecasting modelling on some other data and observed this plot of errors:


Figure 2.14

Then the plot of errors in Figure 2.14, is not apparently random, but follows another periodic pattern. This is indicative of another relationship between the variables which have not been accounted for in this model and which is as a result influencing the errors between the forecast and the actual data observed. If we now return to the Excel calculations presented, there are several key values to note: 1)	 R2 and the simple error are presented 2)	 The column headed coefficients – indicates the values of the intercept on the y axis (a or the U component) and the gradient of the best fit line (b or the T component) – hence ‘x variable 1’ means the coefficient of the first power of x (i.e. if y=a+bx then this is the value for b)). However, the regression function allows you to also explore curve fitting with ease – a topic we look at next. 59
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

2.7	

Non-linear Forecasting and multiple regression– Curve fitting

You may recall at School, that one method of representing large numbers was to use logarithms. Logarithms are effective mathematical shorthand for presenting numbers in the forms of their ‘powers’. For example, the number 100,000 can be represented by 105. It is much easier to work with the ‘powers’ of large numbers, than the large number itself. So logarithm values are tables of power values that represent larger numbers. For example, the logarithm of 100,000 is 5 and the anti-logarithm of 5 is 100,000. You can do the same with any number (and also of note, with any number base system – although by convention we usually use the decimal base 10 number system). That is why to find the anti-logarithm in excel, you type the formula: =10^(your value) (where the ^ symbol is found above the ‘6’ key and means 10 raised to the power of…) Relationships between very large and small numbers can therefore be presented in this form of mathematical shorthand and several ‘laws of logarithms’ then follow. For example, the well known laws include the following (Note both the decimal – base 10 number system and the natural logarithm number systems are shown below): 1)	 Log10 A + log10 B = log 10(A.B) 2)	 Log10An = nLog10 A 3)	 Log10A - Log10B = Log10(A/B) 4)	 Log101 = 0 5)	 Logmm=1 6)	 Log1010n = n 7)	 Logeen=n (the Loge transformation is also often written as LN) These simple laws are very useful in applying the linear regression method to data which evidences non linear relationships between measured variables. For example, we can consider non linear equations and polynomial equations of: Y=a.bx

60
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

Y=a.xb

Y= a.ebx

Y=a.e-bx

61
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

Y=a+bx2+cx3+(dx4.....ixn) – (this is the cubic example)


The polynomial equation – which is plotted above –shows that clearly higher polynomial powers of x would simply generate additional points of inflexion in the plot (inflexion points are data points where the gradient changes through a zero point). The key observation to note for the manager / researcher seeking to curve fit this type of relationship is that we can use the regression function of excel to generate the best fitting equation and where necessary, transform data that is apparently non linear, into a linear format and then use the regression function of excel to solve. Consider the following problem and provided data – which firstly illustrates how to use the Excel Data analysis Add in, to solve polynomial regression problems and then secondly, considers a non linear non polynomial problem: The owner of a newly launched electronic journal is reviewing the marketing revenue and based upon the number of paid subscriptions, wants to know what the expected income might then be if the reported ratings and rankings of the magazine achieves a level of 5.6. She can then determine what type of news item/story to publish to achieve this and whether the revenue generated can cover the increase in story fees.

62
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

Rating -30.00 -29.00 -28.00 -28.00 -26.00 -25.00 -24.00 -23.00 -22.00 -21.00 -20.00 -19.00 -19.00 -17.00 -16.00 -15.00 -14.00 -10.00

Revenue(000s) -12971.00 -11695.45 -11108.00 -10738.00 -7730.80 -7100.25 -6003.20 -4122.75 -3919.00 -3820.05 -4666.00 -3118.95 -3952.95 -1917.25 -2125.80 -1283.75 324.80 -1185.00

Rating -12.00 -11.00 -10.00 -9.00 -8.00 -7.00 -6.00 -5.00 -4.00 -3.00 -3.00 0.00 0.00 1.00 2.00 2.00 4.00 5.00

Revenue(000s) -1399.00 -1152.55 -515.00 -1136.45 -279.00 1020.25 -804.80 -177.25 100.80 1158.25 -730.75 1317.00 1553.00 1261.05 1115.00 -1272.00 1175.20 107.25

Rating 6.50 7.00 8.60 9.00 11.00 11.00 12.00 15.00 14.90 15.00 16.00 17.00 18.00 19.00 20.00 21.00 22.00 23.00

Revenue(000s) 1113.86 1501.75 1823.48 2106.45 252.55 415.55 2723.00 3118.75 4015.88 4722.75 5248.80 3277.25 5066.00 6830.95 6369.00 7509.05 9486.00 11731.75

Rating 24.00 25.00 26.00 27.00 28.00 29.00 30.00

Revenue(000s) 10967.20 13320.25 13600.80 17150.75 18597.00 20630.45 21854.00

www.sylvania.com

We do not reinvent the wheel we reinvent light.
Fascinating lighting offers an infinite spectrum of possibilities: Innovative technologies and new markets provide both opportunities and challenges. An environment in which your expertise is in high demand. Enjoy the supportive working atmosphere within our global group and benefit from international career paths. Implement sustainable ideas in close cooperation with other specialists and contribute to influencing our future. Come and join us in reinventing light every day.

Light is OSRAM

63
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Chapter 2


Figure 2.15

The plot of the data in Figure 2.15, suggests that the ratings generated revenue, has a cubic relationship between the independent variable (ratings) and the dependent variable (revenue). We can test this through a simple worked calculation. First construct a new data table where – as we have assumed a cubic relationship, we construct square and cubic calculations of the independent variable (i.e. (ratings)2 and (ratings)3)). Thus:

64
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

Ratings -squared

Ratings -squared

Ratings -squared

Ratings -cubed

Ratings -cubed

Ratings -cubed

Revenue(000s)

Revenue(000s)

Revenue(000s)

-30 -29 -28 -28 -26 -25 -24 -23 -22 -21 -20 -19 -19 -17 -16 -15 -14 -10

900 841 784 784 676 625 576 529 484 441 400 361 361 289 256 225 196 100

-27000 -24389 -21952 -21952 -17576 -15625 -13824 -12167 -10648 -9261 -8000 -6859 -6859 -4913 -4096 -3375 -2744 -1000

-12971 -11695 -11108 -10738 -7731 -7100 -6003 -4123 -3919 -3820 -4666 -3119 -3953 -1917 -2126 -1284 325 -1185

-12 -11 -10 -9 -8 -7 -6 -5 -4 -3 -3 0 0 1 2 2 4 5

144 121 100 81 64 49 36 25 16 9 9 0 0 1 4 4 16 25

-1728 -1331 -1000 -729 -512 -343 -216 -125 -64 -27 -27 0 0 1 8 8 64 125

-1399 -1153 -515 -1136 -279 1020 -805 -177 101 1158 -731 1317 1553 1261 1115 -1272 1175 107

7 7 9 9 11 11 12 15 15 15 16 17 18 19 20 21 22 23

1114 1502 1823 2106 253 416 2723 3119 4016 4723 5249 3277 5066 6831 6369 7509 9486 11732

24 25 26 27 28 29 30

576 625 676 729 784 841 900

13824 15625 17576 19683 21952 24389 27000

10967 13320 13601 17151 18597 20630 21854

Then- select the Data Analysis Add-In, and select ‘Regression’. Populate the Y range with the revenue data and the X range with ALL the ratings data (i.e. make sure the ratings, ratings2 and ratings3 data are placed in adjacent columns and select them all). Tick the boxes for residuals and residual line plots.

65
Download free eBooks at bookboon.com

Revenue(000s)

Rating

Rating

Rating

Rating

Effective Management Decision Making

Chapter 2

The relevant output from the statistical calculation is as below:

66
Download free eBooks at bookboon.com

Effective Management Decision Making Reading from the presented data, we have generated a best fit equation of: Revenue = 155.72+26.25(ratings)+4.62(ratings)2+0.62(ratings)3 (where X variable 1= rating, X variable 2= (ratings)2 and X variable 3 =(ratings)3)

Chapter 2

For the given data, this generates a R2 Pearson value of 98.6% (i.e. that we have explained all except a computed 1.4% of the variance observed in the data). Hence for a rating of 5.6, we should see a revenue generated of 556.48 (in 000s). From an operational perspective, providing you set up the correct data for the X (independent data values), you can use this regression function to determine the best fit polynomial equation and hence support future forecasting. This approach and use of the Data Analysis Excel Add-in can also be applied to the data relationships where you argue there are power equations or logarithm relationships. For example, the earlier equations can be expanded and the linear regression methodology used – so that:

At Navigant, there is no limit to the impact you can have. As you envision your future and all the wonderful rewards your exceptional talents will bring, we offer this simple guiding principle: It’s not what we do. It’s how we do it.

Impact matters.
navigant.com

©2013 Navigant Consulting, Inc. All rights reserved. Navigant Consulting is not a certified public accounting firm and does not provide audit, attest, or public accounting services. See navigant.com/licensing for a complete listing of private investigator licenses.

67
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making y=a.bx Log(y)=Log(a)+x.Log(b) Which is a linear equation if we assume: Y=Log(y), A=Log(a) and B=Log(b) X is untransformed y=a.xb Log(y)=Log(a)+b.Log(x) Which is a linear equation if we assume: Y=Log(y), A=Log(a) and X=Log(x) b is untransformed y= a.ebx LN(y)=LN(a)+LNebx LN(y)=LN(a)+bx Which is a linear equation if we assume: Y=LN(y), A=LN(a) X and b are untransformed y=e-bx LN(y)=LN(a)-LNebx LN(y)=LN(a)-bx Which is a linear equation if we assume: Y=LN(y), A=LN(a) Hence X and b are untransformed One simple example follows to illustrate this.

Chapter 2

Question: You’re the head gardener for a local horticultural organisation. As part of your job - you have the key decision about when to harvest a ‘crop’ for sale through your distribution network (garden centres). This usually means packaging plants 10% before they reach their optimum sales height. You have been monitoring the growth rate of Crop A, and need to decide when to package the crop. What is your recommendation - which week would you pick the crop?

68
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

Week x 1 2 3 4 5 6 7 8 9 10 11

Height (m) y 0.00 2.65 3.12 3.70 4.54 5.23 6.23 7.43 8.78 10.25 12.23


We expect the relationship between time and growth height to be non linear (it is after all, an organic process). Hence, we consider the non-linear curves presented and select a curve which rises and then levels off (i.e. the plants will stop growing at a typical averaged height). Y=a.xb This is expanded and the logarithms of x and y determined (see table 2.5) and entered into the relevant cells of the regression function. The output from the analysis then identifies b (which is not transformed in this calculation) and A – which does need to be transformed to find a. This generates the forecasting equation: Y=1.074.x0.9505

69
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

Week 1 2 3 4 5 6 7 8 9 10 11

height(cm) 0.00 2.65 3.12 3.70 4.54 5.23 6.23 7.43 8.78 10.25 12.23

log (x) 0.00 0.30 0.48 0.60 0.70 0.78 0.85 0.90 0.95 1.00 1.04
Table 2.5

log(y) 0.00 0.42 0.49 0.57 0.66 0.72 0.79 0.87 0.94 1.01 1.09

Trend 0.00 2.71 3.17 3.73 4.53 5.18 6.11 7.23 8.47 9.82 11.61

The trend forecast is then given in Table 2.5 and the statistical analysis has confirmed a Pearson’s value of 97%. It would be a simple matter to forecast further (increasing x) to determine the height forecasts and determine when the growth rate fell below 10% (i.e. the difference between the next and previous forecast). Whilst this modelled curve has been derived correctly – there is an obvious error in its use – do you see it?

Do you have to be a banker to work in investment banking?
Agile minds value ideas as well as experience Global Graduate Programs
Ours is a complex, fast-moving, global business. There’s no time for traditional thinking, and no space for complacency. Instead, we believe that success comes from many perspectives — and that an inclusive workforce goes hand in hand with delivering innovative solutions for our clients. It’s why we employ 135 different nationalities. It’s why we’ve taken proactive steps to increase female representation at the highest levels. And it’s just one of the reasons why you’ll find the working culture here so refreshing. Discover something different at db.com/careers

Deutsche Bank db.com/careers

70
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Chapter 2

2.8	

Multiple regression and partial regression analysis

Earlier in this chapter, polynomial regression was discussed through the excel data analysis toolpak add in. In one sense of the word, this was multiple regression in that there were a range of potential additional independent variables identified as contributing to the trend observed in the dependent data. However, in that instance those additional independent variables all had some relationship to each other (i.e. square, cubic etc). We can identify that this process of analysis if part of a wider generic approach of defining potential data relationships as: Y (dependent data) = (A)+ (variable 1)x1+ (Variable 2)x2+(variable 3)x3+(variable 4)x4.... •	 Where A = intercept (otherwise known as the constant or underlying trend (U)) •	 Variable 1,2,3 etc refer to the coefficients of the independent data (xi). These are called the partial regression coefficients. A partial regression coefficient is a label used to described a relationship in the data where for example it is the expected change in Y for one increase in the variable (coefficient) of x1, when all the other xi variables are held constant (Malhotra, 2002). This then applies to all the other values of xi. Let’s consider an example using the following data concerned with understanding student attitudes towards their examination grades in ‘management decision making’:
Respondent No. 1 2 3 4 5 6 7 8 9 10 11 12 Attitude towards effort 5 4 6 7 6 4 8 6 5 3 4 9 Attitude towards class time 8 8 7 8 9 5 6 9 9 2 10 10 Attitude towards intelligence 9 9 8 6 5 6 7 8 7 9 9 9

Grade 65 63 60 55 55 54 59 68 62 78 70 65

Table 2.6: Respondent data on the importance of 3 independent factors shaping examination grades (factors scaled from 0 (not important) – 10 (very important).

The analysis of this problem requires (ideally) specific software, or a developed Excel ‘Add in’. A number are freely available on the www for download (for trial). In short, the solution needs to be found for the following series of mathematical relationships:

71
Download free eBooks at bookboon.com

Effective Management Decision Making Grade (Y) = f (Effort (X1), Class time (X2), Intelligence (X3)) Or: Grade (Y) = A+b1x1+b2x2+b3x3

Chapter 2

Lotfi & Pegels (1996) recommend that for these types of problems, you ideally need to have at least five times the number of observations as there are expected independent variables (i.e. in the case here 15 observations as a minimum). By using the commercial data analysis package XLSTAT, the following calculations were obtained for this problem:
Goodness of fit statistics: Observations Sum of weights DF R² Adjusted R² MSE RMSE MAPE DW Cp AIC SBC PC 12.000 12.000 8.000 0.707 0.598 19.813 4.451 4.886 1.169 4.000 38.970 40.910 0.585

The proposed model, using the three independent variables proposed, explained 71% of the variance in the grade achieved by the students (R2=70.7%). The model parameters were determined to be:
Model parameters: Source Intercept Attitude towards effort Attitude towards class time Attitude towards intelligence Value 41.357 -0.651 -0.471 3.741

Giving a multiple linear regressed solution which stresses the intelligence variable: Grade = 41.357-0.651(EFFORT)-0.471(CLASS TIME)+3.741(INTELLIGENCE)

72
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

2.9	

Complete worked non linear forecasting example with seasonality

We now consider an example, which uses both non linearity and seasonality in its solution. Imagine that the following dataset below shows the average monthly temperature (in degrees Celsius) for the last 41 months in Sao Paulo, Brazil. If we assumed that there were no other variables or external factors affecting the observed data, except the time of the year, can we forecast the temperature for the next year?

We start such a problem by, as usual, plotting the data and trying to understand it, in context. This is shown by figure 2.15 as:

Real drive. Unreal destination.

As an intern, you’re eager to put what you’ve learned to the test. At Ernst & Young, you’ll have the perfect testing ground. There are plenty of real work challenges. Along with real-time feedback from mentors and leaders. You’ll also get to test what you learn. Even better, you’ll get experience to learn where your career may lead. Visit ey.com/internships. See More | Opportunities

© 2012 Ernst & Young LLP. All Rights Reserved.

73
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Chapter 2

Temperature in Sao Paulo
50 45 40 35 30 25 20 15 10 5 0 0 5 10 15 20 25 30 35 40 45 Month

Degrees (Celcius)

Figure 2.15

Whilst it is difficult to be certain which causal forecasting model to attempt to use (i.e. linear or non-linear) to predict the temperature for the next 12 months, the plot provides some indicative guidance. Clearly, an accurate model representation would require you to verify the assumptions in your data through researching the context of the data. For the purposes of this question, we will assume the relationship is non-linear – taking the worst case scenario for carbon dioxide atmospheric warming. Based upon the 3 types of non–linear equations discussed above, we will use the generic equation y (temperature – which you will remember is the dependent variable) = a.e+bx. (where: in this case, as the curve is upward sloping (i.e. the T component), the b gradient value is given a positive sign). Our next step is we need to transform the non linear equation, into a linear format to which we can then apply least squares regression to solve. One method of doing this is to take the natural logarithm of both sides of the chosen modelling equation. Hence: Ln (y) = Ln (a.e+bx) which is equivalent to: Ln (y) = ln (a) + Ln(e+bx) which is equivalent to: Ln (y) = ln (a) + Ln(ex).Ln(b) which is equivalent to: Ln (y) = ln (a) +x. Ln(b) Clearly as we have presented, this latter equation can be viewed as a transformation of the generic linear equation (y=a+bx) if we let: Y=Ln(y)

74
Download free eBooks at bookboon.com

Effective Management Decision Making A=Ln(a) B=Ln(b) Note that the x (independent variable) remains untransformed – so that our transformed linear equation is: Y=A+Bx

Chapter 2

To determine B and A, you need to transform the data: i.e. Table 2.7 shows the transformations of x and y and their squared calculations and sums. These sums can then be inserted into the least square equations (2 and 3).

The stuff you'll need to make a good living

STUDY. PLAY.

The stuff that makes life worth living

NORWAY. YOUR IDEAL STUDY DESTINATION.
WWW.STUDYINNORWAY.NO FACEBOOK.COM/STUDYINNORWAY

75
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Chapter 2

Table 2.7

Alternately you could use excels chart and trend function or the regression function, to determine B and A. Hence from table 2.7:

76
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

B

=

41(2626.12)-(861)(113.83) 41(23821)-(861)(861) and (113.83)(23821)-(861)(2626.12) 41(23821)-(861)(861)

A

=

Hence : B =0.0411 to 4 decimal places) A= 1.9140 (to 4 decimal places) However, as this refers to A and B and we need a and b, we must transform these values: a = exp(A) and b=exp (B) Therefore: b=0.0411 (to 4 decimal places) a= 6.7822 (to 4 decimal places) Therefore the final non-linear equation is given as: y = 6.7822e0.0411x. Hence the forecasts for next 12 months are found by inserting additional values of x into this equation which generates table 2.8 below (but remember this is only the Trend component):

Table 2.8

77
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

If you plot these forecasts with the original data, you notice that whilst the trend (T) and underlying trend (U) seem appropriate, we are still missing out on a seasonal variation (S) in the data forecasts. This is the next step to resolve. Seasonality, as was discussed earlier, can be addressed in two ways - the additive method and the multiplicative method. In both methods you are working out how much difference there is between your trend (T) forecast (in table 2.8) compared with the real data you have got. You then adjust your (T) forecast by a suitable amount at the right period in the seasonality) to end up with a final forecast. Your adjustments are based on the two calculations of: Final forecast = T + S (Additive method) Final forecast = T * S (Multiplicative method) We shall use one of these methods to continue with this worked example.

Graph (f) : Temperature Sau Paulo
70 60
degree celcius

50 40 30 20 10 0 0 n=3,15,27 10 20 30 months 40 50 60 Best fit line

Figure 2.16

By looking at the original graph of the temperature in Sau Paulo over time, we could state that the regular ups and downs observed in this data seem to repeat every 12th data value point (hence the seasonality has a period of 12) and that these changes in every 12th point, appear to be more or less the same when compared with a rising trend (T). For example, from figure 2.16 the 3rd data value point (n=3), just after the bottom of the seasonal cycle seems to always be a little bit less than the trend (T) value for this time. Hence the temperature for each March month (n=3, 15, 27, 39) always seems to be the same distance below the trend line (T).

78
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

We therefore need to modify the forecast values we have determined for (T) above in table 2.8, by the amount of difference for each point in the seasonal cycle. This is worked out in this example by the simple Additive calculation of (Actual Forecast (T)). Hence in table 2.9 below, the difference between the actual data observed and the trend forecast is determined. As this cycle of change in the data repeats itself with a period of 12, we can then take the average of the differences for the same points in the cycle. This is shown by the column in table 2.9 headed ‘mean difference’. Hence the first value is the mean of the values (-4.07, -6.57,-10.95,-3.03). Your final forecast is then the addition of the (T) forecast with the seasonal (S) modification. Hence the first final forecast value is found to be (T)+(S), which equals: Final forecast for January (n=1) = 7.07+ (-6.15) = 0.91 If you repeat this calculation for all your months, including your new forecasts, you end up with table 2.9 and figure 2.17.

I joined MITAS because I wanted real responsibili� I joined MITAS because I wanted real responsibili�

Maersk.com/Mitas www.discovermitas.com

�e Graduate Programme for Engineers and Geoscientists

� for Engin

M

Real work International Internationa al opportunities �ree wo work or placements

Month 16 I was a construction M supervisor ina cons I was the North Sea supe advising and the N he helping foremen advis s solve problems Real work he helping International Internationa al opportunities �ree wo work or placements s solve p
Click on the ad to read more

79
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

Table 2.9

80
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

Graph (g):Forecast and Actual temperature Sau Paulo
70 60
Degree

Original data

50 40 30 20 10 0 0 10 20 30 Month 40 Best fit line 50 60 Final forecast

Figure 2.17

Figure 2.17 therefore shows a good fit between your final forecasts and the original data, suggesting for future forecasts are likely to be appropriate and accurate. To determine how accurate your forecasts are, you now need to determine the errors in them. This is examined below. Note - As an aside, if you were to apply the multiplicative method to the (T) forecasts in this example, rather than work out (Actual – forecast (T)) as above, you instead determine: Actual *100% Forecast (T)

Seasonal proportional change =

Hence as with the calculations above, you end up with a proportional change (a % change needing to be made to each (T) forecast) so as to end up with the final forecast needed. This method therefore allows you consider increasing or decreasing rates of change in the seasonality affecting observed data. Mean Square Error (MSE) – This is the summed and squared difference between your forecasts and what data you have observed. It has the form of: Σ (ye – y)2 n

MSE

=

where n = number of data values of y you have It is a better measurement of goodness in your forecasts, as it eliminates negative values in the difference between your forecast and the observed data.

81
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

Mean Absolute Deviation (MAD) – This is the absolute integer difference between your forecasts and the data you have observed. It has the form of: Σ |‌ye – y|‌ n

MAD

=

where n = number of data values of y that you have The straight bars on either side of the equation denote just take the absolute difference (ignoring any negative signs)). It is a better measurement of goodness in your forecasts, as it eliminates negative values in the difference between your forecast and the observed data. If we apply the MSE calculation to the worked example in table 2.9, we need to calculate the sum of the differences between our forecasts and the original data, then divide this by the number of data values we have (n=41). Hence: MSE = 566.55 / 41 MSE = 13.82 (to 2 decimal points) Hence taking the square root of this value, each forecast value is, on average, within 3.17 degrees of the original data. In terms of calculating the coefficient of determination: Σ (ye – y)2 Σ (y - y)2

R2

=

Using the data in table 2.9, we can find this to be: 4611.709 5596.76

R2 =

R2 =0.8239 In other words, the model used has accounted for approximately 83% of observed variance in the data, representing a good fit with the data.

82
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

2.10	Summary
This chapter has built upon the RAT models outlined in Chapter 1 and presented varying methods of predicting future data values – firstly through projective forecasting (concerned with the actual observed data values) and second through causal forecasting (concerned with the relationships between the data values). Decisions about the choices between the methods discussed are based upon confidence in the available data, the longevity of assumptions regarding the trends in the observed historic data and the need for both relevant and realistic complexity in the model to reflect business needs, but also to ensure the model is sufficient simple as to not generate additional costs in its implementation (see chapter 3 for a fuller discussion of practical issues associated with modelling). In the next chapter, the concern with identifying appropriate measured data to reflect the manager’s needs for decision making analysis is considered further, especially when that data is unavailable or only available with a degree of uncertainty. Hence, the focus moves to decision making methods concerned with probabilistic forecasting.

2.11	

Key Terms and glossary

Data forecasting – using historical organisational data to predict future organisational data Projective forecasting – using the actual numerical historical data to predict future organisational data Causal forecasting – using the relationships between the actual numerical historical data to predict future organisational data

Need help with your dissertation?
Get in-depth feedback & advice from experts in your topic area. Find out what you can do to improve the quality of your dissertation!

Get Help Now

Go to www.helpmyassignment.co.uk for more info

83
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Chapter 2

Multiple regression – using more than one independent variable to help identify relationships between historic data Data Smoothing – using a weighting method to bias the combination of most recent forecast and most recent historic data, to predict future organisational data Holts Method (LES) - using a weighting method with a single constant, to bias the combination of most recent forecast and most recent historic data, to predict future organisational data Holts-Winter Method- using a weighting method with two constants, to bias the combination of most recent forecast and most recent historic data and account for a trend in the historic data, to predict future organisational data Classical Data Decomposition – using a relationship method between historic data to predict future organisational data (that there are Trend, Underlying Trend, Seasonality and Random noise relationships in the historic data). Additive model of decomposition - using a relationship method between historic data to predict future organisational data (that there are Trend, Underlying Trend, Seasonality and Random noise relationships in the historic data – and that these affect future forecasts in an absolute manner (i.e. a constant series of increments are added to forecasts)). Multiplicative model of decomposition- using a relationship method between historic data to predict future organisational data (that there are Trend, Underlying Trend, Seasonality and Random noise relationships in the historic data – and that these affect future forecasts in a proportional manner (i.e. a percentage of the value of the data is added to forecasts)).

End of chapter questions
Work through these questions at your leisure. Most of the answers and solutions are presented. Question 1- Part 1a) As Assistant Manager of ‘Handmade Ltd’ – a clothing retailer in Gloucester (UK), your store was affected by the recent floods in the Summer of 2007. As a result, a significant portion of stock was water damaged and is not saleable and the store was closed from July 2007 – through to September 2007 inclusive. Your organisation is now in dispute with InsurCORP Ltd, the insurance company dealing with your claim, concerning the amount of lost sales during the time the store was closed. Two key issues must be resolved: 1)	 The amount of sales Handmade could have expected if the floods had not occurred 2)	 What (if any) compensation is due for excess sales from increased business activity after the floods This latter point is important as £0.3 billion was allocated by the national government to emergency aid to Gloucester and its surrounding region, in the aftermath of the floods. This would have partially resulted in increased sales at stores like Handmade and others, as people started to replace lost/damaged goods.

84
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

Table 1 below shows the sales data for the 46 months preceding the floods. The UK Department of Trade and Industry, has also published data for all stores in the town, as well as sales in the county, for the months that the store was closed. Table 2 shows this data. Use these data tables to determine estimates of the lost sales at Handmade for the months the store was closed. Is there a case to be made for excess flood related sales? If such a case can be made, Handmade is entitled to compensation for excess sales it would have earned in addition to ordinary sales. Prepare a report for the management of Handmade, that summarizes your findings, forecasts and recommendations. Include the following: 1.	 An estimate of sales had there been no flood and an estimate of county wide sales had there been no flood (50%) 2.	 Using the countywide actual stores sales for the flooded months and the estimate in (1), make a case for or against excess flood related sales (20%) 3.	 An estimate of the lost sales for Handmade for the flooded months (20%) 4.	 Include a commentary on what the residuals and errors tell you about the nature of this business activity and the information you can extract from this analysis (10%)
Table 1 : Sales for Handmade Ltd from Sept 2003 through to June 2007.

Month Jan Feb March April May June July August Sept Oct Nov Dec

2003                 1.71 1.90 2.74 4.20

2004 1.45 1.80 2.03 1.99 2.32 2.20 2.13 2.43 1.90 2.13 2.56 4.16

2005 2.31 1.89 2.02 2.23 2.39 2.14 2.27 2.21 1.89 2.29 2.83 4.04

2006 2.31 1.99 2.42 2.45 2.57 2.42 2.40 2.50 2.09 2.54 2.97 4.35

2007 2.56 2.28 2.69 2.48 2.73 2.37            

85
Download free eBooks at bookboon.com

Effective Management Decision Making
Table 2: Sales in stores in the country, Sept 2003 through to June 2007

Chapter 2

Month Jan Feb March April May June July August Sept Oct Nov Dec

2003

2004 46.80 48.00 60.00 57.60 61.80 58.20 56.40 63.00

2005 46.80 48.60 59.40 58.20 60.60 55.20 51.00 58.80 49.80 54.60 65.40 102.00

2006 43.80 45.60 57.60 53.40 56.40 52.80 54.00 60.60 47.40 54.60 67.80 100.20

2007 48.00 51.60 57.60 58.20 60.00 57.00

55.80 56.40 71.40 117.60

57.60 53.40 71.40 114.00

Note: You can assume random and cyclical variations in the wage level forecasts are negligible

Question 1: Part 1b) The government is concerned about the amount of aid to continue to give to the town and county after the immediate flood situation has been resolved. To that end, they are considering the employment levels in the county as an indicator of the health and development of the county’s economic and social infrastructure. Data obtained on employment levels for the county is as follows:
Table 3: Employed in the county of Gloucestershire (UK)

Year 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007

employed 114968 117342 118793 117718 118492 120259 123060 124900 126708 129558 131488 133488 134337

86
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

1)	 Using a suitable regression equation, determine an accurate model of this trend and support your arguments (Note – you may use the regression function on Excel / MINITAB or other software to aid you) (50%) 2)	 If you were to improve the model further (and to account for more dependent variables), what factors might you consider and why? (50%) Part 2 (100% total marks contribution to this part of Assessment 1) Answer: A plot of the data suggests an additive causal model is appropriate (and fits with the context of the data). For the first part of the question, the answer would be:
Absolute Error in Trend forecast

Averaged errors in forecast for N periods -0.40 -0.09 0.45 1.86 -0.19 -0.37 -0.08 -0.09 0.11 -0.12 -0.06 0.02 -0.40 -0.09 0.45 1.86 -0.19 -0.37 -0.08 -0.09 0.11 -0.12

Trend forecast

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22

Sept Oct Nov Dec Jan Feb March April May June July August Sept Oct Nov Dec Jan Feb March April May June

1.71 1.90 2.74 4.20 1.45 1.80 2.03 1.99 2.32 2.20 2.13 2.43 1.90 2.13 2.56 4.16 2.31 1.89 2.02 2.23 2.39 2.14

2.09 2.10 2.12 2.13 2.14 2.15 2.16 2.17 2.18 2.20 2.21 2.22 2.23 2.24 2.25 2.26 2.28 2.29 2.30 2.31 2.32 2.33

-0.38 -0.20 0.62 2.07 -0.69 -0.35 -0.13 -0.18 0.14 0.00 -0.08 0.21 -0.33 -0.11 0.31 1.90 0.03 -0.40 -0.28 -0.08 0.07 -0.19

87
Download free eBooks at bookboon.com

Final Forecast (T+S) 1.69 2.01 2.57 3.98 1.95 1.78 2.08 2.08 2.30 2.08 2.15 2.24 1.83 2.15 2.71 4.12 2.09 1.92 2.22 2.22 2.43 2.21

Quarter

Month

Sales

Effective Management Decision Making

Chapter 2

23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58

July August Sept Oct Nov Dec Jan Feb March April May June July August Sept Oct Nov Dec Jan Feb March April May June July August Sept Oct Nov Dec Jan Feb March April May June

2.27 2.21 1.89 2.29 2.83 4.04 2.31 1.99 2.42 2.45 2.57 2.42 2.40 2.50 2.09 2.54 2.97 4.35 2.56 2.28 2.69 2.48 2.73 2.37

2.34 2.36 2.37 2.38 2.39 2.40 2.41 2.42 2.44 2.45 2.46 2.47 2.48 2.49 2.50 2.51 2.53 2.54 2.55 2.56 2.57 2.58 2.59 2.61 2.62 2.63 2.64 2.65 2.66 2.67 2.69 2.70 2.71 2.72 2.73 2.74

-0.07 -0.15 -0.48 -0.09 0.44 1.64 -0.10 -0.43 -0.02 0.00 0.11 -0.05 -0.08 0.01 -0.41 0.03 0.44 1.81 0.01 -0.28 0.12 -0.10 0.14 -0.24

-0.06 0.02 -0.40 -0.09 0.45 1.86 -0.19 -0.37 -0.08 -0.09 0.11 -0.12 -0.06 0.02 -0.40 -0.09 0.45 1.86 -0.19 -0.37 -0.08 -0.09 0.11 -0.12 -0.06 0.02 -0.40 -0.09 0.45 1.86 -0.19 -0.37 -0.08 -0.09 0.11 -0.12

2.29 2.37 1.97 2.28 2.84 4.26 2.23 2.06 2.36 2.36 2.57 2.35 2.42 2.51 2.10 2.42 2.98 4.39 2.36 2.20 2.50 2.49 2.71 2.49 2.56 2.65 2.24 2.56 3.12 4.53 2.50 2.33 2.63 2.63 2.84 2.62

88
Download free eBooks at bookboon.com

Effective Management Decision Making And for the errors and confidence in the forecasts:
(y-mean y) squared (yf -mean y) squared

Chapter 2

x (month)

Sept Oct Nov Dec Jan Feb March April May June July August Sept Oct Nov Dec Jan Feb March April May June July August Sept Oct Nov Dec Jan Feb March April

1.71 1.90 2.74 4.20 1.45 1.80 2.03 1.99 2.32 2.20 2.13 2.43 1.90 2.13 2.56 4.16 2.31 1.89 2.02 2.23 2.39 2.14 2.27 2.21 1.89 2.29 2.83 4.04 2.31 1.99 2.42 2.45

forecast y (T + S) 1.69 2.01 2.57 3.98 1.95 1.78 2.08 2.08 2.30 2.08 2.15 2.24 1.83 2.15 2.71 4.12 2.09 1.92 2.22 2.22 2.43 2.21 2.29 2.37 1.97 2.28 2.84 4.26 2.23 2.06 2.36 2.36

y (Sales)

0.53 0.29 0.09 3.09 0.98 0.41 0.17 0.20 0.01 0.06 0.10 0.00 0.29 0.10 0.01 2.96 0.02 0.30 0.18 0.04 0.00 0.09 0.03 0.05 0.30 0.02 0.15 2.56 0.02 0.20 0.00 0.00

0.56 0.19 0.02 2.38 0.24 0.43 0.13 0.13 0.02 0.13 0.09 0.04 0.37 0.09 0.07 2.82 0.12 0.27 0.05 0.05 0.00 0.05 0.02 0.00 0.23 0.02 0.16 3.29 0.05 0.15 0.01 0.01

89
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

May June July August Sept Oct Nov Dec Jan Feb March April May June July August

2.57 2.42 2.40 2.50 2.09 2.54 2.97 4.35 2.56 2.28 2.69 2.48 2.73 2.37

2.57 2.35 2.42 2.51 2.10 2.42 2.98 4.39 2.36 2.20 2.50 2.49 2.71 2.49  

0.02 0.00 0.00 0.00 0.12 0.01 0.28 3.64 0.01 0.03 0.06 0.00 0.08 0.01 17.55

0.02 0.01 0.00 0.00 0.11 0.00 0.29 3.81 0.01 0.06 0.00 0.00 0.07 0.00 16.57

 

Standard error of estimate=

0.13

Coefficient of determination=

0.9925

Coefficient of correlation=

0.9963

SUM (sales) MEAN

112.28 2.44087 R squared = 1.06

For the second part of the question, the answer would be: a)	 Focus on appropriateness of model chosen - given the context b)	 Focus on accuracy of model chosen - given the context - better is QUADRATIC hence the HINT in the question c)	 There is scope her for a detailed narrative of economic cycles and patterns too Quadratic equation generated = 72.3x2+650x+114781 Linear equation generated = 1672x+112227 Improving model ? you can add variables, look for other models to use.

90
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

4XDGUDWLF

<HDUE\UHIHUHQFH

<HDUVTXDUHG

HPSOR\HGVTXDUHG

)RUHFDVWV

                

                

                

            

            

                

                

91
Download free eBooks at bookboon.com

Click on the ad to read more

)RUHFDVWV

<HDU

HPSOR\HG

/LQHDU

Effective Management Decision Making The forecasts were made using Excel’s Data Analysis tool: The forecasts were made using Excel’s Data Analysis tool: Quadratic Output Test

Chapter 2

6800$5<287387 5HJUHVVLRQ6WDWLVWLFV 0XOWLSOH5 56TXDUH $GMXVWHG5 6TXDUH 6WDQGDUG(UURU 2EVHUYDWLRQV
$129$  5HJUHVVLRQ 5HVLGXDO 7RWDO GI   

    

6LJQLILFDQFH 66 06 ) ) (   ( (  (    6WDQGDUG (UURU   

 ,QWHUFHSW ;9DULDEOH ;9DULDEOH

&RHIILFLHQWV   

W6WDW 3YDOXH /RZHU 8SSHU /RZHU  (              

92
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

Linear output Test:

SUMMARY OUTPUT

Regression Statistics Multiple R R Square Adjusted R Square Standard Error Observations 0.980186744 0.960766052 0.95719933 1374.410159 13

ANOVA   Regression Residual Total 1 11 12 df SS 508839760.9 20779036.15 529618797.1 MS 5.09E+08 1889003       F 269.3694 Significance F 4.4E-09

  Intercept X Variable 1

Coefficients 112227.1154 1672.071429

Standard Error 808.6320257 101.8780591

t Stat 138.7864 16.41248

P-value 3.4E-19 4.4E-09

Lower 95% 110447.3 1447.839

Upper 95% 114006.9 1896.304

Lower 95.0% 110447.3 1447.839

Question 2: The supervisor of a manufacturing business believes that the assembly line speed (in m/minute) is determining the number of defective parts found during on line inspections of the finished products. To test this theory, management had the same batch of parts inspected visually at a variety of line speeds. The following data were collected.
Line speed (m/minute) 20 20 40 30 60 40 Number of defective parts found 21 19 15 16 14 17

93
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 2

a)	 Develop an estimated regression equation that relates line speed to the number of defective parts found. This solution must be presented in the longhand format. (80%) b)	 Use the proposed model to forecast the number of defective parts found for a line speed of 50 m/minute (20%) In your answers, ensure you provide a clear rationale for the model(s) you choose, that you discuss the errors and residuals in your forecast model and that you provide clear confidence values (Pearson’s coefficient) with an explanation. Answer: Solutions can be reached either by longhand OLS analysis – or excel trend line fitting. A longhand presentation is required for part (a). The solutions for this problem are given below.

Chart Title 25

20

15

10

y = -0.1478x + 22.174 R2 = 0.7391

5

0 0 10 20 30 40 50 60 70

Apply the equation generated (see above) in part b to give a failure rate of approx. 15 parts.

94
Download free eBooks at bookboon.com

Effective Management Decision Making Question 3:

Chapter 2

A pair of entrepreneurs have undertaken some competitive research on their market and have presented to you the following data of relative sales/competitor depending upon competitor concentration:
Number of competitors within 1 km 1 1 2 3 3 4 5 5 6 Estimated sales (daily) £ 3600 3300 3100 2900 2700 2500 2300 2000 1500

a)	 Develop an appropriate model of the relationship between the daily sales volume to the number of competitors within a 1 km radius of the proposed location for the entrepreneurial venture. This solution must be presented in the longhand format. (30%) b)	 Use the proposed model to forecast the daily sales volume for a revised proposed location which would have four competitors within a one mile radius and when there are seven competitors within a one mile radius. (10%)

Brain power

By 2020, wind could provide one-tenth of our planet’s electricity needs. Already today, SKF’s innovative knowhow is crucial to running a large proportion of the world’s wind turbines. Up to 25 % of the generating costs relate to maintenance. These can be reduced dramatically thanks to our systems for on-line condition monitoring and automatic lubrication. We help make it more economical to create cleaner, cheaper energy out of thin air. By sharing our experience, expertise, and creativity, industries can boost performance beyond expectations. Therefore we need the best employees who can meet this challenge!

The Power of Knowledge Engineering

Plug into The Power of Knowledge Engineering. Visit us at www.skf.com/knowledge

95
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Chapter 2

c)	 Now consider the total sales achieved within the 6 km radius of the proposed location and determine an appropriate regression model to identify what the estimated sales (daily) would be when a 7th competitor set up in the market. (40%) d)	 Given your answers for (b) and (c), and that you have learned that the forecasted daily sales breakeven for the proposed entrepreneurial venture is £1050, propose a clear and reasoned business strategy to the entrepreneurs in order to increase the potential to secure a higher daily sales level than these average forecasts (20%) In your answers for both (a),(b) and (c) ensure you provide a clear rationale for the model(s) you choose, that you discuss the errors and residuals in your forecast model and that you provide clear confidence values (Pearson’s coefficient) with an explanation. Answer: Solutions can be reached either by longhand OLS analysis – excel trend line fitting for both (a) and (c). A longhand presentation is required for part (a).If (c) is undertaken care must be taken to accommodate and interpret excel regression equations. The solutions for this problem are given below are (c) for brevity:


Apply equation from (a) = y=3843-356.4x when x=1 then y(estimated sales)=3846.6 and when x=7, y=1348.2 For part (c) – the focus is on the total sales/outlet when 7 competitors set up within a one mile radius. This gives a value of £7593. This gives a mean daily sales volume/outlet = £1136.29 For part (d) the question is open to reflect on the scope of strategy marketing factors chosen to consider options for developing a USP (and reflect on the strategic options) available to the start up business.

96
Download free eBooks at bookboon.com

Effective Management Decision Making Question 4:

Chapter 2

As the director of research and development at Fruhaga Ltd, you are concerned with the rate of innovation in your department - in particular process innovation. You have gathered the following data based on the rate of process innovations / week In order to determine whether to continue to support investment in process innovations, you need to know what the likely rate of innovations will be in the 14th week following a product launch (as this may determine if you continue to fund and support the projects)?

1.00 2.00 3.00 4.00 5.00 6.00 7.00 8.00 9.00 10.00

20.04 21.56 22.28 22.67 22.56 22.89 23.34 23.68 23.40 23.59

0.20 0.24 0.35 0.45 0.54 0.65 1.00 1.10 1.45 1.84

Answer: The focus of this question is to know that innovation in organisations has been observed to evidence non linear relationships in general. Hence non linear models are more appropriate to be chosen – projects A,B and C are evaluated and presented below.

97
Download free eBooks at bookboon.com

Project C Innovations/ week 10.45 10.89 11.23 11.87 12.34 13.04 13.56 14.02 14.67 15.06

Project A innovations/ week

Project B innovations/ week

x (week)

Effective Management Decision Making

Chapter 2

PROJE

x 1.00 2.00 3.00 4.00 5.00 6.00 7.00 8.00 9.00 10.00

revised y 20.04 21.56 22.28 22.67 22.56 22.89 23.34 23.68 23.40 23.59 226.01 22.60 B= A=

ln y 3.00 3.07 3.10 3.12 3.12 3.13 3.15 3.16 3.15 3.16 31.17 3.12 0.01483 3.03526

xlny 3.00 6.14 9.31 12.48 15.58 18.78 22.05 25.32 28.37 31.61 172.65 17.27

square regressed x forecast 1.00 4.00 9.00 16.00 25.00 36.00 49.00 64.00 81.00 100.00 385.00 38.50 21.12 21.43 21.75 22.08 22.41 22.74 23.08 23.43 23.78 24.13 24.49 24.86 25.23 25.61

% diff 94.90 100.59 102.42 102.68 100.68 100.64 101.11 101.07 98.41 97.75

means 97.99 99.66 101.77 101.88 97.99 99.66 101.77 101.88 97.99 99.66 101.77 101.88 97.99 99.66

Full forecast 20.69 21.36 22.14 22.49 21.96 22.67 23.49 23.87 23.30 24.05 24.93 25.33 24.73 25.52

x 1.00 2.00 3.00 4.00 5.00 6.00 7.00 8.00 9.00 10.00 11 12 13 14

(ye-mean y) squared 3.64 1.54 0.21 0.01 0.41 0.00 0.79 1.61 0.49 2.11 10.81      

(y-ymean) squared 6.56 1.08 0.10 0.00 0.00 0.08 0.55 1.16 0.64 0.98 11.16

x 1.00 2.00 3.00 4.00 5.00 6.00 7.00 8.00 9.00 10.00 11 12 13 14

SUMS 55.00 Means 5.50

transform a and b R squared= Hence : b= a= 1.01495 20.8065 0.968638452

Challenge the way we run

EXPERIENCE THE POWER OF FULL ENGAGEMENT… RUN FASTER. RUN LONGER.. RUN EASIER…
1349906_A6_4+0.indd 1

READ MORE & PRE-ORDER TODAY WWW.GAITEYE.COM

98
Download free eBooks at bookboon.com

22-08-2014 12:56:57

Click on the ad to read more

Effective Management Decision Making

Chapter 2

x PROJECT B

revised y 0.20 0.24 0.35 0.45 0.54 0.65 1.00 1.10 1.45 1.84 7.82 0.78 B= A=

ln y

xlny

square regressed % diff means x forecast 1.00 4.00 9.00 16.00 25.00 36.00 49.00 64.00 81.00 100.00 385.00 38.50 0.20 0.26 0.33 0.42 0.54 0.70 0.89 1.14 1.46 1.88 2.40 3.08 3.94 5.05 99.08 92.81 105.65 106.03 99.32 93.32 112.07 96.23 99.02 98.08 99.14 94.74 108.86 101.13 97.99 99.66 101.77 101.88 97.99 99.66 101.77 101.88 97.99 99.66

Full forecast 0.20 0.24 0.36 0.43 0.53 0.69 0.91 1.16 1.43 1.87 2.45 3.14 3.86 5.04

x

(ye(y-ymean) mean y) squared squared 0.34 0.29 0.18 0.12 0.06 0.01 0.02 0.15 0.43 1.18 2.77       0.34 0.29 0.19 0.11 0.06 0.02 0.05 0.10 0.45 1.12 2.72

x

1.00 2.00 3.00 4.00 5.00 6.00 7.00 8.00 9.00 10.00

-1.61 -1.43 -1.05 -0.80 -0.62 -0.43 0.00 0.10 0.37 0.61 -4.86 -0.49 0.2476981 -1.8478607

-1.61 -2.85 -3.15 -3.19 -3.08 -2.58 0.00 0.76 3.34 6.10 -6.27 -0.63

1.00 2.00 3.00 4.00 5.00 6.00 7.00 8.00 9.00 10.00 11 12 13 14

1.00 2.00 3.00 4.00 5.00 6.00 7.00 8.00 9.00 10.00 11 12 13 14

SUMS Means

55.00 5.50

transform a and b R squared= Hence : b= a= 1.2810731 0.1575739 1.018657838

x 1.00 PROJECT 2.00 C 3.00 4.00 5.00 6.00 7.00 8.00 9.00 SUMS Means

revised ln y y 10.45 10.89 11.23 11.87 12.34 13.04 13.56 14.02 14.67 2.35 2.39 2.42 2.47 2.51 2.57 2.61 2.64 2.69 2.71 25.35 2.54 0.0420525 2.3040485

xlny 2.35 4.78 7.26 9.90 12.56 15.41 18.25 21.12 24.17 27.12 142.91 14.29

square regressed % diff x forecast 1.00 4.00 9.00 16.00 25.00 36.00 49.00 64.00 81.00 100.00 385.00 38.50 10.44 10.89 11.36 11.85 12.36 12.89 13.44 14.02 14.62 15.25 15.90 16.59 17.30 18.04 100.05 99.97 98.84 100.18 99.85 101.17 100.87 100.00 100.33 98.75

means 100.08 99.97 99.86 100.09 97.99 99.66 101.77 101.88 97.99 99.66 101.77 101.88 97.99 99.66

Full forecast 10.45 10.89 11.35 11.86 12.11 12.85 13.68 14.28 14.33 15.20 16.19 16.90 16.95 17.98

x 1.00 2.00 3.00 4.00 5.00 6.00 7.00 8.00 9.00 10.00 11 12 13 14

(ye(y-ymean) mean y) x squared squared 5.11 3.32 1.87 0.73 0.36 0.02 0.93 2.46 2.61 6.18 23.60       5.12 3.32 2.20 0.71 0.14 0.11 0.72 1.71 3.83 5.51 23.36 1.00 2.00 3.00 4.00 5.00 6.00 7.00 8.00 9.00 10.00 11 12 13 14

10.00 15.06 55.00 127.13 5.50 12.71 B= A=

transform a and b R squared= Hence : b= a= 1.0429493 10.014645 1.0100495

99
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 3

Chapter 3
3.1	 Developing rational models with quantitative methods and analysis: probabilities
It was discussed in chapter 1, that the study and practice of decision making has an inherent duality associated with it, with at one end of this spectrum, positivistic and quantitative methods and analyses dominating (as denoted by RAT models in figure 1.5 of chapter 1). Chapter 2 continued to focus upon measurable data which can be used to construct models of complex processes. However, the availability of measurable data and its reliability is perhaps not as common as managers would wish it was. As a result, we now need to begin to consider decision making methods which are able to include greater levels of uncertainty in that process. In particular, there is a focus upon the individual’s view of that uncertainty (see Chapter 6). This chapter explores some of the more common and simpler methods of quantitative analysis in probability. It is the aim of this chapter that you will develop your practical and transferable skills in this area of mathematical modelling as well as developing cognitive abilities in effective model selection and justification. To achieve this process requires you to think about modelling as a specific decision making activity and set of practical and cognitive skills to develop. Modelling – is the process through which reality can be understood. Models of reality are nothing more than simplifications of reality. They allow the modeller to understand situations and environments cost effectively and with less risk, than taking an actual decision or intervention in that environment. The modeller chooses to focus upon particular views or data to understand something that is occurring or will occur. Usually, the modeller will have to be clear as to why they have chosen one particular view over another (and this is certainly the case for example when students are assessed in their studies by their tutor!). There are four generic forms of modelling: •	 Iconic – where the modeller creates a physical representation of the process or entity to be understood. These might for example be scalar clay representations of cars, or mock ups of a product. •	 Analog models – are also physical in form, but do not physically resemble the object being modelled. Perhaps the most well known example is the use of water to represent the five sector circular flow of money in an economy. •	 Virtual models – follow from analog models, but rather than create physical models, a computer generated model is constructed (say of a bridge for testing stress areas or of a car). •	 Mathematical models - represent real world problems through a system of mathematical formulas and expressions based on key assumptions, estimates, or statistical analyses. This is the form of modelling focused upon in this chapter. For this chapter, we therefore begin a focus upon mathematical and logical models and probabilities or models with higher levels of uncertainty regarding the confidence of data that is available. In the development of such models (and indeed also for qualitative models) to aid decision making, there are three distinct stages: 1)	 Model Development – This is concerned with determining what are the key data upon which we can apply a method or develop a policy or understanding, that allows us to make better informed decisions in the future (typically judged against some stated preferred outcome criteria). Cost and benefit considerations will also shape the appropriate selection of data for use in a given mathematical model. 100
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 3

2)	 Data preparation – This is concerned with generating data in an appropriate form as depending upon the model developed, it might be that data is not in the required form or units, to be immediately utilised. You may therefore need to progress through a series of transformations, to be able to apply a given method to a dataset. This is explored with data forecasting. 3)	 Model solution – given the nature of modelling, it is important for the manager/modeller to also be aware of the mechanistic limitations of their model, so that the solution generated through its application to a dataset, will also have constraints and assumptions that limit the solution’s effectiveness and/or validity. Clearly therefore the choice of a decision model requires a cost and benefit consideration to be made in selecting an appropriate set of mathematical relationships where frequently a less complicated (although perhaps less precise) model is more appropriate than a more complex and accurate one due to cost and ease of solution considerations. We can also consider from chapter 1, that decision variables and the decision body can also increase the complexity of the model chosen and the data selected. The model’s solution therefore is always seeking to maximize or minimize a given event or outcome, subject to the constraints of the model chosen. The values of the decision variables that provide the mathematically-best output are referred to as the optimal solution for the model. In the area of linear programming for example, which is concerned with resource efficiency use, (but which is not discussed in this text), it is important to attempt to identify the optimizing solution from a range of viable solutions (or usually called feasible solutions). Mathematical models are deterministic when concerned with discrete events and outcomes. Where events and outcomes are subject to variations in inputs that shape the decision making process, they are called stochastic models. Such models are more problematic for managers and difficult to generalise from. Perhaps the most famous such model in studies of Business and Management is that of Gibrat’s Law (Box 3.1).

This e-book is made with

SetaPDF

SETASIGN

PDF components for PHP developers

www.setasign.com
101
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making Box3. 1 : A stochastic mathematical model

Chapter 3

Robert Gibrat stated that the size of a firm and the growth rate of that firm are independent. This is also called the rule of proportionate growth. It has also been applied to city growth where both a pareto and log-normal mathematical growth models are observed for some periods of city growth. These differences in growth rates arise due to the relationship between economic activity and population mobility. To understand how and why individuals cluster in towns and then cities in the same geographical area, requires an understanding of the varied reasons why people move and what economic forces are involved in this decision (Eeckhout, 2004) making it difficult for a deterministic relationship to emerge. We continue this chapter by developing these concerns further through a consideration of probability and decision analysis of finite and ‘state of nature events’. It is important to realise in this discussion that the term ‘event’ refers to an action of interest to a decision maker. ’State of nature’ events specifically refers to mutually exclusive events or outcomes of interest to a decision maker, where if a given event occurs, other events that were possible, can no longer happen. So for most people of modest means, if you choose to buy car A, you are then unable to buy car B, C or D. These are discrete event outcomes, which are from a very small set of possible events or outcomes that a decision maker might choose from. Where there might be many thousands or more potential event outcomes, we clearly are not dealing with a small, manageable discrete set of possible events – but in fact are considering a distribution of events (with some more or less likely to occur or be chosen). We consider this type of event outcome later as ‘distribution functions’.

3.2	

The Decision tree – a map of the solution set

In its simplest understanding, discrete event outcomes of unknown or uncertain decisions, can be described mathematically by using logical analysis. This is often described as decision analysis. We were introduced in chapter 1 to the idea of a solution set – a range of solutions which describe the range of possible outcomes for a given decision. This range of solutions can be represented by a logical decision tree – which is a visual representation of outcomes that emerge from a given decision (and which, when drawn, can look like a tree!). In this manner, this is a logical model reflecting the RAT models outlined in chapter 1. All outcomes can be articulated for a given series of sequential decisions so as to determine the optimal outcome given different likelihoods for those outcomes occurring. Hence a logical ‘tree’ of outcomes is generated which is a chronological representation of the decision problem. There are conventions to adopt to construct such decision trees using a particular form and structure. Each decision tree therefore has two types of outcome (or nodes). Round nodes correspond to the states of nature (the outcomes) while square nodes are used to represent the (further or subsequent) decision alternatives. In such a way, decision alternatives can lead to further state of nature outcome nodes and hence further decisions – and so forth. In this way, the full decision solution set can be explored and visually represented. When all solution set outcomes have been mapped then it is also common practice to identify payoffs/costs/benefits that accrue should that branch of the decision tree manifest in reality. In constructing these trees, it is common practice to begin at the first decision (which is constructed on the left of the subsequent diagram) and logically proceed right, using the appropriate nomenclature (square and round nodes) to represent outcomes from decisions. Only when all the solution set has been captured and visualized, can data be added to the tree. Such a simple articulation of a solution set of course, reflects available information at a given point and time in the models development.

102
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 3

As we see shortly, undertaking actions to improve understanding about the likelihood of a given outcome occurring can subsequently require a revision of the original tree or mapping of the solution set. It is common practice to then describe the original probabilities (likelihood) of a given decision occurring from a solution set as being prior probabilities. When the tree is then revised in light of new knowledge of the problem (and hence an improved model has then been developed), the probabilities (likelihood) of a given decision then occurring from a solution set are described as being posterior probabilities. Let’s consider a simple example (Box 3.2) and review some simple refreshers on basic probability. •	 The probability (likelihood) of a given event (E) occurring can be written as: P(E) •	 Mutually exclusive events (States of Nature) describe that if one outcome event occurs, the others from the solution set cannot. •	 Independent events are described when the occurrence of one event does not affect the likelihood of an alternative event occurring •	 The logical rule of independent events occurring (without regards to their combination) is the OR rule (i.e. P (throwing a 3 or 6 on a single throw of a die) = 1/6 + 1/6 = 1/3. •	 The logical rule of sequential non-independent events occurring is the AND rule (i.e. P (throwing a 3 and then a 6 on a single throw of a die) = 1/6 x 1/6 = 1/36. •	 The likelihood of several given event outcomes occurring following a decision being made must sum 1 (as one of the event outcomes will occur). •	 Conditional probabilities –describe the context where one event DOES affect the likelihood of another then occurring, i.e. probability of event X occurring after Y event has already occurred = P(X │Y) Box 3.2: The Production Line On a production line, 90% of the time it is set up correctly. When this happens, 95% of the output it produces are good parts. If the line is incorrectly set up, then the chance of it producing a good part is only 30%. On any given day, when the line is set up and the first goods made are found to be good, what is the probability that the machine is set up correctly? If we draw a decision tree to encompass the solution set, we begin with the first ‘decision’ – which is the decision to ‘Set up Production Line’ (hence a square node). This can have two state of nature outcomes as defined in the question – the line can be set up correctly or it can be set up incorrectly (round nodes). The inspection of parts from both potential branches of setting up the production line (the next decision) can result (as outcomes) in parts then being assessed as either good or bad. Hence the tree would look like:

103
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 3

After this forward pass through the logic of the question, the simple tree now contains all possible solutions – i.e. that good parts can arise from both a good and bad initial set up. Backward passing through the tree (from right to left) – we then add measured data (in this case the likelihood of producing good parts from a good set up, the likelihood of being set up correctly and the likelihood of producing a good part from a bad set up) – hence:

In the past four years we have drilled

81,000 km
That’s more than twice around the world.
Who are we?
We are the world’s leading oilfield services company. Working globally—often in remote and challenging locations—we invent, design, engineer, manufacture, apply, and maintain technology to help customers find and produce oil and gas safely.

Who are we looking for?
We offer countless opportunities in the following domains: n Engineering, Research, and Operations n Geoscience and Petrotechnical n Commercial and Business If you are a self-motivated graduate looking for a dynamic career, apply to join our team.

What will you be?

careers.slb.com

104
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Chapter 3

The solution set that contains good parts as an outcome (event) of this model can be described as: { P(Parts good | good initial set up), P(Parts good | bad initial set up)}. The | symbol denotes a preceding outcome has already happened (and it is said as ‘the probability of X event occurring given Y event has already occurred). Hence if our decision to resolve is to learn if the production line is set up correctly or not initially, to arrive at the outcome of good parts from an initial good set up, the likelihood of this occurring is (0.9 x 0.95) – as both these outcome events must occur. This equals 0.855 (or 86%). However, we have noted that our solution set {…} above also includes the outcome of good parts from a bad initial set up. The likelihood of this being an outcome event is (0.1 x 0.3) or 0.03. Hence good parts can come from both a bad OR good initial set up with a likelihood of (0.03 + 0.855) or 0.858. Basic probability theory has been used in this simple example and now also allows us to finally answer the question initially asked of P (good production line set up) as: 0.855 / 0.858 or 99.7%. This last calculation is derived by considering which answers of the solution set are the desired ones (i.e. those which product good parts from an initial good set up). This example has used both prior and conditional probabilities (likelihoods). The example given in Box 3.2 did not include pay offs (the returns gathered by following a particular series of decisions and outcomes) per se, which would be associated with the outcome event solution set. Pay offs – when added to a decision tree - sit on the very right of the diagram, and denote (typically) profit, cost, time, distance or any other appropriate measurement argued to preferentially differentiate between preferred outcome events. Pay offs are therefore the result of a specific combination of a decision alternative and a state of nature.

3.3	

Decision Analysis

Before progressing this probabilities discussion further, we can also consider how decisions are made where we may lack specific probability data to support the development of a complete solution set. This is often called decision analysis under uncertainty. Andersen et al (2010) and Waters (1998) describe three approaches to modelling decisions when we are considering alternate states of nature outcomes without comparative probabilities. These are:

105
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 3

•	 The optimistic approach – where the decision maker acts rationally (following RAT models) and chooses the largest payoff (or alternately the payoff which uses a minimum of resources). This is also called the equal likelihood criteria (as each state of nature is equally likely to occur) or sometimes the Laplace decision criterion (named after its originator Pierre-Simon Laplace (1749-1827)(Waters, 1998). This approach does not consider the risk of incurring large losses – it is focused upon the highest average return. This can make this method of decision making unattractive for smaller organisations for example who are more concerned with short term cashflow. •	 The conservative approach – where the decision maker acts rationally and chooses the best of the worst possible payoffs i.e. a payoff may be the maximum of the minimum payoffs (Maximin) achievable. This is also called the Wald decision criteria (named after its originator Abraham Wald (1902-1950)(Waters, 1998).Hence the strategic objective of this approach is to avoid the worst outcomes. In these situations, the decision maker may gain more (in hindsight) by following different decisions, but in choosing the maximin payoff – he/she cannot gain less. •	 The minimax regret approach – where the decision maker identifies the decision that results in the minimum of the maximum regrets (losses) between different outcome events. Hence the objective is to ensure that the larger opportunity losses are avoided (Lotfi and Pegels, 1996). This is also called the Savage decision criteria (named after its founder Leonard Jimmie Savage (1917-1970)). We can illustrate these different decision making strategies through some examples:
237,0,67,&/DSODFH '(&,6,21 $/7(51$7,9( G G G

67$7(RI1$785( V    V    V    0$;,0803$<2))    




5(&200(1'(' '(&,6,21   G

%(673$<2))

Table 3.1

&216(59$7,9(:DOG '(&,6,21 $/7(51$7,9( G G G 67$7(RI1$785( V    V    V    0,1,0803$<2))     5(&200(1'(' '(&,6,21   G




%(67
:2567
3$<2))
Table 3.2

106
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 3

In table 3.1, a rational optimistic decision maker would use the optimistic (or maximax) approach – in other words they would choose the decision that has the largest single value in the payoff table (in this case d3, with all states of nature assumed to be equally liable). In table 3.2, a decision maker would use the conservative (maximin) approach where the minimum payoffs for each decision are listed and the decision maker selects that outcome which is the maximum of these minimum payoffs (in this case d3 again). For the minimax regret approach, reconsider table 3.1 – but this time assume that we choose the optimum decision per potential state of nature, so that for s1 we could chose d1, for s2 we would chose d3 and for s3 we would choose d2. We then subtract this optimal decision payoff per state of nature from the other payoffs per state of nature. So for table 3.3, the first column (s1) which has the optimal payoff of 4 (from s1) – would generate a regret table of (4-4=0, 4-0=4, 4-1=3) to populate the first column. This is also known as an opportunity loss matrix. The second and third column are similarly populated so that:
s1 d1 d2 d3 0 4 3
Table 3.3

s2 1 2 0

s3 1 0 2

After constructing table 3.3, we can then identify those decisions (per state of nature) which result in the biggest (and lowest) maximum regret. In the case of table 3, this is 1 for d1 (by either s2 and/or s3), 4 for d2 (by s1) and 3 for d3 (by s1). Now considering these maximum regret payoffs, we can note that the decision d1 has the lowest maximum regret and would be our final strategic choice. In other words, this is the minimum maximum regret.

107
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Chapter 3

Let’s consider another example of this form of decision making uncertainty. It is particularly relevant when the actions of a decision maker are not judged by that decision maker, but by others elsewhere in an organisation. For example Beckman(1988) in a fascinating read on the causes of famous business crashes since the 17th century, recounted the story of a famous stockbroker in the 1970s who was unsuccessful initially in selling his stock to commodity houses. He soon realised that it was not the inherent quality of the stock he was trying to sell that was actually important to potential buyers, but whether other commodity buyers had decided to buy stock. Hence his decision making (in deciding what to try to sell) was being judged by others. Similarly, a tutor grading student papers may judge a student not by what they achieved (say 65%) but that they did not achieve 100%. In all cases, there is then regret –which is the difference between the actual outcome and best possible outcome. The ‘student’ regret would be 35% in the cited example. Key steps for this minimize regret/Savage decision making criterion are (Waters, 1998): 1.	 For each event find the best possible outcome (i.e. the ‘best’ payoff in a given range of concurrent but mutually exclusive events). 2.	 Identify the difference between the outcomes for those events and the ‘best’ outcome of the events presented – these are the regrets of the decision making process 3.	 Use these regrets to construct a ‘regret’ matrix (otherwise known as an opportunity loss matrix). There must then of course be one zero in each column of payoffs/possible outcomes (which comes from the ‘best’ outcome) and a table of +ve differences from the other possible outcomes). 4.	 For each alternative decision, identify the highest regret (i.e. the largest number in each column of mutually exclusive events). 5.	 Select the alternative with the lowest value of those highest regrets. Waters(1998) offers a simple worked example to illustrate this decision making criteria which is adapted here. Consider three competing market sellers of fruit and vegetables who must each decide on the morning how much (what quantity) to buy. This decision is taken with consideration of a range of variables which (for argument’s sake) depends upon the weather. The market sellers, through their experience, would be able to construct a payoff table (outcome table) such as (Table 3.4) which is the difference (say in tens of pounds) between costs and revenues:
Events Good weather Buy large Alternatives Buy medium Buy small 20 10 6
Table 3.4: Payoff (profit) table for Market Sellers

Average weather 8 9 3

Bad weather 2 6 4

Clearly by inspecting table 3.4, the best outcomes are 20, 9 and 6 for all event outcomes. The regret table (opportunity loss table) is then constructed by subtracting the best payoffs from all payoffs – such as that in table 3.5:

108
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 3

Events Good weather Buy large Alternatives Buy medium Buy small 0 10 14
Table 3.5: Opportunity Loss (regret) table

Average weather 1 0 6

Bad weather 4 0 2

Hence by inspecting table 3.5, the highest regrets by decision alternative are the maximum of those decision alternatives, ie. •	 Highest regret of ‘buy large’ = 4 •	 Highest regret of ‘buy medium’ =10 •	 Highest regret of ‘buy small’ = 14 Hence – the recommended decision alternative is ‘buy large’. Let’s take another example- this time of stock investment. In Table 3.6, the stock rises or falls are noted in stock prices over a four year period (let’s just assume they are given in % year on year). As an investor, one method of making a decision might be to use an optimistic approach and then after a period of time, to review the success (or otherwise) of this model through a minimax regret analysis. So we can assume (as in reality) that we only have Year 1 data to review (initially and so on).
Stock Intel Google Apple Sony Wolfram Year 1 -0.3 0 0.6 0.3 0.2 Year 2 0.9 0.5 0.4 -0.1 -0.6
Table 3.6

Year 3 0.4 0.1 0 -0.7 0.4

Year 4 0.5 0.4 0.6 0.7 0

In Year 1, the optimistic approach to decision making would have been to select Apple stock, which had risen 0.6 percentage points in that year, for year 2. In Year 2, we would then have selected Intel stock for year 3, followed by either Intel or Wolfram in year 3 for year 4 and then finally Sony stock in year 4 for year 5. The key question is, how does your return differ by following this approach, from the ‘best’ approach (the decisions that minimise the maximum regret)? Constructing a regret table generates table 3.7:
Stock Intel Google Apple Sony Wolfram Year 1 0.9 0.6 0 0.3 0.4 Year 2 0 0.4 0.5 1 1.5
Table 7

Year 3 0 0.3 0.4 -1.1 0

Year 4 0.2 0.3 0.1 0 0.7

109
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 3

In other words, for column 1, the optimistic decision for year 2 is to buy Apple (0.6 gain). Hence the regret payoff for d1 (intel) is then 0.6- (-0.3) = 0.9, for d2 (Google) is 0.6-0.6=0, for d4 (Sony) is 0.6-0.3=0.3 and for Wolfram is 0.6-0.2=0.4. Now at the end of Year 2, we select to buy Intel stock (which showed a 0.9 gain) for Year 3. Hence the regret payoff for Year 2 then becomes (table 5) d1 (0.9-0.9=0), d2 (0.9-0.5=0.4), d3 (0.9-0.4=0.5), d4 (0.9-(-0.1)=1), d5 (0.9-(-0.6)=1.5). This is repeated for Years 3 and 4 to complete Table 3.7 (NB Intel stock again was again bought in Year 3). Adding a fifth column to table 3.5, generates table 3.6 which contains the maximum regret per decision as: Stock Intel Google Apple Sony Wolfram Year 1 0.9 0.6 0 0.3 0.4 Year 2 0 0.4 0.5 1 1.5
Table 3.8

Year 3 0 0.3 0.4 -1.1 0

Year 4 0.2 0.3 0.1 0 0.7

Maximum Regret 0.9 0.6 0.5 1 1.5

The minimum maximum regret considering the decisions across the four years is found with Apple (at 0.5) and would, reflecting upon historic data, be the preferred stock to buy. Overall therefore the minimax regret approach requires the construction of a regret table over this period (this is the opportunity loss table) which is determined by calculating for each state of nature, the difference between each forecasted payoff and the largest payoff for that state of nature. This table of data then allows the maximum regret (difference) for each possible decision as listed to be determined. This corresponds with the decision of choosing the minimum of the maximum regrets.
Find and follow us: http://twitter.com/bioradlscareers www.linkedin.com/groupsDirectory, search for Bio-Rad Life Sciences Careers http://bio-radlifesciencescareersblog.blogspot.com

John Randall, PhD Senior Marketing Manager, Bio-Plex Business Unit

Bio-Rad is a longtime leader in the life science research industry and has been voted one of the Best Places to Work by our employees in the San Francisco Bay Area. Bring out your best in one of our many positions in research and development, sales, marketing, operations, and software development. Opportunities await — share your passion at Bio-Rad!

www.bio-rad.com/careers

110
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making Some questions for you to work through are given below (answers then follow).

Chapter 3

Q1) A firm decides to build a new plant to make a new range of toys. The new plant - could either be large or small and this choice depends upon how it is believed the market will react to the new range of toys. The Marketing Staff have decided to view this decision from a long run perspective where demand could either be low, medium or high (in mEuros).This is the projected payoff table:
Plant Size Low Demand Small Large 150 50 Long Run Demand Medium Demand 200 200 High Demand 200 500

Answer the following questions1:1) What is the decision to be made and what is the chance event for this firm? 2) Construct a decision tree and 3) Recommend a decision based upon the use of optimistic, conservative and minimax regret approaches. A1) To choose the best sized plant (which can be large or small), A2)



$QG$
237,0,67,&PD[LPXPSURILW /RQJUXQ GHPDQG 3ODQWVL]H ORZ PHGLXP KLJK    VPDOO    ODUJH &216(59$7,9(PLQLPL]HVPD[LPXP
ORVV
 /RQJUXQGHPDQG PHGLXP  

3ODQWVL]H VPDOO ODUJH 0$;,0805(*5(7

ORZ  

KLJK  

111
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 3

23325781,7</2667$%/( 3ODQWVL]H VPDOO ODUJH ORZ   /RQJUXQGHPDQG PHGLXP KLJK     '(&,6,21 ODUJH

0$;,0805(*5(7

0,1,0800$;,0805(*5(7

  

Clearly an important concern for a decision maker, making decisions under uncertainty is to identify and consider which decision making criteria to use. There is no guarantee or expectation, that the three outlined decision making criterion will generate the same recommended decisions. Which criterion to use, is partially dependent upon who will be judging the success or otherwise of those decisions. If it is not the decision maker for example, then the minimax regret approach (Savage) should be adopted. It has already been noted, that a small company may not be able to bear significant losses and hence will make decisions conservatively (Wald) and if, for a larger organisation, there is little to differentiate between decision alternatives then the optimistic (Laplace) decision criteria can be used. We will return to these issues when considering the value of (new) information regarding decision alternatives shortly.

3.4	

More on probabilities: Expected Monetary Values (EMVs)

The earlier discussion (following from Box 3.1) introduced simple probabilities and their manipulation to develop models of a decision which span the solution set for a given problem. Expected Monetary Values (EMVs) are additional calculations derived from the multiplication of a given outcome likelihood and the payoff for that likelihood. As (usually) there are several outcomes following a decision, one of these outcomes will happen (should that decision be taken) and hence the EMV of that decision is the summed weighted payoffs for the outcomes of that decision. This generates a weighted return for a given decision outcome which can then be compared by the manager, with other weighted returns, to determine, rationally, the optimal decision strategy to adopt. The choice of a given event outcome for decision di is then determined by: EMV (di) = Σ P(si)xVi for i=1 to n event outcomes (states of nature). Vi= payoff for ith event outcome (state of nature) Consider a simple example - in an organisation, 2 projects are being considered for funding. You have been asked to calculate the EMV of each project using the data given in order to determine which project (and hence decision) is likely to yield the largest return. The probability of project A being very successful (for £6000 profit) are 20%, the probability of project A being moderately successful (for £3500 profit) are 50% with all other outcomes generating profit (of £2500). The probability of project B being very successful (for £6500) profit are 10%, of being moderately successful (for £4000) are 60% with all other outcomes generating profit (of £1000). Therefore the EMV (Project A) is going to be £3700 and the EMV (Project B) is going to be £3350. Rationally, project B would be supported. Now consider a more involved example (derived from Oakshott, 1998) – where a company is considering whether to launch a new product onto the market place. The commercial success of the idea however, depends on the subsequent ability of a competitor to bring out a competing product (estimated at 60% likelihood) and then the relationship of the competitor’s prices to the firm’s price. This has been determined as (based upon previous experience):

112
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 3

Company Price

Profits (£,000s) Competitor Pricing Low Medium 42 45 30 High 45 49 53

Profit with no competition

Low Medium High

30 34 10

50 70 90

As the company is launching its product onto the marketplace, probabilities have been made about the pricing of the competitor product in light of the pricing policy of the company. These have been estimated as:
Company Price Low Low Medium High 0.8 0.2 0.05 Competitor’s Price Probability Medium 0.15 0.7 0.35 High 0.05 0.1 0.6

We can construct a decision tree to identify the solution set to this problem and by determining EMVs, identify the optimal decision strategy (see figure 3.1). Hence adding the probabilities to the outcome event branches we can determine that:

678'<)25<2850$67(5©6'(*5((
&KDOPHUV 8QLYHUVLW\ RI 7HFKQRORJ\ FRQGXFWV UHVHDUFK DQG HGXFDWLRQ LQ HQJLQHHU LQJ DQG QDWXUDO VFLHQFHV DUFKLWHFWXUH WHFKQRORJ\UHODWHG PDWKHPDWLFDO VFLHQFHV DQG QDXWLFDO VFLHQFHV %HKLQG DOO WKDW &KDOPHUV DFFRPSOLVKHV WKH DLP SHUVLVWV IRU FRQWULEXWLQJ WR D VXVWDLQDEOH IXWXUH ¤ ERWK QDWLRQDOO\ DQG JOREDOO\ 9LVLW XV RQ &KDOPHUVVH RU 1H[W 6WRS &KDOPHUV RQ IDFHERRN

113
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Chapter 3

EMV(5) = 32.55 / EMV (6) = 43.4 / EMV(7)=42.8 and EMV(1) = 39.53 / EMV(2) = 53.92 / EMV (3) = 61.68. Clearly EMV(3) generates the largest potential return. The optimal decision strategy therefore is to 1) Enter the market place and set a high price (regardless of what the competitor’s pricing policy is).
WĂǇŽĨĨƐ
>Kt

ŽŵƉ͍͘

zĞƐ

>Kt

ϱ
EŽ

D/hD ,/',

ϯϬ ϰϮ ϰϱ ϱϬ

ϭ

>Kt

 ϯϰ ϰϱ ϰϵ

D/hD ŶƚĞƌDĂƌŬĞƚ
^ĞƚWƌŝĐĞ

ŽŵƉ͍͘

zĞƐ

ϲ

D/hD ,/',

Ϯ
EŽ

͍

ϳϬ 
>Kt

ϭϬ ϯϬ ϱϯ  ϵϬ 

,/',

ŽŵƉ͍͘

zĞƐ

ϳ

D/hD ,/',

ϯ
EŽ

ŽŶŽƚĞŶƚĞƌDĂƌŬĞƚ

Ϭ

Now let’s return for a moment to the earlier discussion of decision making under uncertainty (where we lacked specific probabilistic data on the likelihood of a given event occurring), but add the issue of what if the decision maker was able to buy more information about the range of given outcomes, so that they could make a more informed decision? This is typically why organisations hire consultants or marketing specialists. So, an organisation is considering launching a new type of suncream with different sun protection factors (SPF values), sales of which are very weather dependent. A consultant is hired to undertake a forecast for the next 12 months (perhaps using methods discussed in Chapter 2) and to project potential sales (revenues in £00s) for that time period. The gathered data suggests the following (table 3.9):

114
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 3

Forecasted weather (sun intensity and duration) Low Cream 1 (SPF 10) Cream 2 (SPF 30) Cream 3 (SPF 50) 30 20 10
Table 3.9

Medium 30 35 40

High 5 45 45

Now, we could evaluate this payoff table using either optimistic (Laplace), conservative (Wald) or minimax regret (Savage) criteria in the normal manner. However, we would expect (or hope to assume) that the consultant would be delivering perfect information to the decision maker – so the company knew in advance which weather outcome was going to occur. So, let’s assume the consultants advise that the weather / sun will be low for the year coming. The rational decision is then to make and sell Cream 1. If the consultants advise the weather/sun would be medium, then the rational decision would be to make and sell Cream 3. If the consultants advise that the weather /sun will be High, then the company could make and sell either Cream 2 or Cream 3. However we cannot forget there is a fee for the consultants services (F). Hence we can add a fourth row to our payoff table so that (table 3.10): Forecasted weather (sun intensity and duration) Low Cream 1 (SPF 10) Cream 2 (SPF 30) Cream 3 (SPF 50) Payment for services 30 20 10 30-F
Table 3.10

Medium 30 35 40 40-F

High 5 45 45 45-F

Resolving this decision under uncertainty (but with that uncertainty borne by the consultant for a fee to the company) we can generate solutions of: Optimistic (Laplace) criteria: Cream 1 average revenue of 21.67, Cream 2 average revenue of 33.34 and Cream 3 average revenue of 31.67.The payment for market knowledge is (30-F+40-F+45-F)/3= 38.34-F Comparing with the expected revenues for the best cream sales (cream 2), then the condition at which the manager would pay for perfect future market knowledge is given by: 38.34-F>=33.34 or F<=5

115
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 3

Hence the maximum price that should be paid for market knowledge under optimistic (Laplace decision making) is £5000. Conservative (Wald) criteria: Cream 1 worst revenue of 10, Cream 2 worst revenue of 30 and Cream 3 worst revenue of 5. The worst payment fee for market knowledge (of all the combinations of (30-F, 40-F, 45-F) is going to be 30-F rationally. Comparing with the best of the worst expected revenue for the cream sales (cream 2 of 30), then the condition at which the manager would pay for perfect future market knowledge is given by: 30-F>=30 or F<=0 Hence there is no expected condition in which the manager should pay for additional market information under the conservative (Wald) criteria.

Linköping University – innovative, highly ranked, European
Interested in Engineering and its various branches? Kickstart your career with an English-taught master’s degree.

Click here!

116
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Chapter 3

Minimax (Savage) critieria:
If we create a regret (opportunity loss) table from table 3.10, we generate the following (table 3.11): Forecasted weather (sun intensity and duration) Low Cream 1 (SPF 10) Cream 2 (SPF 30) Cream 3 (SPF 50) Payment for services 0 10 20 F
Table 11

Medium 10 5 0 F

High 40 0 0 F

Hence the highest regret for each cream is given by Cream 1 = 40, Cream 2= 10, Cream 3 = 20, regret fee payment of F. Hence the lowest of these regret values is 10, hence F<=10 as the maximum amount that should be paid for that consultant information.

3.5	

Revising problem information – Bayes Theorem

Let us consider another worked example, but this time also including data which allows the first decision tree to be reviewed (i.e. think of this as the common practice of undertaking market research to improve your view of how a market works before launching a product for example). As the problem and solution are presented, the impact of this revision to the prior (assumed) market probabilities is discussed. Box3.3: Revising Probabilities with new market data It is likely that the current difficult economic climate in the US and UK, will potentially will affect the numbers of students entering further and higher education. As a result, expected sales of student textbooks (whether electronic or traditional papercopies) will fall and/or fluctuate. Let’s assume you undertake some internal market research (i.e. you ask your colleagues) for their view of this impact upon your company’s performance. It is suggested that the company can expect a profit of £1.5m if the student numbers studying (for new entry in the coming year) fall by a small amount, a profit of £0.5m if numbers fall by a moderate amount and a loss of £2m if numbers fall by a heavy amount. You have estimated that the likelihood of these events is P(small)= 0.4, P(moderate)=0.3 and P(heavy)=0.3. Your first step is then to draw a simple decision tree of these assumptions of the market and the likely market value for the company in the coming year.

117
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 3

WƌŽĨŝƚƐ;ŵάͿ нϭ͘ϱ ^ŵĂůůŵĂƌŬĞƚĨĂůů;Ϭ͘ϰͿ  нϬ͘ϱ ϭ DĞĚŝƵŵŵĂƌŬĞƚĨĂůů;Ϭ͘ϯͿ   ͲϮ >ĂƌŐĞŵĂƌŬĞƚĨĂůů;Ϭ͘ϯͿ
EMV (Market Node (1)) = (1.5)(0.4)+(0.5)(0.3)+(-2)(0.3) = 0.15(m£) Whilst therefore the solution set tree above indicates a likely profit, the concern with the ‘best guess’ market probabilities is a concern for the company. To potentially offset this, your company is considering reallocating some of their production capacity by leasing it to another organisation. If they do this, the potential loss of profit will not be as significant as projected above (due to the additional leased income), but this will limit the productive capacity of the organisation, if the expected decline in student numbers does not occur. In this scenario, you have projected a profit of £1m if student numbers fall by a small amount, a profit of $0.75m if student numbers fall by a moderate amount and a loss of $0.5m if student numbers fall by a heavy amount.

WƌŽĨŝƚƐ;ŵάͿ ^ŵĂůůŵĂƌŬĞƚĨĂůů;Ϭ͘ϰͿ нϭ  нϬ͘ϳϱ DĞĚŝƵŵŵĂƌŬĞƚĨĂůů;Ϭ͘ϯͿ   ͲϬ͘ϱ >ĂƌŐĞŵĂƌŬĞƚĨĂůů;Ϭ͘ϯͿ
EMV (Market Node with leasing (2)) = (1)(0.4)+(0.75)(0.3)+(-0.5)(0.3) = 0.475(m£)

Ϯ

118
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 3

The optimal decision therefore, by comparing EMVs is to invest in the leasing of some productive capacity. What however, is the additional EMV benefit of deciding to lease some of the company’s productive capacity? To determine this we can combine the two previous tree diagrams and determine something called EMV with Perfect Information. (EMVwPI). This is an expectation of the optimum value of given event outcomes and represents the maximum return that could be expected given that the ‘optimal’ event outcomes manifest. It provides therefore some indicative measurement of the price (and hence benefit) that a manager would be willing to pay to learn of perfect information about a decision (i.e. how things would manifest). Hence constructing the next tree as a combination of the preceding trees generates:

EŽ ϭ >ĞĂƐĞ͍ zĞƐ Ϯ

Hence EMVwPI= (0.4)(1.5)+(0.3)(0.75)+(-0.5)(0.3)=0.675(m£) – i.e. the best EMVs are taken for each state of nature after considering both decision outcomes (lease or not lease). Hence the EMV of Perfect Information = EMVwPI - EMV with leasing = 0.675-0.475 = 0.2(m£) Given that the decision to lease some productive capacity can be made flexibly (and is indeed a preferred decision), the board of the company decided it would be prudent to wait until September and then use data on the numbers of students who are applying during the Clearing Period, as the best indicative guide to actual expected student enrolment. (The ‘Clearing Period’ is a traditional few weeks in August where students who are late in applying to universities or missed the terms of their offer for a place of study, can contact all the Universities in the UK and discuss with them, the possibility of gaining an offer of a place of study). It is therefore argued that an active clearing period in September is likely to require more productive capacity from the company and the leasing option becomes less attractive. Estimates of the probabilities for a very busy clearing period (CP) are P(CP|s1) =0.30, P(CP|s2) =0.2 P(CP|s3) =0.05, where si (i=1..3) denote the states of nature outcomes for a small, moderate or heavy fall in student numbers respectively. To see how this decision affects the solution set for the problem, we now need to go back and reconsider the tree AND consider how this new market data changes earlier assumptions in our analyses (i.e. how this shapes the previous prior probabilities). D1 and D2 refer to ‘no leasing’ and ‘leasing decisions’.

119
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 3

The part of the new tree (nodes 10,11 and 5) – have been resolved earlier in the discussion. Nodes 6,7,8 and 9 are resolved by considering THE LOGICAL ORDER of the decisions. We know for example that P(S1 |D1) represents the first probability on Node 6, followed by P(S2 |D1) and P(S3 |D1). The second event outcomes are similarly, P(S1 |D2), P(S2 |D2) and P(S3 |D2). We have already determined however the following probabilities of: P(CP|s1) =0.30, P(CP|s2) =0.2 P(CP|s3) =0.05, where si (i=1..3) or in terms of the nomenclature of figure 2, and Node 4, P(Very Busy Clearing | S1), P(Very Busy Clearing | S2), P(Very Busy Clearing | S3). In other words, we need to revise the prior probabilities in light of this new conditional information. This can be achieved using a BAYESIAN revision (discussed later). Consider first the BUSY CLEARING PERIOD.
State of Nature (after Clearing) S1 (Small Market) S2(Medium Market) S3(Large Market) Prior 0.4 0.3 0.3 Conditional 0.3 0.2 0.5 Joint 0.12 0.06 0.15 0.33 (SUM) Posterior 0.36 P(S1 |D1) 0.18 P(S2 |D1) 0.45 P(S3 |D1)

And hence for Node 8 and Node 9: where Node 9 EMV (with leasing) and Node 8 EMV(without leasing): EMV(9) = (0.36)(1) +(0.18 ) (0.75)+(0.45) (-0.5) EMV(8) = (0.36)(1.5))+(0.18)(0.5)+(0.45)(-2)

120
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making Similarly, we can generate the posterior probabilities for a NON BUSY CLEARING PERIOD.
State of Nature (after Clearing) S1 (Small Market) S2(Medium Market) S3(Large Market) Prior 0.4 0.3 0.3 Conditional 0.7 0.8 0.5 Joint 0.28 0.24 0.15 0.57 (SUM)

Chapter 3

Posterior 0.49 P(S1 |D2) 0.31 P(S2 |D2) 0.26 P(S3 |D2)

And hence for Node 6 and Node 7:where Node 6 is EMV (with leasing) and Node 7 EMV(without leasing) for a NON BUSY CLEARING PERIOD. EMV(6) = (0.49)(1) +(0.31 ) (0.75)+(0.26) (-0.5) EMV(7) = (0.49)(1.5))+(0.31)(0.5)+(0.26)(-2) Finally, as noted EMV(Node 10) and EMV (Node 11) have previously been determined. It is therefore straightforward to determine EMV (3),EMV(4) and EMV (5) and hence EMV(2).
^ŵĂůůDĂƌŬĞƚ ^ŵĂůůDĂƌŬĞƚ;^ϭͿ ϭ EŽƌŵĂů ůĞĂƌŝŶŐ  ϯ Ϯ tĂŝƚĨŽƌ ůĞĂƌŝŶŐ Ϯ  ϰ sĞƌǇƵƐǇ ůĞĂƌŝŶŐ ϭ Ϯ ϵ ϳ DĞĚŝƵŵDĂƌŬĞƚ;^ϮͿ >ĂƌŐĞDĂƌŬĞƚ;^ϯͿ ^ϭ ^Ϯ ^ϯ ϭ ^ϭ ϴ ^Ϯ ^ϯ ^ϭ ^Ϯ ^ϯ ^ϭ ϭϬ ^Ϯ ^ϯ ^ϭ ^Ϯ ^ϯ

ϲ

ϭ  ϱ Ϯ &ŝŐƵƌĞϮ ϭϭϳ

ŽŶ͛ƚǁĂŝƚĨŽƌůĞĂƌŝŶŐ

ϭϭ

121
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 3

In Box 3.3, the estimation of conditional probabilities (which can also sometimes be presented as new market research say as being favourable or unfavourable to a product launch or sale) required a revision to be made to the earlier prior probabilities and the logical order of the decision tree. This was described as a Bayesian revision (so called after its founder The Reverand Thomas Bayes (1702-1761))– but we’re getting a bit ahead of ourselves. Let’s recap on some of the new methods and modelling approaches discussed so far.

3.6	

The Value of Sample and Perfect Information

Market research can be undertaken by a manager in a company so as to improve (and ideally full understand) their likely chances of competitive success in a given market. By learning more about that market, we can take one assumption of learning ALL about that market. In this case it is called perfect information – about the states of nature. So for example, a firm could decide to launch product X (D1) into a good market (S1) which has the potential to generate £20m in that market or a firm could decide to launch product Y (D2) into a medium good market (S2) which has the potential to generate £7m in that market. If the company knew that S1 was going to occur, it would select D1 and similarly if the company knew that S2 was going to occur, it would select D2. This would be optimal decision strategy with perfect information. If we now knew what the probability of S1 and S2 were, we would be able to determine the expected value of this optimal strategy in the normal manner. This expected value is called the expected value with perfect information. Where we don’t know or have access to perfect information (the normal situation!), the expected values of a given decision tree and solution set, can also be referred to as the expected value without perfect information. Clearly, the difference between the two expected values represents the benefit that comes from knowing exactly how the market or situation will develop. It is therefore the value OF perfect information. For a manager, it represents the maximum ‘price’ that they would be prepared to pay to know more about how the market would actually develop for them. Of course, it is highly unlikely that any market research would ever generate enough insight to guarantee for-knowledge of future events – so the value of perfect information represents an intangible ceiling upon the fee for any additional market knowledge. In the worked example of Box 3.3, we were introduced to a refinement of this idea of perfect knowledge – that of sample information. Sample information here means simply that more ‘sampled’ information is generated about the states of nature of how more or less likely they actually are, to occur in reality. We are (as managers) improving the subjectivity of our initial (prior) assumptions about the market or product or buyer behaviour etc through some sampled information on that market or product or public. The important mathematical and practical difference in how we then develop our model of the problem to be understood, is how the sample information changes the prior probabilities. When we revise our initial or prior probabilities in this way, they become posterior probabilities. It is common in assessments /using this information, that the sampled information is viewed as being for or against a particular decision. For example you may undertake market research which finds that the public are favoured or unfavoured towards a new product or service, or that , as in Box 3.3, you may have a decision to make about when to act in the market (e.g. act now or do nothing now). To articulate this argument fully means we must also consider HOW the states of nature we are asking about relate to the findings of that market research (sampled information).

122
Download free eBooks at bookboon.com

Effective Management Decision Making Box 3.4: Bayes analysis, the Deck of Cards and the Faulty Good Test

Chapter 3

You draw a card, at random from a standard shuffled deck of cards. The probability of this card being a jack is 1/13 ( or P(Jack)=4/52). If however you are told that the card you have picked is a picture card then the probability becomes 1/3 (or P(Jack | Picture card) = 4/12).(Accessed from http://www.trinity.edu/cbrown/ bayesweb/ July 2011). Let’s assume that a good is being sold by a company which may or may not be faulty in use. You have a test which can be used to indicate if the good will become faulty in use. You wish to know the strength of the relationship between the test finding for a good and if that good then goes on to become faulty (i.e. how reliable is the test and should it then be something you use in all your products). So – you have experience which leads you to believe that 2% of your goods are faulty in service (on average).The test you have available can detect a future fault in 85% of the time (i.e. its effectiveness has been proven to be 85%). Sometimes the test generates incorrect findings (i.e. the test indicates a future fault but in service, the good remains faultless). Again, your experience of this suggests this occurs in 8% of sampled goods. Hence you have this table of data:
Good is faulty? Test is positive for a fault Test is negative for a fault 85% 15% Good is OK? 8% 92%

123
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Chapter 3

We interpret this representation of data as we know 2% of our goods are faulty. If we find a faulty good then there is an 85% chance our test will verify this. If we choose another good that we know to be fault free, then there is an 8% chance our test will say that it is (erroneously) faulty. So – suppose you now choose a good at random and test it and the test indicates it is faulty – what are the actual odds that the good IS faulty? With the test finding a fault we are concerned with the top row of the table above (i.e. the 85% and 8% data). We can evaluate this by stating: Chance of the good being faulty AND the test found the fault = 2% x 85% = 0.017 (or 1.7%) Chance of the good not being faulty AND the test found a fault = 98% x 8% = 0.0784 (or 7.8%) So going back to the original question of what is the likelihood that we have chosen a faulty good given the test indicates the good is faulty? P (Good faulty | Test reveals a fault) = 1.7% / (1.7% + 7.8%) = 17.9% (i.e. the outcomes of interest are ‘test actually reveals a faulty good’ divided by all possible outcomes (test reveals and faulty good and test reveals a fault in a non faulty good). So the likelihood of a good being faulty and revealed by the test is 18% (or there is an 82% chance that the good is not faulty). (Adapted from http://betterexplained.com/articles/an-intuitive-and-short-explanation-of-bayes-theorem/ July 2011). Bayes theorem is a famous mathematical explanation of this problem in Box 3.3 and Box 3.4 – i.e. it gives you the actual probability of a desired event given the results of a test event. It is often written out (in mathematical style as): P (A|X) = P (X|A) P (X|A) P(A)+ P (X|~A)P (~A)

Where: P (A|X) = Probability that A event occurring given supporting sampled information X P (X|A) = Probability that the sampled information was supportive and A event then occurred P (A) = Probability of Event A occurring P (~A) = Probability of Event A not occurring P (X|~A) = Probability that the sampled information was supporting and A event did not then occur. This equation is essentially asking the question of whether the probability of A is true given the probability of X being true. This is called a conditional probability – the probability that one event probability is true provided that another proposition is true as well.

124
Download free eBooks at bookboon.com

Effective Management Decision Making Bayes’ Theorem is stated as therefore: P (A)P(X|A) P(A)P(X|A)+P(B)P(X|B)

Chapter 3

P (A|X) =

And: P (B)P(X|B) P(A)P(X|A)+P(B)P(X|B)

P (B|X) =

Where A and B are mutually exclusive event outcomes and X is a test of the likelihood of an outcome. Alternatively, the tabular method (see Box 3.3) – is a more intuitive approach to understanding and using Bayes’ Theorem (and which also follows in Box 3.4). The table is constructed to identify and determine the relevant components of Bayes’ Theorem. So let’s assume we have 2 state of nature events (A and B). Each of which will of course have prior probabilities. We also have favourable evidence from some test about the likelihood that (for example) A and B will (or will not) occur – NB in the first table we will assume they will occur. Data for illustrative purposes has been used to populate the table 3.12.
Conditional Probabilities

Events

Prior Probabilities

Joint Probabilities

Posterior Probabilities

A B

0.75 0.25

0.05 0.09 SUM=
Table 3.12

0.0375(a) 0.0225(b) 0.06 (c)

0.625(d) 0.375(e)

Where (a) = 0.075 x 005 and (b) = 0.25 x 0.09 and (c) Sum of all event outcome probabilities that generate a (in this case favourable outcome of these events occurring) and (d) = probability of A event occurring given a favourable finding in the test (0.625/ 0.06) and (e) = probability of B event occurring given a favourable finding in the test (0.375/ 0.06).

3.7	Summary
This chapter has introduced and outlined simple probabilistic modelling. Beginning with a discussion on the nature of modelling, this chapter begins to move our analyses of decision making from a wholly rational and deterministic approach (as outlined in chapter 1 and continued largely in chapter 2 with data forecasting), to one of recognising and introducing uncertainty in that decision making. This uncertainty can be analysed based upon assumptions about the market or buyers etc as well as then undertaking revisions to those assumption in light of new market research evidence. In both approaches however, the solution set of discrete event outcomes remains small and manageable (in the sense that you can logically outline the progression of decision and outcomes). In the next chapter though, we start to consider how this becomes much more difficult when event outcomes might be reflect a very large number of possibilities, through the analysis of distribution functions.

125
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 3

3.8	

Key Terms and glossary

Bayes Theorem – is the probabilistic relationship between events after a given event has occurred. It is named after its founder, the Reverend Thomas Bayes. Conditional probabilities – are probabilistic statements and values describing the likelihood of an event occurring given another event has already occurred. They are computed using Baye’s Theorem. Decision analysis – is the phrase used to describe resolving problems which have limited or uncertain probabilities for events that could occur. Distribution function – describes a probabilistic relationship over which a large range of values are possible. Expected monetary values – describes the probabilistic payoff (or cost) that is associated with a given outcome for a chosen decision (or series of decisions). Modelling – is a general term used to describe the simplification of a problem in reality through the identification of key relationships – which can be mathematically stated (or otherwise represented). Optimal solutions – are solutions which are maximised. Perfect Information – describes the situation whereby all known events and their outcomes are fully understood.

26 destinations 4 continents
Bartending is your ticket to the world

GET STARTED

126
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Chapter 3

Posterior probabilities – describes probabilities that are assumed about a given situation and event, after new information on that situation has been determined Sample Information – describes the situation whereby market relevant information is determined to aid understanding of a given problem. State of nature – describes events which are mutually exclusive (such that if one event occurs the others cannot). Stochastic models – are models which rely upon random inputs.

3.9	

Chapter closing question

You have been hired to advise the management team of a very small energy station. The energy station is based on the East Coast of England, and uses a small number of tide and wind power generators to produce electricity which is supplies to the National Grid. Staffing levels are very small and most work is automated. Occasionally the management team recruit more local workers on a temporary contractual basis, paying the minimum wage (or thereabouts). The East Coast Energy Station management team is because of changing legal requirements, having to consider investing in a new generator this year (2007), ready for expected demands in Winter 2007 and Spring 2008. The generator would cost £5,000 to buy and install. They have analysed the situation carefully and believe that it would be profitable, providing the snowfall is heavy in the coming Winter. The team also believes it will make a reasonable profit if the snowfall is moderate, but it will lose money if the snowfall is light. Specifically, and as near as possible, the team estimates a gross income of £7,000 if the snowfall is heavy, £2000 if the snowfall is moderate and a £9000 loss if the snowfall is light. The latest long range forecasts from the Metereological Office are that the likelihood of a heavy snowfall in Winter 2007 is 40%, of a moderate snowfall is 30% and of a light snowfall is also 30%. As the consultant for the station: a)	 Draw a decision tree for the team of the energy station (25%) b)	 Determine the expected value at each state-of-nature node (15%) c)	 Based on this information, what would the expected value approach recommend the team does with regard to their investment? (5%) In addition to this potential acquisition, the team is also considering purchasing a smaller biomass energy converter to add to the existing generating capacity of the power station. The biomass energy converter would cost £1,100 to buy and install. This would allow the station to increase production of greener energy (but not as much as adding a new generator). In addition, as the biomass energy converter is slower to respond to changes of power demand, it is not as effective an addition to the capacity of the energy station to respond to peaks in power demand (for example, during the Christmas holidays).With this information, the team has determined that gross income will be increased for the station by £3500 if the snowfall is heavy, £1000 if the snowfall is moderate or incur a £1500 loss if the snowfall is light, if the biomass energy converter is installed. d)	 Prepare a new decision tree showing all three alternatives (30%) e)	 What is the optimal decision using the expected value approach? (15%) f)	 What is the expected value of perfect information? (5%)

127
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 3

Now consider if the team decides to wait until September, before investing, to check the temperature patterns before committing to a final decision. Estimates of the probabilities associated with an unseasonably cold September (U) are : P(U|s1) = 0.3, P(U|s2) = 0.2, P(U|s3) = 0.05 Where s1,s2 and s3 refer to the states of nature discussed in the problem. g)	 Give your recommendations if the team observes an unseasonably cold September or does not observe an unseasonably cold September (5%) (HINT – You may find Bayes Theorem helpful in part g)

.

128
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Chapter 4

Chapter 4
4.1	  eveloping rational models with quantitative methods and analysis: D Distribution Functions and Queuing Theory
For a majority of Business / Management students, the preceding chapter on probability is usually manageable and accessible to them, as they are able to relate and visualize the range of outcomes that might follow from a decision making context. However, that discussion represents a narrow and atypical environment, within which probability can be studied. It is one extreme and simplified interpretation of a much wider set of relationships that can be called distribution functions. The easiest method of considering this is to reflect on how useful plotting a decision tree would be if the outcome of a decision was a hundred (or more) possible pathways. This rational approach to determining decision and outcome nodes, then rapidly becomes cumbersome. Yet, the majority of decision situations for individuals and organisations to manage are of this type. Consider for example how you would plot the likelihood of a given customer attending your retail store on a given day? If your business was highly unique with only a few known customers world-wide, you might be able to construct a probability table based upon your knowledge of the market and customer needs therein. However, for the vast majority of organisations, this is simply not feasible. We are therefore left in a quandary about how to manage this kind of decision making problem. To address this, we need to consider some more mathematics.

4.2	

The mathematical function: discrete and continuous variables

Mathematicians categorise data distinctively, depending upon its origin and scope. For example, in the preceding chapter on probability, all the data we were presented with described fully all possible potential outcomes. We could therefore know exactly and describe fully a given decision context. When this is the situation, the data values used are called discrete random variables. Usually, this data is presented (and comes from decision contexts) as positive integer (whole number) data. For example, in a sample of 6 students chosen at random from a class of 30, how many of the size might have black hair? Clearly, the answer must be whole number and as we are concerned with the sample of 6, we can know and describe all outcomes. The use of discrete random variables is (probably) familiar to you from you research methods courses and studies (or similar). Other examples would be determining the number of defects in products produced on a production line, from a sample of 100, or interviewing 50 people at random in the street about who they voted for in the last national election. One easy way to think about this type of data, is that it is usually used to count, not measure items. Continuous random variables however are the opposite of this, in that they can take any value between two interval points – i.e. they measure rather than count. For example, in the same class of 30 students, now studying in an exam, the time taken to complete the exam by each student, can be any measured value from the start (time =0) to the scheduled end of the exam (t=end). Thus many measurements of a potentially infinite level of accuracy can be taken. Data gathered in this way and data gathered for the sampled black haired students, constitute different distributions of data. Hence: •	 Data gathered from discrete random variables construct discrete distributions of data •	 Data gathered from continuous random variables construct continuous distributions of data

129
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 4

The phrase ‘distribution’ often generates fear for students who have not met it before. It should not. It is simply a word used to describe how the data is arranged and presented. For example, let’s go back to our class of 30 and their exam and assume that the exam was such that each question answered could only be right or wrong. You ask one of the students to predict how likely it is that they will get X questions wrong and they generate this table for you:
Number of questions answered incorrectly 0 1 2 3 4
Table 4.1

Predicted probability 0.02 0.23 0.39 0.26 0.10

Clearly, from the above table, the interviewed students is 100% confident they will not score 5 or more questions incorrectly, but is less confident that they will score less than 5 questions incorrectly, as per the probabilities given. A plot of this discrete distribution function would then be:


Figure 4.1

Figure 4.1 illustrates the plot of the discrete distribution function of the data gathered. We would then be able to for example, determine the mean probability of the long run average of occurrences. This is where we move beyond asking one person their view and ask everyone in class – i.e. we have more than one trial (this is what is meant by the statement ‘ long run average’). So – the mean (μ or expected value) is given by the sum of the product of the number of possible incorrect answers and the likelihood of achieving those incorrect answers, or:

130
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 4

μ= Σ [(number of incorrect answers given in exam) x (probability of achieving that many incorrect answers)] OR More generallyμ= Σ [(outcome of interest) x (probability of that outcome of interest)] You can also easily work out the variance and standard deviation of the mean of a discrete distribution. If you are unfamiliar with these terms, then any introductory statistical text will guide you, although a short definition here would be: variance (σ2) – the measurement of how far numbers in a given dataset, are spread out from one another. Standard deviation (σ) is a closely associated term and describes how much spread there is in the data from the expected mean (determined above as μ). So: σ2 = Σ [( number of incorrect answers given in exam – μ)2 x (probability of achieving that many incorrect answers in the exam)] OR σ2 = Σ [( x – μ)2 x (P(x))] And the standard deviation is the square root of σ2 or: σ =√( Σ [( x – μ)2 x (P(x))] )

Think Umeå. Get a Master’s degree!
• modern campus • world class research • 31 000 students • top class teachers • ranked nr 1 by international students Master’s programmes: • Architecture • Industrial Design • Science • Engineering

Sweden www.teknat.umu.se/english

131
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making You can therefore easily construct a table of this data (using that from table 4.1) as:
Number of questions answered incorrectly (x)

Chapter 4

0 1 2 3 4

0.02 0.23 0.39 0.26 0.10

0.00 0.23 0.78 0.78 0.40

4.80 1.42 0.04 0.66 3.28

0.10 0.33 0.01 0.17 0.33

Hence mean=

2.19

Hence variance =

0.93

Hence standard deviation =
Table 4.2: Calculations for discrete distributions

0.97

This all seems good and interesting (!) – but over the centuries, certain types of discrete distributions of data have been identified and used professionally as they have patterns that can be used for modelling. These types of discrete distribution are more useful for the manager in particular to become familiar with. Let’s begin to investigate these firstly with the roulette wheel.

4.3	

The discrete distribution – the binomial function

Before we delve into the depths of mathematics further, let’s use an example you may be familiar with, to explore some important implications of discrete distribution functions.

132
Download free eBooks at bookboon.com

(x-mean) squared multiplied by P(x)

Predicted probability (P(x))

x multiplied by P(x)

(x-mean) squared

Effective Management Decision Making

Chapter 4

A gambling roulette wheel has a fixed number of slots (numbered) into which the spun ball can fall. When the ball falls into one of those slots, it cannot occupy any other (i.e. these are mutually exclusive events). The number of numbers which can be selected will not vary (i.e. there are only 37 numbers so the probability of a number’s slot appearing or not, will not change in a single trial) and therefore in subsequent spins of the wheel, whatever number appeared previously, is equally likely to appear again (subsequent events are independent and they are states of nature as we have previously discussed). So far, so good. What if we were looking to determine how often a given number may appear in N spins of the roulette wheel? We can rephrase this slightly (in a more mathematical sense) and instead ask what is the probability of R successes (our number appearing) in N attempts or trials (N spins)? I.e. How many times would the number 3 appear (for example) in 5 spins? So – we can write this as: P(exactly R successes)=P (R successes AND (N-R) failures) OR P(exactly R successes)=P( R successes) x P(N-R failures) If we now start to generate the simplest case here and assume that P (our number appearing) =p and therefore P (our number not appearing)=q, we must have that for any single event (i.e. one spin of the wheel) p+q=1. We spin the wheel 5 times (N=5) and we have asked to find out the probability of seeing the desired number (R times) in N spins. Now one possible outcome of these five spins is that we ONLY see our number appear (of the 37 possible choices). In you recall from Chapter 4, we know that the relationship between subsequent independent events is calculated using the logical operator AND – so that the P (3 in the first spin, 3 in the second spin, 3 in the third spin, 3 in the fourth spin and 3 in the fifth spin) is going to be given by the value: P (3 in ALL spins (or R=5)) = p5 (i.e. our chosen number probability multiplied by itself 5 times) OR P (R successes in N trials) = pR But we might also see failures (when our chosen number does not appear (which is much more likely!1)) - hence the probability of failure in N trials is going to be given by:

1	You can work out that the probability of seeing the number three appear, each time, in 5 spins is 0.0000000144 (or (1/37)5). Hence it is much more likely that you will not see our number 3 at all in any of the spins.

133
Download free eBooks at bookboon.com

Effective Management Decision Making P((N-R) trials being failures) = qN-R.

Chapter 4

If we now combine these probabilities and ask the question of what is the probability of seeing R successes in a given number of N spins, we are then exposed to both successes and failures in those N spins – or: P( The first R trials being successful and the next N-R trials being failures)= (pR * qN-R). Although this calculation will generate a probability of seeing a single given sequence of successes and failures in N spins, we must also recognise that this sequence (say of seeing our number 3 times in 5 spins) can be achieved by a variety of combinational possibilities. Hence we must also ‘count’ this number of potential other valid combinations of our ‘success’ (i.e. seeing our number 3, 3 times in 5 spins). Fortunately, another branch of mathematics allows you to easily determine the number of possible combinations of number orders we could see (when in this case, we are not concerned with what particular order our number appears in those 5 spins). This is given by the calculation: N! R!(N-R)!

N

CR =

So- in the equation above, there are R things (successful appearances of our number 3) to chose from in N possible outcomes and if the order of that selection does not matter (as is the case here), then clearly, there are many potential ways of presenting that selection (these are called combinations). The calculation above divides the factorial of N (the number of spins) by the product of the factorial of R (the number of successes we need (i.e. our number to appear three times) and the factorial of the difference between N and R. Thus for our scenario, the calculation will be:

How could you take your studies to new heights?
By thinking about things that nobody has ever thought about before By writing a dissertation about the highest building on earth With an internship about natural hazards at popular tourist destinations By discussing with doctors, engineers and seismologists By all of the above

From climate change to space travel – as one of the leading reinsurers, we examine risks of all kinds and insure against them. Learn with us how you can drive projects of global significance forwards. Profit from the know-how and network of our staff. Lay the foundation stone for your professional career, while still at university. Find out how you can get involved at Munich Re as a student at munichre.com/career.

134
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Chapter 4

N

CR =

5! 3!(2)!

=

(5x4x3x2x1) 3!(2)! (3x2x1)(2x1)

OR NCR = 10 Hence there are 10 possible combinations where we will see our number appear 3, appear 3 times in 5 spins – e.g. (3,3,3,X,X), (3,3,X,X,3), (3,X,X,3,3), (X,X,3,3,3) (X,3,3,3,X), (3,3,X,X,3), (3,3,X,3,X), (X,3,X,3,3) (3,X,3,X,3), (3,X,3,3,X) Hence there are NCR possible sequences of R successes and (N-R) failures, each with a probability of pR * qN-R. The overall probability of R successes in N trails is therefore: P (R successes in N trials) = NCR * pR * qN-R This type of distribution, where the range of number outcomes is extracted from a discrete (finite) set of possibilities, is known as the Binomial Probability Distribution. It is a very important number distribution in mathematics that has applications for managers. Binomial means there are only two possible state of nature outcomes to consider and that each trial (attempt or spin of our roulette wheel, is independent of whatever outcome has been previously observed). Consider for example the following problem: A salesman knows historically he has a 50% chance of making a sale when calling on a customer. If he arranges 6 visits determine the following: 1.	 What is the probability of making exactly three sales? 2.	 What are the probabilities of making any other numbers of sales? 3.	 What is the probability of making fewer than three sales? We have a discrete distribution profile (6 events) with 2 states of nature outcomes possible per event (a sale or no sale) and we know that, based upon historical records, the salesman has a 50% chance of securing a sale (one state of nature) per event (the visit). So we know we can apply the binomial distribution to understand this saleman’s endeavours. Let’s assume that the success of each visit (P(success)=0.5 or P(p)=0.5). We know N=6 (i.e. 6 trials (or visits for this example). The first question asks to determine the probability of securing 3 sales from those 6 visits. This answer is not, as you perhaps might first think just 50%, but is determined by P(R=3)= 6C3*(0.5)3*(0.56-3)=0.3125 or in longhand ((720/36)*0.125*0.125). Similarly, the probability of making any other number of sales means we would need to determine either the following:

135
Download free eBooks at bookboon.com

Effective Management Decision Making 1)	 P (1 sale OR 2 sales OR 4 sales OR 5 sales OR 6 sales) 2)	 P (1-(no sales OR 3 sales)) Let’s work out (1) and (2) to illustrate they are in fact the same. P(R=1) = 6C1*(0.5)1*(0.56-1) = (6)(0.5)(0.03125) = 0.09375 (or 9.4%) P(R=2) = 6C2*(0.5)2*(0.56-2) = (15)(0.25)(0.0625)=0.23475 (or 23.5%) P(R=4) = 6C4*(0.5)4*(0.56-4) = (15)(0.0625)(0.25) = 0.23475 (or 23.5%) P(R=5) =6C5*(0.5)5*(0.56-5) = (6)(0.5)(0.03125) = 0.09375 (or 9.4%) P(R=6) = 6C6*(0.5)6*(0.56-6) = (1)(0.015625)(1) = 0.015625 (or 1.6%)

Chapter 4

Hence the probability of making any number of sales EXCEPT exactly 3 will be the summation of the above probabilities (0.6726 or 67.3%). We arrive at the same solution if we now solve (2) above: P(no sales) = P(no success hence R=0) = 6C0*(0.5)0*(0.56-0)= (1)(1)(0.015625)=0.015625. We know the probability of exactly 3 sales successes as 0.3125. Hence we add these two vales together (in other words (1- (P(no success)+P(3 sales exactly)) = 1-0.328 = 0.67 (or 67%)). Hence the final question above, is simply determined by evaluating P(no sales OR 1 sale OR 2 sales). Question - One final example to consider is to determine the following solution: in 2010, students on the business degree programme were sampled about their views on the support they had received during their studies of ‘Management Decision Making’. At that time 75% said they thought they had received ‘good’ support. Assuming this figure remains valid for 2011, 2012 and 2013 – let’s consider that if the class of 2011 is sampled (say 34 out of a class of 54) that they will also express the view that they have received ‘good’ support during their studies? Solution P(success (or that the student felt they received ‘good’ support)) = 0.75 = R P(q failures (i.e. students commenting another view of their module support) = 0.25 For the 2011 class, we have 54 trials and wish to know the probability that 34 comments will be successful – hence we can use the equation:
54

C34*(0.75)34*(0.25)20)=3.213*1014 x 5.6504*10-5 x 9.0949*10-13 = 0.0165 (or 1.65%)

136
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 4

Hence it is very unlikely that for 2011, the tutor would see exactly 34 of the student’s commenting on the ‘good’ support received during the module of study. On average (in the long run (i.e. with repeated sampling)), the tutor can expect to see (0.75)(54) or 41 students that comment on their module’s ‘good’ support.

4.4	

Extending binomial number sequences

As a mathematical probability distribution of discrete numbers, the Binomial number sequence can be expanded. Let’s take now the generic example from above and expand this: P (R successes in N trials) = NCR * pR * qN-R P(R=0)+P(R=1)+P(R=2)+P(R=3)+P(R=4)+P(R=5)+P(R=6) This describes the full discrete distribution and can therefore be written as: P(discrete distribution) = 1P0Q6+6P1Q5+15P2Q4+20P3Q3+15P4Q2+6P5Q1+1P6Q0 (Which, you may have noted, is the expansion of (p+q)6) Clearly, this is useful providing we have a manageable limit on the value of N (and subsequently R). Let’s look at some further examples, before we consider what happens when N becomes very large.

Scholarships

Open your mind to new opportunities

With 31,000 students, Linnaeus University is one of the larger universities in Sweden. We are a modern university, known for our strong international profile. Every year more than 1,600 international students from all over the world choose to enjoy the friendly atmosphere and active student life at Linnaeus University. Welcome to join us!

Bachelor programmes in Business & Economics | Computer Science/IT | Design | Mathematics Master programmes in Business & Economics | Behavioural Sciences | Computer Science/IT | Cultural Studies & Social Sciences | Design | Mathematics | Natural Sciences | Technology & Engineering Summer Academy courses

137
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Chapter 4

Suppose you are a quality inspector working on a production line and you are sampling the finished items at random for defects. You know from your historical data (which could be from yesterday for example) that the probability of selecting a faulty item at random is 5%. If you select an item and then don’t put it back into the sample, can you be said to still be following a binomial distribution of faults found in your selection? (i.e. of locating faulty ‘successes’ in your N selections (trials)?). Well no and yes. The ‘no’ response comes from the fact that by not replacing the item back into the population, the subsequent trial is not now independent of previous trials (as that trial event has affected the probability distribution of the remaining items in the sample and whether they may or may not be faulty). However, if your population sample is large (say you are selecting buttons at random from a large box of finished buttons) the impact of not replacing that one button when there are thousands still to be trialled in the box, is not significant. Black (2004) recommends that as a general guide, when the sample size n is less than 5% of the population size, we should not be concerned about the dependency of events (if the sample if not returned to the population) and can then view sampling as a series of independent events. If however, the sample was say 20% of the population – then we can no longer assume that the sampling events are independent. The impact of a sample being taken will therefore affect the likelihood of a faulty part (or the event of interest) being observed in the next subsequent sampling undertaken. In addition of course, we are also implicity concerned with very large values of N (and hence continuous distributions). In these situations, we need to reconsider the binomial sequence of numbers – and generate the Poisson sequence.

4.5	

The Poisson sequence

This sequence of probabilities (numbers) is named after the French statistician Simeon-Denis Poisson, who published the essence of the formulation in 1837 (citing Napoleonic Cavalry survival (as returns to their stables) as empirical data!). The difficulty of explaining and discussing this number sequence and its value to managers and students of business, is that most texts on this topic, do not explain clearly how the Poisson distribution function is derived from the Binomial as the mathematics tends to be relegated to more rigorous mathematical texts. There is instead, a focus upon the value of the application of the sequence in practical organisational contexts and in the management and allocation of resources. This latter point is appropriate and most students can make the transition between the two distribution functions without needing to understand their linkage. For some, it is simply a matter of faith and then matching contexts to which interpretation of the distribution function to adopt. However, in the author’s experience, a detailed explanation of the origin of the Poisson distribution helps to understand its value to managers and so the following section will present this. You are of course welcome to skip ahead to the section afterwards if you feel familiar with this derivation.

Deriving Poisson from the Binomial distribution function
You may recall that we can present the binomial function as the following generalized relationship:
N

CR * pR * qN-R

Some texts present this as:

138
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 4

Where the first bracket of n over k represents the combinational value previously discussed, k represents R (the success events of interest to us) and (n-k) is the same as N-R (i.e. number of trials less the number of successes of interest to us). Let’s break one of the conditions in which the binomial function works by saying we DON’T know what N will be (i.e. we don’t know how many trials or sampling we will undertake). We do know however, the average success rates we see over a time period (i.e. we know from our records that a shop may sell 20 pairs of shoes in a day). We therefore know the rate of success per day (in this example) but we don’t know the original probability of success occurring in a given number of trials (i.e. we don’t know R (or K above) or N). One method of progressing this uncertainty is to define a new variable to reflect the RATE of successes we see in the uncertain sampling we undertake. So if we have N trials and the probability of success in each trial is p, then the rate of success(λ) we see will be: λ = N*p If you have difficulty seeing where this equation came from, consider the simple example of tossing a coin. If you toss a coin N times and a success is when you see a ‘head’ then your rate of success is N*0.5. Tossing a coin 10 times in succession and seeing a ‘head’ ten times is then given by the value of (10*0.5=0.05). In other words you only have a 5% chance of this sequence occurring, so that λ is then the success rate of this sequence (but don’t forget you may also be concerned with other ‘success rates’!- hence the need for the combinational calculation to be retained in the Poisson distribution). If we go back to our binomial equation and now insert this new relationship, we see that:
N

CR * (λ /N)R * (1- (λ /N))N-R

In the above equation, we have just eliminated p and q through substitution. Now let’s also go back to the implicit assumption in the above equation and our earlier statement that we don’t know what N is – but we are assuming it is a very large number – so large that we are not concerned with discrete data, but with continuous data. In this situation we assume that N tends towards being infinitely large. If you are familiar with your mathematics, you may have come across this concept before that we take N to the limit. This is written in the following way (and by expanding the combination calculation):

OLP1ǌ15ǌ115 1ń515
Now as with any equation where only one variable is changing (here N), the other values are in effect constants (i.e. as we increase N, the values of R and λ in effect, remain unchanged. We can therefore re-order the equation and extract those values which are not affected by the limit (as they are regarded as constants). In doing this our equation changes to:
E35VXFFHVVLQ1WULDOVLQWKHOLPLW

D35VXFFHVVLQ1WULDOVLQWKHOLPLW

ǌ5OLP1ǌ11ǌ15 51ń1515

139
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 4

The expansion of the RHS of equation (a) – is shown in (b) – if you doubt this – substitute some numbers into (a) for λ, N, and R and then for (b). You will find they give the same solution. To progress this further and work towards the next probability distribution by changing the assumptions in the binomial distribution, we need to finish the expansion of (b). If you expand the following portion of the RHS of (b):

FOLP1 1111«15 1ń1515151515«15
OR (as a majority of the numerator values cancel out the denominator values):

GOLP1 1111«15 1ń151515
OR by expanding the equation in (d) – through recognising that both numerator and denominator are simply powers of R to:

HOLP1 1111«15 1ń151511111
If you now imaging that N becomes infinitely large – then each value in (e) will tend towards 1.

Cyber Crime Innovation

Web-enabled Applications

Are you ready to do what matters when it comes to Technology?

140
Download free eBooks at bookboon.com

Data Analytics

Implementation

Big Data

.NET Implementation

Click on the ad to read more

IT Consultancy

Technology

Information Management

Social Business

Technology Advisory

Enterprise Application

Java

SAP

Cloud Computing

CRM

Enterprise Content Management SQL End-to-End Solution

Effective Management Decision Making

Chapter 4

We can now consider the remaining part of the equation in (b) i.e. the calculations now after the ‘limit’ operation and firstly, the equation part of: (f) - (1- (λ /N))N

35VXFFHVVLQ1WULDOVLQWKHOLPLW ǌ5OLPǌ11ǌ15 51ń
To resolve this – we need to use a piece of mathematical sleight of hand – by recognising that the part equation given above in (f) looks similar to the following derivation of ‘e’ (to which the solution is known (NB – we will meet ‘e’ again shortly, but for a fuller discussion please review Chapter 2 and see later in this chapter):

JH OLP[[ [ń
Now recall that we initially set up a value to describe our RATE of success (R) in N trials as: λ = N*p If we rearrange this slightly – so that we obtain and equivalent expression: -N/λ = -1/p and let this equal x If we then substitute this equation for x (and hence p) into (g):

KOLPǌ11 OLP[[ǌ 1ń
Now if you compare (h) with (g) it is easy to see that:

LOLPǌ11 OLP[[ǌ Hǌ 1ń
Hence equation (b) has now become:

35VXFFHVVLQ1WULDOVLQWKHOLPLW ǌ5H²ǌOLPǌ15 51ń
We now need to expand the final part of the limit in the equation above i.e.:

141
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 4

OLPǌ15 1ń
Clearly, as N approaches infinity then this equation reduces to (1-R) = (1). If we now substitute this final part of the expansion, back into equation (b) – we derive the following:

35VXFFHVVLQ1WULDOVLQWKHOLPLW ǌ5H²ǌ 5
OR P( Of observing R successes give a rate of success λ in a given period) = (λR * e –λ)/ R! (j) This is known as the Poission Distribution Function – and as you have seen, can be derived from the binomial distribution function – by relaxing some of the key assumptions of that distribution (namely that N is very large and we only know the RATE of success achieved in a given time period). Phew! Now you may be wondering that this is interesting, but what is the value of it? How does it help you as a student of Business or a professional manager? The answer is that unlike the binomial function, this distribution allows you to view the ‘success’ of an event happening not from a probability calculation but only from its frequency in the past. It therefore can describe rare events or unlikely events against a ‘normal’ context of them not occurring. Put another way, you can use the number sequence described by (j) to help you understand discrete events occurring against a continuous background of them being very unlikely to occur. So it can be used for a manager to help them forecast demand (and hence resourcing needs). It will help you answer for example the following situations: 1)	 How many customers will call in for Pizza at the local Pizza delivery shop on any given evening – (i.e. customers calling = success against an almost constant background of customers not calling (if all potential customers did call in – you would need a very large shop!)) 2)	 The number of paint flaws evident of new cars sold in a given period 3)	 The rate of arrival of aeroplanes at an airport requiring landing 4)	 The incidents of accidents recorded at a factory over a given period. You can see that the distribution characteristics described by the Poisson function – where you have a low observed success rate against a background of (much more likely) non success rates – can be used in many areas of Management. Black (2004) advises that in using the Poisson distribution function – you must however be confident that the rate of success λ (e.g. customer arrival, paint flaws observed etc) remains relatively constant over a given time period (or at least the period in which you are concerned). This may mean for example that λ would vary between Monday’s and Friday’s for the Pizza Manager and any staffing rota thus produced to manage demand – should reflect this variation in λ.

142
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 4

4.6	

Queuing Theory

Now much of the use of Poisson distribution function in Business – is categorized as Queuing Theory – in other words as managers, we are concerned with the effective use of organisational resources to manage demand, quality or shipping for example – where allowing arriving customers or departing goods to wait – both costs money and potential results in lost future business. We therefore need to manage arrival queues or departing goods. The Poisson distribution function therefore allows us to visualise customers arriving or goods departing as ‘successes’ against an almost continuous background of their non arrival or non departure. It has found particular value in minimising the occurrence of ‘bottlenecks’ in organisational functions (such as manufacturing, customer service etc). Let’s take a simple example: (Q)	 A construction company has had 40 accidents in the past 50 weeks. In what proportion of weeks (i.e. how many weeks) would you expect 0,1,2,3 and more than 4 accidents to occur? (A)	 A small number of accidents occur, at random (this is our assumption and these are our ‘successes’). We are not interested in the accidents that did *not* occur, so we can use the Poisson distribution function. (B) We can determine that the Mean number of accidents/week (λ = rate of accidents (i.e. our ‘successes’)) = 40/50=0.8 per week.

AXA Global Graduate Program
Find out more and apply

143
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Chapter 4

So if P(0) is the probability of seeing a week with no accidents (i.e. no success), we substitute R=0 and λ =0.8 in equation (j), which will give 0.449 (satisfy yourself of this) You can then repeat this to determine P(1 accident ‘success’), P(2 accident ‘successes’) ,P(3 accident ‘successes’) and P(4 accident ‘successes’) – (NB your answers should be 0.359,0.143, 0.038,0.0076). This use of the Poisson Distribution function was first proposed by Danish engineer A.K.Erlang at the turn of the 19th century when the problem of telephone line congestion was analysed (and the implications of requiring a customer to wait for service, what a reasonable waiting time would be for a given customer in an organisation and how fast a customer can be served etc). Each of these problems has a natural trade off in terms of resourcing and service – and this is the heart of the issue for management decision making. Should an organisation invest in large plant and machinery to ensure it can produce – with minimal waiting times – a given product for a consumer? It would for example be attractive to a consuming customer to be able to always walk into a pizza retailer and – regardless of the time of day, day of the week or time in the year – receive instant service. This is possible and could be done – but the resourcing required to deliver this is not attractive enough for the retailer (unless they then charge a price large enough to support this level of service). Would you pay >£100 for a pizza with this guarantee of service? If we now start to bring together some of these threads discussed so far – we find we have a small number of fundamental concerns to work with. Fundamentally, the length of any queue for service (when the context of that service satisfies the use of the Poisson distribution function (i.e. there are a very large number of ‘trials’ and the probability of a success in any one of those trials is very small)) is dependent upon: 1)	 The rate at which ‘customers’ arrive to be served – called the ARRIVAL RATE (λ) 2)	 The time taken to serve that ‘customer’ – called the SERVICE RATE (μ) 3)	 The number of outlets / servers available to service a customer- called the number of SERVICE CHANNELS (this can be labelled as k or s (depending upon the materials you are reading)). Remember – we have assumed (in order to use this probability distribution function) that the arrival of customers/parts etc in a system is (largely) a random event. In stating this, we have therefore assumed therefore the probability of the arrival of a given or specific customer or part is very small (tending towards zero) whilst the number of potential customers / parts that could arrive is potentially infinite (or very large). We must also take into consideration that the service rate (how quickly a customer is served or a part dispatched/auctioned for example) is described by a continuous random distribution. We usually assume this is determined from the exponential distribution. This type of distribution is continuous and describes the times between random occurrences (rather than with the Poisson function which is discrete and describes random occurrences over a given time interval). Let me explain this further (and review the constant ‘e’ introduced earlier in this chapter).

144
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 4

Anyone employed to complete a task that has some regularity to it – will take a similar but not always exact time, to do that task. The more tasks that are added to the work to be done, then the more likely a longer time will be needed. Clearly, if you were to undertake an experiment to time the duration needed to complete tasks – there would be some variation about a mean and some relationship between the passage of time and the number of tasks that could be undertaken in that time. Take for example, loading a car with luggage. Different people – because of natural variation – would take different – but similar lengths of time to do this. Adding more or less luggage would also vary the duration of time needed to complete the task. You could plot these variations in time on an x,y graph and you would be able to visualize a distribution function of the SERVICE rate for that task. Let’s say you are really bored one day and find a friend ‘willing’ to load (and then unload) your car. You give him a set of suitcases and time his loading. The mean time you measure over say 20 trials is 15 seconds (this would be the service rate (μ) for this completion of the task). You know from your data that although the service rate had this mean, there would have been some variation in those times. Each different value obtained for μ therefore generates a different distribution function. So there would be (potentially) an infinite number of distribution functions for each task to be serviced (although some would be very very unlikely). The exponential function arises naturally when you therefore try to model the time between independent events that happen at a constant average rate (μ). We know that modelling, as discussed earlier, is a mathematical representation of an apparent set of relationships between data. There will always be some variation in the probability of a task needing to be serviced and it has been found that using the exponential function ‘e’ is a good way to model these observed relationships. So what is ‘e’? The meaning of ‘e’ ‘e’ is a rather unique mathematical constant. If you recall some basic calculus relationships - we could start to describe this uniqueness by saying that the derivative of the function of ex is itself – i.e. e. Think of it this way, velocity is nothing more than a measure of how fast the distance travelled changes with the passage of time. The higher the velocity, that more distance that can be travelled in a given time (and of course the reverse is also true). Velocity is then said to be the derivative of distance with respect to time (you may recall seeing this written as ds/dt at school?). All derivatives (of this first order) are therefore just measures of change in one variable with respect to another. However amongst this range of derivatives, there is one unique value where the rate of change in the one variable (say distance) with respect to (say) time is the same as that variable (say distance). Look at this data of distance covered (metres) vs time taken (seconds):
S m 10 10 20 20 30 30 40 40 50 50 60 60 70 70 80 80 90 90 100 100

Clearly an x,y plot would give this graph as a straightline – i.e. the rate of change of distance with respect to time is constant. The gradient of this plot is fixed and is described by therefore a single constant velocity – which is the derivative of this data (i.e. in this case 1 metre per second (or change in distance/change in time)). Let’s now look at another set of data which shows a different relationship between distance (height in cm) and time (days) – this could be for example plant growth from a seed.

145
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 4

S(cm) T (days)

10 0.1

20 0.2

30 0.4

40 0.8

50 1.1

60 2.3

70 4

This plot would look like:



146
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Chapter 4

Clearly, we do not have a constant growth rate – but that the growth rate seems to be increasing with the passage of time. The derivative here (i.e. as a measure of the rate of change of height with respect to time) is not going to be a fixed value… or is it? Compare this data with some other data that seems to show a changing height vs time relationship:


Both curves look similar – they both increase with time (y values become larger) – but they seem to be increasing at different rates – which is certainly clear from say the 50th day onwards. Both curves therefore are described by a different relationship between height and time. The bottom curve – if you consider the data – is described by the following:
S(cm) T (days) 0.01 0.1 0.04 0.2 0.16 0.4 0.64 0.8 1.21 1.1 5.29 2.3 16 4

By inspecting the data – you can see that s is simply the square of t – i.e. s=t2. Hence if this data did occur in reality, our mathematical model would be s=t2. We can work out the velocity by considering the rate of change in distance over any given period of time – this is (as noted above) often shortened to ds/dt (or more accurately δs/δt – where the symbol δ means ‘small change in’). To save many different calculations being made by considering these small mutual changes, it has been proven that the derivative of this equation would be simply 2t (if you need refreshing on your basic differential calculus – please pick up a basic maths text book at this time!). What this means is that ds/dt (here velocity) can be modelled by the equation 2t – so if t was say 0.5 (half a day) then the growth rate here would be 1cm at that point in time (i.e. 2x 0.5). Now going back to the initial argument identified earlier about the value of ‘e’ – ask yourself what if rather than s=t2, the growth of the plant was shaped by its already evident growth – i.e. if we assume its growth at day 1 is set to 1 (arbitrarily), and its growth each subsequent day was determined by how much it grew on the previous day: (l) s=Growth at time t = (1+ 1/n)n

147
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 4

So – just like before, growth is some power relationship (here simply given the label of n) – but that the current observed growth is proportional to its existing growth. If you chose a fairly large value of n (say n=50) and plotted s (using (l)) – you end up with this graph:


Increasing the value of n does not change the general shape of the curve (plot it yourself to see). The rate of change of s in (l) is s! So if we decided that ds/dt could be found by viewing s as equal to et, then ds/dt = et aswell. In other words, a mathematical modelled relationship where the rate of change is proportional to the relationship itself is called an exponential function and then only called ‘e’ where its derivate is the same as the function itself. It happens that ‘e’ has an indeterminate value of 2.71828.... (to 5 decimal points) and is itself derived by solving – at the limit equation (l). Now – after that mathematical interlude – if we return back to our car loading (generous!) friend, we stated that the service rate (μ) for this job was timed (mean) at 15 seconds – but that each different value obtained for μ – would generate a different probability distribution function (i.e. the probability that our friend could complete the loading task in a faster or slower time). Our friend works at a service rate of 15 seconds to load the car and suitcases can be placed in front of him (independent events) at a rate of λ. The exponential function ex (which is written in excel as EXP(X)), can be used to model this service relationship (where the rate of work (tasks served) is proportion to the tasks to be completed). If we let β = 1/ λ (otherwise described as μ), then we can write the probability distribution function (f(x)) as: (m) f(x) = β.e- βx

148
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 4

Remember – in using this mathematical relationship to model service rates for a given event(s) occurrence, we have assumed a continuous distribution (i.e. a potentially unknown number of events could occur), there is an (infinite) family of service distributions that could occur (where we usually focus upon a mean μ to describe one service rate distribution), this equation (m) will generate x values that peak at the apex (when x=0) and which gradually rises (but proportionately so) as x increases). Review the following graphs (figure 4.2 and figure 4.3) to convince yourself of the validity of choosing this mathematical relationship to explain the probability of a waiting customer being served / actioned and the probability of the service being completed by some time period t.


Figure 4.2 – Exponential probability plot (for an arbitrary service time)


Figure 4.3 – Exponential probability plot (for an arbitrary service time)

149
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 4

4.7	

Examples of poisson problems

Consider this question - What is the probability that our generous friend can load the suitcases into the car in less than 10 seconds? The area under the curve described by (m) represent all the probabilities included between two x points (i.e. it includes all probabilities that generate that area of the curve in equation (m)). We also know that as this is probability relationship – the whole area under the curve must equal 1 - hence to answer the question, all we have to work out is the probability of 1 minus the sum of all other probabilities that result in the car being loaded in less than 10 seconds – i.e.:
;  ; 

3ORDGLQJWLPHVHFRQGV ƴǍH²[Ǎ

Or:
;  ; 

3ORDGLQJWLPHVHFRQGV ǍH²[ǍG[

Just as with the derivative of the exponential function, the integration of it (to determine the area under the probability curve that lies between time x=0 and time x=10) is also the same function (multiplied by any constant value in the power of the exponential) – hence the integration calculation reduces to: P(loading time < 10 seconds) = (1/ μ )( μ )(1-e –10/ μ) In other words, the first integration when x=0 has reduced to 1 and the second integration for x=10, has become e –10/ μ. You will find that most textbooks simply give you this formula as: P(event of interest< event maximum time (x)) = 1-e –x/ μ As we know μ=15, we can now substitute values so that we derive: P(loading time < 10 seconds) = 1-e –10/ 15 or 49% (if you evaluate the equation above). In other words there is a 49% chance that our generous friend will be able to load the car (complete the task) in less than 10 seconds (given a mean service rate of 15 seconds). Similarly, if you wanted to know the probability of our friend completing the task between 20 and 25 seconds (maybe he’s not feeling well...) you need to first determine the probability of completing the task in less then 20 seconds and then in less than 25 seconds. Subtracting one probability from the other will then be the area under the curve and be equal to the probability of the task being completed in 20<task<25 seconds. Hence:

150
Download free eBooks at bookboon.com

Effective Management Decision Making 1)	 P (loading time<20 seconds) = 1- e –20/ 15 = 0.263 2)	 P (loading time<25 seconds) = 1 - e –25/15= 0.188 3)	 P (20 seconds < loading time < 25 seconds) = 0.263-0.188 = 0.0747 (or 8 % rounded up)

Chapter 4

So- these ideas are fundamental to how managers can evaluate and resource different systems, where queuing is a potential problem and would arise due to differential arrival and service rates for a given process. Clearly, as we have said queuing cannot be eliminated without significant resource expenditure (which is unlikely to be supported by the business case). But as a manager, you can simplify and model the queuing process, to minimize ‘bottlenecks’ and unnecessary (and potentially therefore costly) waiting periods. Let’s work through some more examples, before we begin to formulize and professionalize the language concerning queuing.

4.8	

More examples of poisson problems:

Freddie Madeup is a stock trader – who deals with transactions as they arrive on his desk. These transactions have a mean arrival rate of (λ) 20 per hour (or one per three minutes on average). Freddie’s work efficiency means at this time, his service rate (μ) is two minutes per order. (Remember that for a queuing problem to be resolvable μ must be less than λ, for the process to be able to be ‘cleared’ – otherwise the queue would grow infinitely long!). You can now determine the following (answers are provided underneath): 1)	 What is the probability that no orders are received within a 10 minute period? 2)	 What is the probability that exactly four transactions are received within a 10 minute period? 3)	 What is the probability that more than 10 orders arrive within a 10 minute period? 4)	 What percentage of transactions will take less than 60 seconds to process? 5)	 What percentage of transaction require more than 4 minutes to process? Answers: 1)	 Using (λR * e –λ)/ R! Or (10/30)x(e-10/3)/0! = 0.0356 (or 3.6% chance) (Hint – recall that although we are given the mean arrival rate at 20 per hour, (or one per three minutes on average) and we are concerned with a 10 minute period of that hour (or 1 sixth). Hence the mean arrival rate (λ) of transactions for the period of interest is 10/3). 2)	 Again we use the same equation of : (λR * e –λ)/ R! where exactly 4 transactions are received in 10 minutes (so that λ = (10.3) again so that : (10/3)4 * e –10/3)/ 4! = 0.1835 (or 18% chance). 3)	 This answers relies upon determining P (x>8 orders in 10 minutes) and therefore implies P(x>10) = 1-P(0)P(1)-P(2)-P(3)-P(4)-P(5)-P(6)-P(7)-P(8) – which you can work out using the equation and process in (3) to be: P(x>10) = 1-0.0356- 0.1189-0.198-0.220-0.183-0.122-0.067-0.032-0.031= 0.99257 or 99.2% chance. 4)	 This solution requires you to use the equation (P(x<x0)= 1-e –x/ μ noting λ=1/ μ so that: P(x<1 minute (1/60))= 1- e(-(1/60)*30)) = 0.135 (or 13.5% chance. 5)	 Finally, this solution follows from (4) but noting that P(T>4/60) = 1-(1-e(-30*(4/60))) = 0.135 (or 13.5% chance.

151
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 4

4.9	

Queuing Theory – Modelling reality

We noted earlier that the key features of understanding and modelling queues – reduces to a number of model parameters (if you assume certain probability distributions for the arrival of an event of interest and the rate at which that event is serviced. Thus recalling key parameters as: 1)	 the rate at which ‘customers’ arrive to be served (A) 2)	 the time taken to serve that customer (B) We can also note that (1) and (2) will be shaped by how many staff are available to service the arrival of events of interest. Hence we can also add, that any model we develop for managing a queue process should also include: 3.	 the number of outlets / servers available to service a customer.(k) Some texts and authors (see Anderson et al, 2009 for example) therefore present these 3 parameters in a shorthand format to describe the resultant queuing model developed as A/B/k = where A,B and k are used to denote the assumption describing the arrival rate (is it Poisson?), the assumption denoting the service rate (is it exponential?) and the number of channels available to service the arrivals (sometimes also given the letter of s). Usually – for most management problems we assume that the distribution of arrival events is a random event and thus we can, in the limit use the Poisson distribution to model them. Equally, service time is also usually a random event which when it occurs, takes an average length of time and follows an exponential distribution function. We defining a ‘queue’ as a single waiting line consisting of arrivals, servers and differing waiting line structures. For example, an elevator is an example of a first come first served (FCFS) waiting line structure. There are other waiting line structures which may assign members of the queue different priorities and then serve those first (such as a hospital Accident and Emergency department). You can visualize the relationships between these parameters as:

152
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 4

  
dŚĞ͚ƌĂŶĚŽŵ͛ĐĂůůŝŶŐƉŽƉƵůĂƚŝŽŶ


ƌƌŝǀĂůǀĞŶƚƐ;WŽŝƐƐŽŶW&Ϳ

    

tĂŝƚŝŶŐůŝŶĞƐƚƌƵĐƚƵƌĞƐ

^ĞƌǀĞƌ;ƐͿͲĞǆƉŽŶĞŶƚŝĂůƐĞƌǀŝĐĞW&

Figure 4.4 – Basic components of a Queue

I’M WITH ZF. ENGINEER AND EASY RIDER.
www.im-with-zf.com

CH ARLES JENKIN

S

Scan the code and find out more about me and what I do at ZF:

Quality Engineer ZF Friedrichshafen

AG

153
Download free eBooks at bookboon.com

Click on the ad to read more

ƵƐƚŽŵĞƌƐ^ĞƌǀĞĚ

Effective Management Decision Making

Chapter 4

If we summarise our model so far we note that our calling population is an infinite number of customers (large enough so that one more customer can always arrive) but that we are dealing in our processes with a finite number of potential customers. They will have an arrival rate (λ) and we assume their frequency of arrival follows a Poisson probability distribution. We also note that our efficiency in serving those customers means we will have a service rate for our waiting line structure (μ) and that the service rate must exceed the arrival rate, if our waiting line process is ever going to clear. Waiting line structures can vary by organisation and by sector, but there will be queuing characteristics which emerge because of these structures . For example we may have more than one server in a process (think of bank cashiers or supermarket check out operators – these would be described as multiple channels – i.e. channels are the number of parallel servers. You may also come across the word ‘phases’ the describe the number of sequential servers a customer must go through (think about boarding an airflight – there is the check in desk, security, passport control and baggage etc. You can visualize these typical different waiting line structures as:
^ŝŶŐůĞĐŚĂŶŶĞů͕ƐŝŶŐůĞƉŚĂƐĞʹĞŐ ǁĂŝƚŝŶŐĨŽƌƐĞƌǀŝĐĞŝŶĂƐŵĂůůƐŚŽƉ ^ŝŶŐůĞĐŚĂŶŶĞů͕ŵƵůƚŝƉůĞƉŚĂƐĞƐʹĞŐ͘

ŽĂƌĚŝŶŐĂĨůŝŐŚƚĂƚĂƐŵĂůůĂŝƌƉŽƌƚ

DƵůƚŝƉůĞĐŚĂŶŶĞů͕ƐŝŶŐůĞƉŚĂƐĞʹĞŐ

ǁĂŝƚŝŶŐĨŽƌƐĞƌǀŝĐĞĂƚĂůĂƌŐĞƉŽƐƚŽĨĨŝĐĞ

DƵůƚŝƉůĞĐŚĂŶŶĞů͕ŵƵůƚŝƉůĞƉŚĂƐĞƐʹ ĞŐďŽĂƌĚŝŶŐĂĨůŝŐŚƚĂƚĂůĂƌŐĞĂŝƌƉŽƌƚ

We also noted earlier that queuing models can be presented in shorthand as A/B/k to denote different assumptions about their assumed probability distributions. You may therefore see the following symbols used (or select their use in your models) as for A and B: M – Markov distributions (usually Poisson or exponential), D - Deterministic (constant – i.e. customers arrive in predetermined constant order (such as for dental or GP appointments), or G – for a general distribution (with a known mean and variance). In this text we are only concerned with A parameters of M or D. For example, M/M/k refers to a waiting system process where arrivals occur according to a Poisson distribution, service times follow an exponential distribution and there are k servers working at identical service rates. (If you are feeling a bit lost with the mathematics and formulae – there is a summary sheet provided at the end of this chapter for you to print out and refer to).

154
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 4

It has been noted and stressed in this first half of this book, that much of the core of management decision making is about managers trying to simplify complex processes. It should not be expected that models developed will present optimal solutions to queuing problems (or other problems as we see when we consider data forecasting). For queues, we have managed to eliminate much of the complex mathematics to focus upon key queue characteristics but even so don’t forget that the steady state is constant and represent average values for performance characteristics that the system will reach after a long time. As models, they are only based upon historic patterns / trends in observed data in organisations. It is important to realize and appreciate this – and therefore acknowledge that modeling is really only as good as the modeller’s skill!

4.10	

Defining the queuing characteristics

The discussion in this chapter so far has attempted to outline the broad rationales for the development of probability analysis when we move away from discrete quantifiable events, to random events occurring against a background of ‘non events’ (or at least events not of interest to us or the manager). This has been undertaken with a clear focus upon supporting the derivation of the mathematical models that result from this. However, the next stage of this chapter does make a number of assumptions regarding what are termed as ‘queuing characteristics’ when we begin to consider the A/B/k model formats for queues in general. You will find this is a common focus on the majority of undergraduate texts on quantitative decision making – where the derivation of the formulae used in queuing A/B/k models is presented rather than derived. This text follows this same practice – but there are good reasons for this. Firstly, whilst of interest, the derivation of these formulae does not aid deeper understanding of these methods of decision making (that has already been presented through this chapter with a focus upon the fundamental conceptual building blocks), secondly – the derivations are time consuming and lengthy and thirdly, that a managers skill derives from the correct assessment of different impacts of different queuing A/B/k models and subsequent selection of appropriate derived formulae to use to analyse a queue per se. So let’s begin with M/M/1 models (i.e. events arrive for action following a Poisson distribution/ Which are serviced according to exponential service rates/ We only have one server in our system). We have noted in the preceding discussions, the core concepts of μ and λ. We also noted that for a queuing systems to work and actually (at some time t) clear (and hence be no one person or entity waiting for action), that μ > λ (or at the least equal to it). We can now go a little further here and note that a waiting system will be busy a certain proportion of time – and this proportion is given by the ratio of the arrival rate to the service rate (or λ/μ). (Logically you can also equate this statement to the probability that a customer/arriving entity has to wait before it is process also as λ/μ). This ratio is often called the system utilization factor. If for example if λ=8 per hour and μ=10 per hour, then the waiting system will be busy 8/10 – or 80% of the time. Clearly therefore, the system will be empty (with no person or entity waiting for action – 20% of the time on average). We can present this as: P0 (Probability that there will no one person/thing in the system) = 1- (λ/μ) Hence the probability that there are n customers in the waiting structure is given by: Pn (Probability that there will be n customers in the system) = P0 . (λ/μ)n

155
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 4

OR that the probability of there being n customers in the system is the probability of the system being empty at a given point in time AND the likelihood of it not being busy for 1,2,3,n people It can also be proven that the following characteristics apply for M/M/1 queuing models: The average number of units / people /entities in the queue (this EXCLUDES those being served by the system – make sure you note this difference):

/T ǌ ǍǍǌ
Which can also be written as:

/T ǌ Ǎǌ
The average number of units/people/entities in the system (this INCLUDES those being served by the system): L = Lq + (λ/μ) The average time a unit/person/entity spends in the queue (this EXCLUDES those being served by the system): Lq

Wq=

λ/μ

The average time a unit/person/entity spends in the system (this INCLUDES those being served by the system): W=Wq+(1/ μ) The probability that an arriving unit /person/ entity has to wait for service (as noted earlier) is given by: Pw = (λ/μ) Finally – as noted above, the probability that there are n customers in the waiting structure is given by: Pn = P0 . (λ/μ)n

156
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 4

4.11	

Example of M/M/1 system

You’ve bought 4 tickets to attend a circus which comes to town. On the night of the show you arrive early and find yourself at the back of a long queue waiting to get into the big top tent. As a student of business, you quickly analyse the queuing characteristics. You note the following general information: There is only one server at the head of the queue checking and selling the tickets. You are waiting and counting how many new customers arrive and how many are served over 3 ten minute periods. You average those observations to reach a conclusion that the arrival rate seems to be 15 every 10 minutes and the service rate seems to be 18 every 10 minutes. As far as you can observe, there are no factors about the service and operation which lead you to think that the arrival rate is not Poisson distributed and the service rate not exponentially distributed. You feel confident therefore that you are dealing with an M/M/1 system where λ= 15 every 10 minutes (or 90 per hour) and μ = 18 every 10 minutes (or 108 every hour). Now as you arrived in good time to see the show, you joined an existing queue. Your friends seem bored and ask you how long you think they will have to wait in the queue. What do you say? Answer: 	 Lq

As: Wq=

λ/μ

= 5 hours.

If it really matters, make it happen – with a career at Siemens.

siemens.com/careers

157
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making as: λ2 μ.( μ- λ)

Chapter 4

Lq=

= 4.16 people long on average

With a service utilization factor of 83% it is perhaps no surprise to you that you expect to be queuing for 5 hours – but this comes as a shock to your friend! Clearly even when the service rate μ exceeds the arrival rate λ, you can easily still generate significant queues in waiting processes. You observe the serving staff at the head of the queue and think that if there were more servers or if the server function was separated into one person checking and one person selling tickets – the queue would move a lot faster. However, the door into the big top is very narrow and you don’t see how these can be easily accommodated. It does seem possible however, that the queue could be filtered – so that those with prepaid tickets (such as yourself and friends could be processed differently). Feeling bold (and hearing the disgruntled comments from your fellow queue members) you shout up and ask how many people have prepaid tickets. You estimate 80% of the queue have prepaid tickets and hence it seems reasonable to assume that 80% of arrivals to the queue, will also have prepaid tickets. This leads you to propose two arrival rates:λ prepaid = 12 people every 10 minutes (or 72 per hour) and λ to pay = 3 people every 10 minutes (or 18 per hour). You take a guess that the service rates are likely to be significantly different for each type of customer – with the current observed service rate likely to fall a little if the single server did not have to service prepaid customers. The relationship will not be a simple additional one (i.e. it is not as simple as μprepaid = μobserved - μto pay) as the observed service rate will be a function of the service rates for the prepaid and the ‘to pay’ customer. Nonetheless, you estimate that μprepaid = 30 every 10 minutes and μto pay. = 23 every 10 minutes. So if all the prepaid customers were to be identified and allowed to enter via another door after a simple check of tickets, you estimate: Lq prepaid λ/μ

Wq prepaid=

= 0.65 hours (or 39 minutes)

Wq to pay=

Lq to pay λ/μ

= 0.14 hour (or 8.4 minutes)

as: λ2 μ.( μ- λ) λ2 μ.( μ- λ)

Lq - prepaid=

=0.26 people long on average

Lq – to pay=

=0.019 people long on average

(correct respective μ and λ values have been inserted into the above equations). Clearly, from a management perspective, there is therefore scope to think about the cost / benefits of different waiting line structures. Let’s explore this a bit more:

158
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 4

4.12	

Queuing cost analysis of M/M/1 models

A typical problem in business is the cost management of queues. For example, to improve customer services- management (as in our circus example) may want to test two alternatives to reduce customer waiting time (and hence increase revenues and stop potential customers becoming bored in the queue and leaving): 1.	 Another employee to check prepaid tickets whilst the first continues to take for new tickets – both working at the same counter. 2.	 Another employee checking prepaid and to pay tickets at a separate counter. Take the first alternative solution, where the circus management adds an extra employee – on the same counter - to increase service rate from 18 customers every 10 minutes (or 108 customers per hour) to 25 customers every 10 minutes (or 150 customers per hour). With only one queue being served, the arrival rate of customers remains fixed at 15 every 10 minutes (or 90 per hour). The extra employee on the door is estimated to cost an additional £15 / hour (or £150 / week for a ten hour week of operating the ticket counters). It could be estimated that each 10 minute reduction in customer waiting time avoids losing (on average) £5 in lost sales (and bored customers). We know from the earlier analyses that customer waiting time with one employee on one counter is 5 hours on average. With an additional employee the service rate increases, lowering the waiting time in the queue to: Lq

As: Wq=

λ/μ

= 0.63 hours (or 38 minutes)

as: λ2 μ.( μ- λ)

Lq=

=0.38 people long on average

Hence the reduction in waiting time is 4.37 hours, representing (26x5 = £131) saving due to customers now being prepared to wait to be served. However, with a wage rate of £150 over the duration of a working week, this would represent a loss of £19 for the circus – despite the greater efficiency of the queuing operation. Can you prove that the breakeven service rate is modelled by the equation 0.9μ3 – 81μ2 = 8100? As this scenario does not look attractive to the circus managers, the second option is considered. Here a new second counter is opened and the doors of the big top tent widened., It is estimated that the new counter, door alterations and equipment will cost £6000 plus £100 per week for cashier till and salary thereafter. Customers will form and join one queue, then choose whichever counter is open to them when they reach the head of the queue. As a result, the arrival rate for each counter is reduced from λ= 90 per hour, to λ= 45 per hour. The service rate for each service operator is as per the original formulation of μ=108 per hour. Can we solve this now? Well- not yet – first we need to learn a bit more about multiple service operations.

159
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 4

4.13	

M/M/k queues

We can now consider M/M/k waiting line structures where k>1 (i.e. multiple service operations). For these structures, variations on the preceding queuing characteristic equations can be derived. They assume that multiple servers exist in a structure, each with (usually) the same efficiency (i.e. service rate μ) and that arrivals join a single queue before taking up the next available serving station as they arrive at the head of the queue. (Note – this model for example does not apply when the customer /unit/ entity must decide which server to use when multiple servers are open to corresponding multiple queues to be formed).We again assume a Poisson arrival rate to the single queue and an exponential service probability rate. Key queuing characteristics are therefore: P0 (Probability that there are no people /units/ entities in the waiting system) =


N

ƴǌǍQǌǍNNǍ
Q QNNǍǌ

This is a complex calculation and usually, data tables are presented for P0 based upon selected values of the utilization ratio (λ/μ) and k. Andersen et al (2009) for example present this data table on page 536 of that text.

www.sylvania.com

We do not reinvent the wheel we reinvent light.
Fascinating lighting offers an infinite spectrum of possibilities: Innovative technologies and new markets provide both opportunities and challenges. An environment in which your expertise is in high demand. Enjoy the supportive working atmosphere within our global group and benefit from international career paths. Implement sustainable ideas in close cooperation with other specialists and contribute to influencing our future. Come and join us in reinventing light every day.

Light is OSRAM

160
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making Other relevant queuing characteristics can be determined by: The average number of units/people/entities in a queue (which EXCLUDES those being served / actioned):

Chapter 4

/T ǌǍNǌǍ3 NNǍǌ
The average number of units / people / entities in the system is found from: L= Lq +(λ/μ) The average time a unit spends in a queue: Lq

Wq=

λ/μ

The average time a unit/person/entity spends in the system (i.e. in the queue AND being served): W=Wq+(1/ μ) The probability that an arriving unit/person/entity has to wait for service/action:

3Z ǌNNǍ3 NǍNǍǌ
The probability of their being n units/people/entities in the system:

3Q ǌǍQ3 IRUQ N Q 3Q ǌǍQ3 IRUQ! N NNQN
As μ = mean service rate for each channel of service, then k μ = mean service rate for a multiple channel waiting system. Hence these formulae (as before) are only valid when k μ> λ. We can now use this information to return back to our circus question and develop the answers for the multiple service operation. The customer waiting time when we have two servers is now found from Lq , P0 and Wq. Hence:

161
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 4

/T ǌǍNǌǍ3 NNǍǌ
Where P0 can be found from the tabular data presented (say in Andersen et al ,2009) for the values of λ= 45 per hour, μ=108 per hour and k=2 – this gives P0 as approx. 0.667. Substituting the remaining values gives Lq = 0.0192 people on average. Hence Wq is found to be 0.000426 hours (or 0.025 seconds – quick service to get to the head of the queue!). Hence this system almost eliminates the need for queuing. Hence in comparison with the original queuing problem for the circus and a waiting time of nearly 5 hours, this set up eliminates the waiting time – i.e. saving 5 hours. With a financial benefit of £5 every 10 minutes saved in queuing time this equates to £150 saving per time period (per week saved). Hence the payoff period for this server operation would be 120 weeks (or just over 2 years). (If you cannot see where this came from consider that there is a one off investment cost to payback, but also an extra cashier till to rent and salary to pay. The extra revenue generated (through queue management) is £150, £100 of which covers ongoing costs and £50 is payment for the counter and door modifications. Hence £6000/£50 =£120 weeks, after which the additional £50 becomes an extra revenue).

4.14	

The economic analysis of queues

We can formalize some of the relationships revealed in the above discussion. This is sometimes labeled as the economic analysis of queues. This is an important focus in decision making for queues and waiting line structures and as presented in our earlier circus discussion, allows the manager to make comparative decisions regarding the cost-benefit of different waiting line structures. However – you should not just consider the cost implications of different waiting line structures – there may be other in kind / goodwill benefits that may warrant support for less efficient (financially) types of structure. You can for example make the queuing time more engaging with entertainment whilst queuing, or presenting a virtual presence in a queue until you near the front of it (such as happens in large theme parks around the world), or offer other services such as drinks and food (complimentary?), improve the environment of the queue to help pass time or use the queuing time to complete any relevant documentation (again typical of customs declarations or landing card information at airports (Waters, 1998)). In other words, the management science of queuing should not just be thought of, or practiced as a numerical analysis – but one that also considers other aspects of effective management of resources. Returning to the cost considerations at present, we need to be able to present an analysis that articulates the total cost of a given waiting line structure. This is called a total cost model (described by the notation TC). Hence: TC = CwL + CsK Where Cw is the waiting cost per unit/person/entity per time period, L is as before the average length of the queue (or the number of units/persons/entities in the queue), Cs is the service cost per time period for each channel of service that ‘serves’ the queue and K is also as before, the number of channels in the waiting line system.

162
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 4

It is not easy obtaining or estimating some of these costs – as for example, the cost Cw is not a direct cost to an organisation, but represents a opportunity cost loss as customers may decide to take their business elsewhere if the waiting time is deemed unacceptable. Service costs are usually easier to identify and determine (as with our circus example) as they refer to a business function with costed inputs. The basic relationship between service cost and waiting cost is familiar to students of microeconomics as ‘cost curves’.


Figure 4.5 Comparative cost curves for Queue waiting structures

A simple example might be the following – consider first of all, a specialist retail store selling model aeroplanes. This store operates one cashier till with one server on that till. That server has a variable cost associated with their salary, benefits and other direct costs associated with overheads and cash till rental. Let’s assume this works out to be £10 / hour. As noted about, the cost associated with customer waiting is harder to estimate but it could be expected that in this context, for customers specifically locating and entering the shop (with little competition in the immediate vicinity as the shop is specialized) it could be argued that the waiting cost would be lower than say a café on the seafront. We could therefore assign a waiting cost of £5 hour. As: TC = 5L + 10(1) Where L could be determined as before, from the parameters of the service (waiting and service times).

163
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 4

4.15	

Other queuing systems and different waiting line structures

So far we have reviewed M/M/1 and M/M/k waiting line structures. What others are there, that an organisation could exploit? One of the more common types are M/G/k models and M/D/k models. In both cases we assume we have a Poisson arrival rate for customers, but in the first case we have an unknown or unspecified service rate with one or more servers in operation whilst in the latter case we have a constant service rate (called D – for deterministic). For this latter case, perhaps the easiest way to see this in operation is to visualize a multiple bay garage. There are therefore several servers (let’s assume 3 bays, hence k=3) but whilst work arrives randomly (aside from prescheduled bookings), the work undertaken follows a constant fixed price (i.e. an hourly rate). There are also situations and waiting line structures we have an unknown service rate (i.e. one which does not follow an exponential service rate – usually when the work undertaken by the server is not routine but bespoke to that customer). In such cases, the queuing characteristics can be determined to be the following equations which as you see – also include the standard deviation of the service time (σ). Probability that there are no units/persons/entities in the system: P0 = 1-( λ/μ) The average number of units/persons/entities in the queue: Lq = (λ2σ2+( λ/μ)2)/ (2(1- λ/μ))

At Navigant, there is no limit to the impact you can have. As you envision your future and all the wonderful rewards your exceptional talents will bring, we offer this simple guiding principle: It’s not what we do. It’s how we do it.

Impact matters.
navigant.com

©2013 Navigant Consulting, Inc. All rights reserved. Navigant Consulting is not a certified public accounting firm and does not provide audit, attest, or public accounting services. See navigant.com/licensing for a complete listing of private investigator licenses.

164
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making The average number of units/people/entities in the system: L = Lq + (λ/μ) The average time a unit/person/entity spends in the queue: Wq=Lq/ λ The average time a unit/person/entity spends in the system: W=Wq+1/μ The probability that a unit/person/entity has to wait for service: Pw= λ/μ

Chapter 4

4.16	

Example of an arbitrary service time queue

You have returned to University to continue your studies and as usual, one of the first stages involved is re-registration. Now – as a student of management – you want to understand the University’s operational procedures and make some observations whilst waiting to be served so you can re-register. You observe: All re-registrations are handled by one member of administrative staff. You have counted – over 3 thirty minute periods, 5,8,8 students arrive – giving a mean arrival rate of 7 per 30 minute period – or 14 per hour (or λ=14/60 or 0.233 per minute). The standard deviation of this service rate is (to remind you): σ= √ (((x1-mean x)2+(x2-mean x)2+(x3-mean x)2) / (n-1)) = √3 = 1.732 (when n=3) You also time how fast a student is served by the staff member and over 5 observations (of 2,4,3,3,2 minutes) derive the mean service time as 2.8 minutes. Hence the mean service rate is the inverse of this (i.e. 1/ 2.8 minutes) of 0.357 students per minute. You therefore determine the queuing characteristics as: P0 = 1-( λ/μ) = (1- (0.233/0.357)) = 0.347 (or 35% chance the queue is empty and a 65% the queue is not empty). Lq = (λ2σ2+( λ/μ)2)/ (2(1- λ/μ)) = (3.054+0.425) / 0.694 = 5 students L = Lq + (λ/μ) = 5+0.652 = 5.652 students Wq=Lq/ λ = 5/0.233 = 21.46 minutes

165
Download free eBooks at bookboon.com

Effective Management Decision Making W=Wq+1/μ = 21.46+2.80 = 24.26 minutes Pw= λ/μ = 65% (see above).

Chapter 4

So let’s now taken an example of a single bayed garage as mentioned before – but in the case of a very specialized operation such as those garages which ONLY supply tyres or exhausts. In such cases, the standard deviation of the service time is approximating service (the tasks are the same and should take the same time to complete). This is called the M/D/ 1 model (where D refers to deterministic service rates). Hence σ=0 and Lq as noted above, is reduced to: Lq = (λ/μ)2/ 2(1- λ/μ). Let’s take this example one stage further and consider a multiple bayed garage where customers have tasks (work) evaluated and determined before arrival – hence all work is predetermined and the hours to be expended on that work, known. Any new arrivals seeking an open channel are denied access if a customer is expected or the bay is in use (as subsequent times for that bay are also likely to be booked out). This is called an M/G/k model as the model assumes customers will arrive randomly (using a Poisson distribution) but the service rate is unspecified (i.e. general – hence G). Customers will only gain access to a server if one channel (or bay) is free – otherwise they are refused entry – and finally that there may be k servers (or garage bays) available for use. Other assumptions of this model are that the service rate is the same for each channel (which it should be for identified tasks on owner’s cars). The problem for the manager to decide upon, is exactly how many servers (or garage bays) should they open? Rationally – the manager would like the bays created to achieve high levels of service utilization (i.e. for λ/μ to approach 1 in the limit). Having too many bays means some will be underused and the ratio of λ/μ will tend towards a low value or zero, whilst having too few will result in lost business as customers cannot be serviced and λ/μ will equal 1. Hence by determining the probabilities that j of the manager’s ‘k’ channels will be busy – will answer this uncertainty. This can be found from the following equation – where Pj is the probability that j of the available k channels are busy: Pj = ((λ/μ)j/j!)
k

Pj =

Σ(λ/μ)i / i!

i=0

As before we can also note the following characteristic queuing equations for this type of M/G/k as: Probability that there are no units/persons/entities in the system: P0 = 1-( λ/μ) The average number of units/persons/entities in the queue: Lq = (λ2)/ (2μ(μ- λ))

166
Download free eBooks at bookboon.com

Effective Management Decision Making The average number of units/people/entities in the system: L = Lq + (λ/μ) The average time a unit/person/entity spends in the queue: Wq=Lq/ λ The average time a unit/person/entity spends in the system: W=Wq+1/μ The probability that a unit/person/entity has to wait for service: Pw= λ/μ

Chapter 4

4.17	Summary
This chapter has introduced some basic concepts in probability distributions and how they can be used to guide managerial decisions regarding the efficient use of resources (and the implicit tradeoff between process service characteristics and resourcing). Davern (2004) highlighted, the now common practice, of preprinting air plane boarding cards prior to arrival at the airport (and of course this can now also be completed via mobile phone technologies). Furthermore, Brady (2002) gives the interesting discussion of the need to consider queuing theory beyond the immediate cost efficiency argument in a study of the queues that formed outside cinemas about to show the Star Wars 1 movie: The Phantom Menace. He argues that queues also capture an ethical tension between self interest and civility when the customer is experiencing stress and tension. The FCFS model was found to still be a valid guiding queuing discipline for organizers and participants, but with a need for greater awareness of using the queuing time for opportunities for self enrichment. McCorvey (2011) also stresses the issue of the trade-off between service excellence and costs for service intensive industries citing the seemingly counter intuitive research observation that it can be better to reduce the number of customers that a business seeks to serve to ensure those it targets are serviced efficiently and, as appropriate, incur an increased fee for that service received. In this sense, McCorvey (2011) further argues we might see more businesses exploit the ‘concierge medicine’ model, as customers are prepare to wait less and less, to receive their service, but pay more for that type of queuing discipline. In the next chapter, we continue to move away from quantitative ‘precision’ that has been the core of the first chapters of this text, to embrace more of the uncertainty often found in human decision making including bias, prejudice and as appropriate, ‘guesses’. In essence therefore, we begin to consider more closely, the second half of the management decision making problem – as described in chapter 1 – by focusing upon people more. Implicitly therefore, this text seeks to develop in the reader, a shared understanding of the value of contingent thinking in decision making. This is not an easy task, nor will the reader find the aim simple to achieve.

4.18	

Key terms and glossary

Discrete variables – are data variables that are explicit and measured values. They count some progress or measurement. Continuous variables – are data points measured between interval points.

167
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 4

Binomial function – describes a particular frequency distribution where we are concerned with a small number of successes against a small range of possible trials. Poisson function - describes a particular frequency distribution where we are concerned with a small number of successes against an infinite range of trials. Queuing theory – describes the application of the Poisson function to problems concerned with small numbers of events of interest against a large range of events not occurring (such as customer’s arriving in a shop to be served). ‘e’ - is a particular mathematical value which describes when the rate of change in a variable is the same as the variable itself.

End of chapter questions
Question 1) A small local grocery store is to be launched with only one checkout counter. You can assume that shoppers will arrive at the checkout according to the Poisson probability distribution, with a mean arrival rate of 15 customers per hour. The checkout service times follow an exponential probability distribution, with a mean service rate of 20 customers per hour. a)	 Determine the operating characteristics for this waiting line (40%) b)	 If the manager’s service goal is to limit the waiting time prior to beginning the checkout process to no more than 5 minutes, what recommendations would you provide regarding the proposed checkout system? (10%)

Do you have to be a banker to work in investment banking?
Agile minds value ideas as well as experience Global Graduate Programs
Ours is a complex, fast-moving, global business. There’s no time for traditional thinking, and no space for complacency. Instead, we believe that success comes from many perspectives — and that an inclusive workforce goes hand in hand with delivering innovative solutions for our clients. It’s why we employ 135 different nationalities. It’s why we’ve taken proactive steps to increase female representation at the highest levels. And it’s just one of the reasons why you’ll find the working culture here so refreshing. Discover something different at db.com/careers

Deutsche Bank db.com/careers

168
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Chapter 4

After reviewing your calculations, the manager of the proposed venture wants to consider one of the following alternatives for improving service. What alternative would you recommend? Justify your recommendations (50%) a)	 Hire a second person to bag the groceries while the cash register operator is entering the cost data and collecting money from the customer. With this improved single channel operation, the mean service rate would be increased to 30 customers per hour. b)	 Hire a second person to operate a second checkout counter. The two channel operation would have a mean service rate of 20 customers per hour for each channel. (HINT –Refer to the attached table for values of P0 when there are multiple channel waiting lines with Poisson arrivals and exponential service times.)
Table of P0 values:

Ratio (λ / μ) 2 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 0.8605 0.8182 0.7778 0.7391 0.7021 0.6667 0.6327 0.6000 0.5686 0.5385 0.5094 0.4815 0.4545 0.4286 0.4035 0.3793 0.3559 0.3333

Number of Channels (k) 3 0.8607 0.8187 0.7788 0.7407 0.7046 0.6701 0.6373 0.6061 0.5763 0.5479 0.5209 0.4952 0.4706 0.4472 0.4248 0.4035 0.3831 0.3636 4 0.8607 0.8187 0.7788 0.7408 0.7047 0.6703 0.6376 0.6065 0.5769 0.5487 0.5219 0.4965 0.4722 0.4491 0.4271 0.4062 0.3863 0.3673

169
Download free eBooks at bookboon.com

Effective Management Decision Making Answer:

Chapter 4

Answer: This question is based on queuing theory. Once that has been grasped, the question is a comparative discussion of the differences between three proposed service options for the proposed business. P0=1-λ/ μ= 1-15/20=0.25 Lq= λ 2/( μ(μ- λ))= 15(15)/(20(20-15))=2.25 L=L+ λ / μ = 3 Wq=Lq/ λ = 0.15 hours (or 9 minutes) W=Wq+1/ μ = 0.20 hours (12 minutes) Pw= λ /μ= 15/20 = 0.75 At 9 minutes the checkout service needs improvements. For the second part of the question, we can assume for example some threshold of target improvement (suggested at 5 minutes). For a) λ =15, μ=30 per hour Hence Lq= λ 2/( μ(μ- λ))= 15(15)/(30(30-15))=0.5 Wq=Lq/ λ = 0.0333 hours (or 2 minutes) For b) λ =15, μ=20 per hour From given tables P0=0.4545 Lq=((( λ / μ)2 λ μ)/1!(2(20)-15)2) P0 =0.1227 Wq=Lq/ λ = 0.00822 hours (or 0.492 minutes) Recommend one checkout counter with two people. This meets a 5 minute waiting time threshold (at 2 minutes) – but does not incur the additional cost of installing a new counter. (You can obtain a table P0 values from (say) Andersen et al (2009:623)).

170
Download free eBooks at bookboon.com

Effective Management Decision Making Question 2:

Chapter 4

A couple of entrepreneurial students are considering leaving university and buying into a national franchise label, for fast food production and delivery. Of the many issues to consider, you have been approached to advise them on three urgent concerns. The first of these concerns is focused upon operational advice for the proposed venture. Part of the franchise and likely location for the fast food business, will support a drive up window food service operation. The two entrepreneurs are considering what would be the best way to manage this operation and have proposed three alternative scenarios, that they have asked you to evaluate: 1)	 A single channel operation where one employee fills the order and takes the money from the customer. It is anticipated that this will take 2 minutes to complete. 2)	 A single channel operation where one employee takes the order and another employee takes the money from the customer. It is anticipated that this will then take 1.25 minutes. 3)	 A two channel operation in which there are two service windows and two employees. The employee stationed at each window fills the order and takes the money for customers arriving at the window. The anticipated time this will take for each employee is 2 minutes. Advise them which is the better option (1, 2 or 3) and answer the following questions: a)	 What is the probability that no customers are in the system? (i.e. queuing to be served) (10%) b)	 What is the average number of cars waiting for service? (10%) c)	 What is the average time a car has to wait for service? (10%) d)	 What is the average time in the system? (10%) e)	 What is the average number of cars in the system? (10%) f)	 What is the probability an arriving car will have to wait for service? (10%) You can assume that customer arrival (in their cars) follows a Poisson probability distribution, with a mean arrival rate of 24 cars per hour and that service times follow an exponential probability distribution. Arriving customers are expected to place their orders at an intercom station at one part of the parking area before driving forward to the service window to receive and pay for their orders. After you have completed this analysis for the entrepreneurial couple, you are then given further data. This is additional time and motion research, which has now been gathered by the entrepreneurs based upon observed trends with competitors. The results are found to be: •	 Customer waiting time is forecasted to have a financial cost of £25 per hour (full cost) •	 The full cost of each employee is determined to be £6.50 per hour •	 Additional costs (to account for equipment and space) would also add an additional cost of £20 per hour attributable to each service channel (option)

171
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 4

Of the three initially proposed service operations, which is the lowest cost design for the proposed fast food business? (40%) Answer: This question is based on queuing theory. Once that has been grasped, the question is a comparative discussion of the differences between three proposed service options for the proposed business. λ=24
System A Characteristic of the system (k=1, µ=30) a)P0 b) Lq c) Wq d) W e) L f)Pw 0.2 3.2 0.133 0.1667 4 0.8 (k=1, µ=48) 0.5 0.5 0.02 0.0417 1 0.5 (k=2, µ=30) 0.4286 0.1524 0.0063 0.0397 0.9524 0.2286 System B System C

Real drive. Unreal destination.

As an intern, you’re eager to put what you’ve learned to the test. At Ernst & Young, you’ll have the perfect testing ground. There are plenty of real work challenges. Along with real-time feedback from mentors and leaders. You’ll also get to test what you learn. Even better, you’ll get experience to learn where your career may lead. Visit ey.com/internships. See More | Opportunities

© 2012 Ernst & Young LLP. All Rights Reserved.

172
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making System C provides the best service characteristics Second part of the question is determined as: System A= 6.5+20=£26.50/hour System B=2(6.5)+20=£33.00/hour System C=6.5+20=£26.50/hour Total cost = CwL+Csk System A= 25(4)+26.50(1)=£126.50 System B = 25(1)+33(1)=£58.00 System C= 25(0.9524)+26.50(2)=76.81 System B is the therefore the most economical.

Chapter 4

End of chapter summary formulae
Note – in using this summary – you may also wish to refresh yourself with basic statistical descriptive mathematical formulae too. Queuing Equations: Binomial Distribution (for a finite sample set) P (R successes in N trials) = NCR * pR * qN-R Where p = probability of the event of interest occurring Where q = probability of the event not occurring Where C refers to the combinations of possibilities of the order of events occurring of interest and is solved by the formula: N! R!(N-R)!

N

CR=

Queuing Equations: Poisson Distribution (for an infinite sample set) e- λ * λ R R!

P (R) =

Where λ = mean arrival rate of an event Where m = mean service rate for an event Where R = no. of events of interest

173
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 4

Probability of delivering a service for an event (e.g. customer arriving) in less than a given time t (for a Poisson distribution arrival likelihood): P(service time<=t)= 1-e- µ*t OR: P(service time<=t)= 1-e- t/λ Probability of not delivering a service for an event (e.g. customer arriving) in less than a given time t (for a Poisson distribution arrival likelihood): P(service time>=t)= 1- P(service time<=t)= e- µ*t OR: P(service time>=t)= 1- P(service time<=t)= e- t/λ P0 = Probability of no (zero) customers in the system ρ = utilization factor of a system (the proportion of time it is in use) Single Server Model Characteristics (M/M/1)





174
Download free eBooks at bookboon.com

Effective Management Decision Making Constant service times - single server models (M/D/1)

Chapter 4


Multiple channels – single phase model (see also table attached for P0): (M/M/k)



175
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 5

Chapter 5
5.1	 Developing holistic models with qualitative methods of decision analysis: Irrationality in Management Decision Making
The preceding chapters have stressed the role and function of quantitative data in developing models of management decision making. From chapter 1 and RAT models, this was defined as being the rational approach to decision making. Through chapters 2, 3 and 4 however, the reliance upon measurable data to define the decision context has weakened as the manager relies on more statistical relationships of data to anticipate future organisational needs; hence our focus in these subsequent chapters is to consider the ‘irrational’ models of decision making, where we are specifically concerned with the management of human decision values. To address this concern we can review the different approaches to incorporating the human input into the decision making process, at both an individual (Chapter 6) and collective level of analysis (Chapter 7). To achieve this, this chapter and subsequent chapters are focused upon decision making from an individual bias, collective bias, prejudice in decision making and interpreting data and in discussing systems approaches to modeling the decision making process which allow these human values to be considered. We, in particular, focus on mode 1 and mode 2 of Peter Checkland’s Soft Systems Methodologies in this chapter, as a vehicle to bring together both the human values in decision making and the rational use of measured data.

The stuff you'll need to make a good living

STUDY. PLAY.

The stuff that makes life worth living

NORWAY. YOUR IDEAL STUDY DESTINATION.
WWW.STUDYINNORWAY.NO FACEBOOK.COM/STUDYINNORWAY

176
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Chapter 5

Throughout this chapter and the presented context, the reader is encouraged to recognise the value of a contingent approach to management decision making, and as such, we will initially consider the individual human input to decisions. We are aware from the first chapter that humans are subject to ‘bounded rationality’ (Simon, 1951), where we are limited in our process abilities and capacity to identify and evaluate all relevant information necessary to achieve an optimal decision outcome. Instead our decision outcomes are satisficing. Similar early studies on decision making in public organisations identified that in traditional (tall, stable, large) organisations – the practice of incrementalism was observed in decision making and decisions reached by ‘muddling through’ (see for example Lindblom, 1959). This approach to decision making stresses the satisficing view then and that preferred outcomes reflect previous similar historic patterns of decision making and where possible decisions taken are minor variations of past outcomes. Cohen, March and Olson’s (1972) ‘Garbage Can’ model of decision making, is a famous interpretation of organisational decision making where satisficing decisions are the expected outcome for organisations. This arises where the necessary decision body and context inputs of staff, resources, timing and context are rarely convergent to support an optimised decision outcome. This is perhaps not a surprise, given that public institutions and organisations tend to be risk averse and where there is a focus upon time for gaining knowledge and commitment (organisational learning), which relies on existing organisational knowledge of the problem and ‘muddling through’ using knowledge of similar problems, so that decisions are simpler combined insights of those solutions. This methodology was improved by assigning a goal to the ‘muddling through’ which is often described as logical incrementalism decision making. This approach to decision making has also been core to political decisions, where it is has been described as neo-functionalism and emerged as a dominant area of study of political negotiation and in particular political integration in the EEC in the 1970s. It is helpful to reconsider the concept of a three phase model of decision making introduced in Chapter 1, to view the qualitative contribution of the human input into decision making (Jennings and Wattam, 1998). This three phased model reflects a more realistic view of how individuals make decisions which are satisficing, rather than optimizing, by presenting this process as an interdependent 3 stage activity of: 1) problem identification – 2) solution development – 3) solution selection; however, the latter two stages are presented as being concurrent in this process. Adopting the three phase model is also helpful as it provides a structure to analyse and understand decision making without proposing a normative framework typical of RAT models. The three phase model therefore allows consideration of the strategic context, goals, ethics and risk in decision making in particular – which will shape the decision context. Clearly, developing a realistic interpretation of decision making in organisations will also require that such models consider organisational hierarchy (and changes to that hierarchy, such as ‘delayering’) and time – where for example, smaller organisations often report limited strategic engagement with decision making (with ‘firefighting’ decision making being their norm). It also means recognising that there is a large cognitive aspect of strategic decision making for organisations – with a need to consider the subjective construction of individual reality (see for example the discussion of the Cognitive School of Thought of Strategy in Mintzberg, 1998). Use of words and language between individuals is very important in effective decision making– how for example, is our experience shaping our perceptions or risk? Is the entrepreneur less risk averse because they perceive a situation differently? (i.e. do they perceive it as an opportunity or as a problem or is it because they have techniques for managing it?) As Frank Knight (1921) would agree, risk and uncertainty in modelling decision making then become more important concepts to address.

177
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 5

This issue can be illustrated by the difficulties that emerged in the development of a famous financial instrument in the 1990s. The so called Black-Scholes formula was used to determine the price of futures options in the stock market. This formula aimed to reduce the risk (due to human irrationality) of how markets value shares and stocks. The historic solution has been to buy an option to buy a stock (which was first proposed by Louis Bachelier at start of the 19th century). The option therefore is a transaction - i.e. the option to buy (which carries a fee) a stock if prices rise or not to buy if prices fall – hence the problem for market traders was to determine how to price the option to allow the buyer to complete the contract and purchase the stock if prices rise (or not) - becomes a primary concern. This price however, is dynamic and shaped by a large number of other market variables. It could be represented by a complex mathematical formula model which was developed in the 1970s by Myron Scholes, Fisher Black and Bob Merton. This model, through the use of modern computing, allowed real time calculations of the value of an option to be made and hence the risk of buying a stock, to be determined and hence the return for that investment, calculated. Scholes, Black and Merton subsequently raised $3 billion of funds with investors from Wall Street to found the company - Long Term Capital Management (LTCM), with the promise of using this dynamic hedging (or a continuous rebalancing of market option prices) on a huge scale to, in effect, reduce the risk of buying stocks in organisations. In that aim, they were initially enormously successful with a ROI of 20%, 43% and 41% in their first three years of operation (BBC, 1999). However at the tail end of the 20th century things started to go wrong. The trouble started in Asia - markets were collapsing and deviating significantly from their historical norms and therefore the market deviations began to increase in magnitude and violate the assumptions underpinning their mathematical model. LTCM carried on as normal, convinced that market behavior would restabilize, as their mathematical model said it would do. However, Russia then defaulted on IMF loan repayment, which it should not have done according to the model and LTCM faced huge losses. In order to prevent the global economic collapse that would have resulted from the failure of LTCM, the US Federal Reserve bailed out LTCM - to the value of $3 billion. Prior to the Enron and 2008 global financial collapse, this was the largest bail out in American corporate history. The key observation from that experience is that clearly there are limits to the relevance and applicability of developing mathematical models bereft of consideration of the ‘human condition’ and this is the primary focus for this and subsequent chapters. In developing modeling which encompasses both qualitative (soft) as well as quantitative (hard) data, we are rejecting the previously maintained credence decomposition approach of modeling, presented chapters 2, 3 and 4 as being able to fully articulate all the complexities of reality. In other words, the situations in which managers find themselves are unlikely to be reducible to a series of appropriately developed mathematical relationships. Decision making methods have been developed to aid the manager in the cognitive inclusion of both hard and soft data and one such method is known as Monte Carlo simulation. This is discussed next.

178
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 5

5.2	

The Monte Carlo Simulation

The Monte Carlo simulation, is an approach to decision making which does not rely upon measurable input data (unlike for example those discussions in chapter 2), subjective estimates of likelihoods of given events occurring (such as those presented in chapter 3) nor on assumed patterns in success or failures in the environment of the organisation (such as those presented in the poisson distribution and queuing theory of chapter 4). Instead, the method uses randomness to assign weightings to potential outcomes and in so doing, allows the manager to make recommended decisions without recourse to the context of those decisions. For example, assume the following investment expectations and likelihoods:

Table 5.1

If we assign random numbers to each ‘cash in ‘possibility in the same ratio as the forecast probability and then repeat for ‘cash out’, we would generate the following:

I joined MITAS because I wanted real responsibili� I joined MITAS because I wanted real responsibili�

Maersk.com/Mitas www.discovermitas.com

�e Graduate Programme for Engineers and Geoscientists

� for Engin

M

Real work International Internationa al opportunities �ree wo work or placements

Month 16 I was a construction M supervisor ina cons I was the North Sea supe advising and the N he helping foremen advis s solve problems Real work he helping International Internationa al opportunities �ree wo work or placements s solve p
Click on the ad to read more

179
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 5

Table 5.2 If we then generate random numbers and assign them to each input and output, e.g. Cash In = 46, Cash Out = 81 and repeat this with many simulations – the more likely combinations of cash flow will occur more often than the less likely combinations. With sufficient simulations (usually <1000) –the probabilities of more likely combinations can be determined. So that:

Table 5.3

Then for this short run simulation, the net cash flow of -20000 – has in this run a 4/10 (0.25) or 25% chance of occurring. If this is solved with a Decision Tree – this EMV is actually 0.165 but with a sufficient number of trials, this solution would be achieved. So that:

Table 5.4

180
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 5

In this simple example – where the decision tree can be easily drawn out, we would not use simulation – but in more complex tasks – with many uncertain probabilities and hence uncertain EMVs – a monte carlo simulation provides a useful way to identify and manage risks. Let’s consider another example to illustrate that decisions can be informed by limited information. Consider the case of uncertainty for a firm seeking to launch a new product – uncertainty can come both from market demand and 3rd party demand. Production may also carry uncertainty – such as where to make the product (large factory? small factory? – with a consideration of the differences in efficiencies and so forth). It might be estimated that the range of possible costs associated with this activity could be represented as:

Table 5.5

Applying Monte Carlo simulation to these range of outcomes, generates a risk profile to reflect the scope and likelihood of potential return and loss depending upon the variables in the table – i.e.:

&ĂĐƚŽƌǇy

&ĂĐƚŽƌǇz

Figure 5.1

181
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 5

From figure 5.1, factory Y exhibits stochastic dominance over factory X (recall the earlier discussion on stochastic models from Chapter 1 Box 1.1). Expected annual profit from Factory Y>expected annual profit from Factory X however, factory Y is not risk free, as there is a finite chance (approx 10%) of an annual loss occurring too. In summary, Kwak & Ingall (2007:45) in defining Risk Management define Monte Carlo simulation as any technique of statistical sampling employed to approximate solutions to quantitative problems which relies upon assigning random values to model variables represented by a distribution function (which were discussed in chapter 4) and then simulating reality through repeated operations of the model. The simulation then generates outcomes against which decisions can be made and risk managed. It has also found applications in the science disciplines too.

5.3	

Systems thinking about decisions

If we now wanted to develop understanding of the decision context more, we note that the three phased model outlined earlier and in Chapter 1, stressed the need to understand the problem domain of a decision. This means identifying the critical features from which a model could be developed which would be able to represent reality (given the balance between cost and efficiency in the development of those models). Effective modeling appeals to rationality, cost and reliability issues, but are constrained by waste and politics. Moreover, they should reflect the purpose of the activity and stress a particular viewpoint and aim – such as the need to meet demand or increase efficiency, stress systematicity, structure and communication. To achieve these goals as modeller, the problem domain and problem identification also needs to focus upon entities, actors, processes, perceptions and ensure there is monitoring and feedback.

Need help with your dissertation?
Get in-depth feedback & advice from experts in your topic area. Find out what you can do to improve the quality of your dissertation!

Get Help Now

Go to www.helpmyassignment.co.uk for more info

182
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Chapter 5

The major flaw of rational decision making in RAT models, is that we (the modeller) lose the ‘richness’ of the problem being modeled. The RAT methodology is one focused upon problem reductionism through causal relationships rather than how what is being studied interacts with every other part of the system (Skarzausklene, 2010). Hence, decision making involving human values is not easily resolved as they are non reductionist problems in nature. Reductionist methods are derived from the pure sciences (and were explored in chapters 2, 3 and 4) where problems are dissected into smaller sub problems, which are then resolved, so that the larger problem can then be addressed by converging the sub element solutions in a linear fashion. However, when sub problems involve human agents, the relationship between those elements also becomes relevant (as they are now interdependent). These problems are then defined as complex, and to resolve them we can adopt a systems view of the problem to be resolved. This approach is called General Systems Theory (GST) which amongst other aims, seeks to identify the agreed ‘facts’ of the situation, the individual values in the problem domain, issue interdependency in the problem or decision to be taken. The outcome is a better understanding of the complexity of the problem domain. Walonick (1993) describes that GST originated with Ludwig Von Bertalanffy in 1928 who proposed the view that a system is characterized by the interactions of its components and the nonlinearity of those interactions. Walonick (1993) continues that resolving problems can be addressed in three general ways – firstly through a reductionist approach (as discussed earlier and the focus of chapters 2, 3 and 4) which examines the sub systems within the system, a holist approach, which considers the systems as a whole functioning unit (and which are considered in this chapter) and a functionalist approach that looks upward from the system to its roles and activities in a larger system. Holism or systems views are primarily concerned with identifying the orderliness of a situation and the context of the decision environment as a whole (Skarzausklene, 2010). (refer back to Chapter 1 for a refresher on this). We deliberately are not concerned with attempting to develop prescriptive and normative models now. We are instead focused upon identifying solutions to problems and decisions, which encourage concensual understanding amongst those affected human participants. Peter Checkland, originally an engineer by training, considered in the 1960s the reasons why many projects overran and had difficulty in implementation. From a background of using hard data he realized that human values and beliefs were shaping the intended outcome of project solutions. This included human bias and prejudice about for example, what might be the cause of a problem for an organisation. From these observations, it was realized that it was important to resist any sub system focus in resolving decisions which involve human inputs and to specifically focus on defining the nature of the problem domain. Mingers (2000) for example comments that mathematical management science methods (see for example again chapters 2, 3 and 4) were in general of limited benefit to the practicing manager. Too often, the real world was not nicely packaged into a series of identifiable mathematical relationships. There was a need for better ways to conceptualise problems.

183
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 5

Systems views of the organisation and its environment are defined as an assembly of elements or components put together in an organised way (the structure) so that this assembly does something in the environment (it is a process or transformation) and that if one or more elements of the assembly are missing (i.e. there is an absence) the systems will be affected. Finally, these characteristics are viewed as being of relevance to one or more observers, who maintain a distinctive worldview of that system – i.e. the process achieved through a given structure, requiring key elements, thus adding value to an observer (Turner, 2009). Checkland’s subsequent Soft System Methodology (SSM) embraces these views and is therefore a learning system for an organisation (as reasons for actions in organisations by its staff are understood) but it also recognizes that models in an organisation are, in fact, only perceptions of the real world by members of that organisation (Checkland, 1985). There is an implicit assumption in this definition of a system however, that there is an ability of the observer to be distinctive from that of the system, i.e. they seek to add value through their more objective perception. This is not possible, however as from the system’s viewpoint the observers deriving value from that system also comprise elements of that system. Thus we must recognize the reflexivity of developing a system’s viewpoint of a decision or problem opportunity and that as the ‘interested’ observer considering a very rich and structured problem, it is not going to be possible to determine an optimal outcome. Satisficing decision solutions will instead be achieved via concensual agreement and through a process of improving identified divergences in the processes observed, to achieve an agreed system goal. The satisficing solution therefore could be articulated in the form of an action ‘plan’, to direct process improvements towards an idealized and group preferred/supported outcome. This generic systems approach forms the core of what is known as Checkland’s Soft Systems Methodology (mode 1). A practitioner oriented version of SSM also subsequently emerged over the 1980s, called Mode 2. Further developments of Systems Thinking (such as the Viable Systems Model) are outlined later although readers are directed to additional texts for detailed information (Flood, 1999: Beer, 1979).

5.4	

Checkland’s Soft System Methodology (SSM) – Mode 1

In its most accessible format, SSM is a seven stage structured process for identifying the scope for organisational change and to improve the likelihood that the organisation will achieve more of its intended objective. There is a recognition that the achievement of ‘a preferred outcome’ is subject to the individuals in that organisational system, their perceptions and values and how those individuals learn and better understand their complex inter-relationships. SSM is therefore concerned with recognizing reflexivity (as stated), recognizing that the aim of analysis is to understand the problem domain and recognizing that, typically, managers are interested in understanding what the identified system should be doing and what needs to be addressed to achieve this. Through this analysis, an agenda for discussion of change is generated, from which can emerge operational and strategic actions for the organisation. In Mode 1 of this approach, the analyst is assumed to know very little of the organisation and its problem context. Mode 1 is therefore focused upon an external intervention in the organisation to provide a structured debate about possible changes, focusing upon apparent differences between intended organisational outcome and emergent outcome. The seven steps in Mode 1 are:

184
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 5

      

Z>/dz ,GHQWLI\WKHXQVWUXFWXUHGSUREOHPGRPDLQ ([SUHVVWKHSUREOHPGRPDLQ /D'/Ed/KE ,GHQWLI\WKH5RRW'HILQLWLRQRIWKHV\VWHPLGHQWLILHG 'HYHORSDFRQFHSWXDOPRGHORIWKHLQWHQGHGDLPRIWKHLGHQWLILHGV\VWHP &RPSDUHWKHLQWHQGHGPRGHOZLWKREVHUYHGSUDFWLFH Z>/dz ,GHQWLI\IHDVLEOHDQGGHVLUDEOHFKDQJHV 3UHVHQWDQDJHQGDIRUFKDQJHDQDFWLRQSODQ

The transition between stage 2 →3 represents a mindset change from considering the reality of organisational practice, to the idealized context of organisational practice. Equally, the transition from stage 4→ 5 represents a mindset change from considering the idealized context of organisational practice to the reality of organisational practice. Hence we can also consider the methodology as one that operates in both halves of the brain (Lumsdaine & Lumsdaine, 1994) – the left hemisphere which is specialized in rational, logical and procedural analysis and here concerned with reality, whilst the right hemisphere of the brain which is concerned with creativity and imagination and here is concerned with what could be done to improve an organisational activity. We might describe this methodology therefore as whole brain thinking.

185
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Chapter 5

Its second key attribute, especially necessary for managers making inclusive organisational decisions, is that through the methodology, different perspectives on a problem or opportunity are specifically sought. This is called the worldview (or sometimes described through the German word Weltanshauung). This concept describes the set of values, perceptions and individual social constructs that allow individuals to derive meaning from their organisational activities. With the recognition that this mix of individual perspectives exists within the organisation’s human resources, there is a clear need to seek to identify what will help motive those resources to engage with and implement appropriate change in organisational activities. An aim therefore of this system view of the organisation is to try to seek consensual change, where the human resources are actively engaged with the decision making process and where their breadth of views and perceptions are recognized within that decision making process. There is therefore not necessarily a defined strategic objective to be achieved through the methodology of trying to identify an optimal solution or action for a problem or opportunity. Instead the objective is to progress towards an improved competitive and co-operative position (both internally and externally to the organisation). The methodology, whilst having an analytical structure, still follows the three phased models imperative outlined in chapter 1, of not seeking to generate prescriptive interpretations of actions. An intervention in an organisation in Mode 1 therefore commences with step 1 – entering the problem domain. This step is concerned with collecting appropriate information regarding the organisation and the problem or opportunity. This should involve both hard (quantitative) data and soft (human value) data. The second step is to translate this information into a rich picture. This is a visual representation of all that is known about the problem or opportunity requiring action. It can include words, images, icons, arrows, flowcharts and so forth. It should be able to easily communicate to another viewer, what the emergent concerns and issues are that are shaping the operation of the organisation in the areas identified. This image therefore captures the nature of the relationships in the problem domain. Next, is the analysis of the image and data collected. This step can be most effectively addressed by not considering what the problem is in the rich picture, but by seeking to identify the scope of emergent perceptions of organisational activity and the aims of those activities. We can describe these as the holons of the organisational system. Two interpretations of Mode 1 are presented in this chapter for this – the original Checkland method and the Midgely and Reynolds (cited by Williams and Hummelbrunner (2010) method. Firstly, the Checkland method asks the analyst to identify the following from the organisational data: 1)	 C- Customers – who/what is receiving the service/output of the organisational activity considered? 2)	 A – Actors – who/what is performing the activity to deliver/produce the service/output? 3)	 T – Transformation – what is being acted upon and being changed by the system activity? 4)	 W – Weltanshauung – what is the worldview that gives meaning to those involved in the activity, to produce the service/output? 5)	 O – Owners – who/what is in a position (internally or externally) to be able to stop/affect the activity of the organisation? 6)	 E – Environment - who/what in the external environment of the organisation is in a position to be able to affect / influence / halt the considered activity of the organisation?

186
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 5

Clearly, it is important to realize quickly, the relationship between the T and W in this CATWOE analysis. They should be identified as only being valid, when viewed from a paired perspective. Hence the change being sought through an organisation must be valid (and desired) by some individual or entity of or by, the organisation. The analyst should also expect to find a range of paired T-Ws deriving primarily from the C and A identified in the activity(ies) of the presented system. One important issue to note in these stages of analysis, is the nature of T. A T (Transformation) being enacted by the system considered, needs to be viewed as fundamentally a change of state for an input in the considered activity. Returning to an earlier notion about systems analysis being concerned with the overall orderliness of that system, the change of state refers to an increase or decrease in that system order (sometimes described as the entropy of the system). Above all else then, soft systems methodology is an interpretive paradigm (Brocklesby, 1994). For example the following would be considered valid T, for appropriate C and/or A: 1)	 Unplayed pitch → Played pitch (the pitch has been ‘transformed’ (gone from non churned up to churned up) through the activity of the game being played) 2)	 Untired players → tired players (the players have had their energy levels reduced through the game being played) 3)	 Unassembled car parts → assembled car parts (the parts have been acted upon through the activity of the organisation (the production line) and rearranged into a whole (although they still exist as parts). 4)	 Unresolved issue →resolved issue. This would be a generic transformation description. 5)	 Time available →time taken up. This would be also a generic transformation description common in an organisation (and typical of Mode 2 analysis discussed shortly). With an identified collection of descriptions now gathered for CATWOE, the methodology moves from the reality of the organisational activity, to the imaginary and considers what is the identified aim of the system considered when selecting one pairing and T-W (and appropriate C,A,O and E therein). This is called a root definition of the system being considered. Again, as there are potentially a large variety of T-W pairings, there can also therefore be as many root definitions too for a given system of interest. It is helpful to construct an identified root definition of the system considered, by bringing together the CATWOE descriptions in the form of a single sentence statement. Such that the root definition can be stated as: To do W by A, by means of T given the constraints of E and O (in order to achieve X for C). Consider an example of a media company owned by an independent Foundation, producing news stories on organic and sustainable foods. From a data collection activity and rich picture generation, let’s assume the following CATWOE is generated: •	 Customers = sustainable agriculture lobbyists •	 Actors = project evaluators, farmers, retailers, Sustainable Food Collaboration staff •	 Transformation = preponderance of bad stories replaced by a preponderance of good stories •	 Weltanschauung = the belief that stories bring about pressure for social change •	 Owner = The Foundation •	 Environment = established practice, isolated area, poverty and lack of investment capital 187
Download free eBooks at bookboon.com

Effective Management Decision Making This might then generate a root definition of this system as:

Chapter 5

To bring about social change(W) through evaluators, farmers, retailers and collaborative staff(A) who generate positive stories(T) cogniscent of the limits of resources and practice(E), on behalf of the Foundation(O) and for the consumption of external lobbyists (C ). Or – if this sentence is tidied up a bit: Foundation sponsored activities by the Sustainable Food Collaboration and their stakeholders create a set of good news stories about consumer use of sustainable agricultural products which lobbyists may subsequently use as part of their policy development levers. The second variation on the soft systems methodology, is described by the BATWOVE acronym. In this method, the key information to extract from the rich picture considers the following: 1)	 B – Who/What is/are the beneficiaries of the system (which can now be ideas as well as people)? 2)	 A- Who is/are the Actor(s) of the system? 3)	 T – What the ‘Transformation process’ is? 4)	 W – What the ‘Weltanschauung’ is? 5)	 O –Who is/are the owner(s) and could cause the system to halt? 6)	 V – Who /what is the victim of the system (which can be ideas as well as people)? 7)	 E – What environmental constraints on the problem domain exist?

Brain power

By 2020, wind could provide one-tenth of our planet’s electricity needs. Already today, SKF’s innovative knowhow is crucial to running a large proportion of the world’s wind turbines. Up to 25 % of the generating costs relate to maintenance. These can be reduced dramatically thanks to our systems for on-line condition monitoring and automatic lubrication. We help make it more economical to create cleaner, cheaper energy out of thin air. By sharing our experience, expertise, and creativity, industries can boost performance beyond expectations. Therefore we need the best employees who can meet this challenge!

The Power of Knowledge Engineering

Plug into The Power of Knowledge Engineering. Visit us at www.skf.com/knowledge

188
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making Hence the root definition is then described by the following ordered sentence:

Chapter 5

To do W by A, by means of T given the constraints of E and O in order to achieve X for B and change V. The next step (4), in Checkland’s original Mode 1 seven step method, is to build a conceptual (or notional) model of the imagined system (derived from the root definition). This is in short a visual representation of what actions must be undertaken to achieve the aim of the considered system. Hence, the focus upon building the conceptual model is to use the verbs from the root definition. This is most effectively undertaken, when the number of distinctive actions varies between 5 and 9, to achieve the aim of the considered system. Consideration must also be given to identifying appropriate measures of progress towards the achievement of the aim of the system, in other words the monitoring and control of the system. These are sometimes described as the ‘5E’s’: 1)	 Effectiveness – indicators that allow measurement of whether the conceptual model achieves what is needed and contributes to the organisation? 2)	 Efficacy – indicators that allow measurement of whether the conceptual model produce the required result (will it work?)? 3)	 Efficiency – indicators that allow measurement of whether the conceptual model uses a minimum number of resources? 4)	 Ethicality – indicators that allow measurement of whether the conceptual model will produce change that is moral/good/appealing? E.g. do staff sharing the viewpoint that it is a good solution(s)? 5)	 Elegance – indicators that allow measurement of whether the conceptual model is engaging through its simplicity. Is it beautiful / focused? (e.g. number of staff motivated and happy to engage with the solution(s)) Let us return to our earlier example and assume we have a problem statement that needs to be evaluated: Problem statement: Young managers are not entering the farming industry due to preconceived perceptions about the job and lifestyle. An analyst charged with addressing this statement (and finding ways to increase the number of managers entering this industry (as a system and strategic aim)), might therefore commence by interviewing new graduates, local employees, press and media, current farmers and undertaking desk research on incomes, pressures, lifestyle, future demand forecasts and so forth, to understand the process and issues that have generated this statement. This would generate a rich picture, from which a CATWOE analysis might then be derived: •	 C (or B) – farming population, local stores.. •	 A – farmers, journalists, teachers… •	 T – falling farming population → rising farming population OR negative press stories → positive press stories •	 W – Farming lifestyle is difficult and hard with little benefits (applies to both identified T’s) •	 O – Land owners, large contractors (e.g. supermarkets) •	 E – weather, regulatory changes (health and hygiene)

189
Download free eBooks at bookboon.com

Effective Management Decision Making This might then generate a root definition of:

Chapter 5

To improve perceptions of the farming lifestyle through journalists publishing more positive news stories given the macro and micro pressures of the economy in order to improve interest and careers consideration. From this definition, a conceptual model can then be constructed, which could be:

*DWKHUQHZVGDWD

0HHWZLWK)DUPLQJSRSXODWLRQV

&RPSLOH'DWD

'LVVHPLQDWH
Figure 5.2: Simple conceptual model

Figure 5.2 presents the conceptual model and to ensure progress towards the achievement of the aim of this considered system from the conceptual model of increasing the number of meetings between farmers and journalists, the following 5E’s could be developed: 1)	 Effectiveness – Will this contribute to the organisation’s activities? We can consider an indicator of this system aim through increased farmer participation (attendance) at meetings. 2)	 Efficacy – Will this generate more positive stories and scope for social change? We could count number of stories received and those which focus upon positive attributes 3)	 Efficiency – Is this efficient? We could review the expected cost of organizing and operating these meetings and if they were better than alternatives? 4)	 Ethicality – Is it morally good? We could consider the number of stakeholders engaged, their breadth, prestige and reputation rankings as indicative of an inclusive discussion and that decisions are not being taken by special interest groups. 5)	 Elegance – Is the model simple, clear and foused upon the aim? We could count the attendance at meetings, investigate and question perceptions of those involved. Figure 5.2 can also be called a first order model, as within in, there are other activities that have to be undertaken to achieve the first order activity listed. For example, ‘Gather news data’ will involve a variety of activities and order in those activities to be undertaken. Such a grouping of activities are called second order models, for the first order model (Brocklesby, 1994). Eg.

190
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 5

'HILQHUHVHDUFKIUDPHZRUN 'HFLGHRQDSSURSULDWHPHWKRGRORJ\ &RQVWUXFWGDWDFROOHFWLRQWRRO 6FKHGXOHZRUNDQGFRVWLW ,PSOHPHQWGDWDFROOHFWLRQ 9HULI\VDPSOHVL]H«
The next step (5) is to compare the imagined conceptual model stages of activity, with what is actually undertaken by the organisation to achieve the aim of the considered system. This focuses upon answering the questions: •	 Why aren’t we doing this ? •	 Why aren’t we doing it this way? •	 Why are we doing it that way? •	 What are the reasons for the current system?

Challenge the way we run

EXPERIENCE THE POWER OF FULL ENGAGEMENT… RUN FASTER. RUN LONGER.. RUN EASIER…
1349906_A6_4+0.indd 1

READ MORE & PRE-ORDER TODAY WWW.GAITEYE.COM

191
Download free eBooks at bookboon.com

22-08-2014 12:56:57

Click on the ad to read more

Effective Management Decision Making

Chapter 5

You can approach this stage in several ways. For example, either through a narrative discussion of these imagined and observed differences, or through a simple comparative table which considers the actions in the conceptual model, the observed practice in those observations and recommended actions. Recommended actions are also then considered in terms of their resource implications and cultural feasibilities. The final step considers what changes are desirable and which can then be implemented. The merit of the implemented changes can then be determined by iterative applications of the soft system methodology.

Mode 2 : Soft Systems Methodology
With the development and repeated implementation of Checkland’s original seven stage analysis, manager and analyst experience (the manager-as-analyst) of the issues arising from its implementation identified that managers within the organisation undertaking such an analysis, were able to short circuit several of the stages and were not necessarily bound by the linearity of the original seven stage approach. Checkland himself in his second text on Systems Analysis (Soft Systems Methodology in Action (1990)) rejected the seven stage method as being overly structured and restrictive (Mingers, 2000). Mode 2 recognises that those involved with the system being considered will have valid experiences of that system too and which will have been internalized by those individuals (Gold, 2001). Hence, we could describe Mode 1 as a formal intervention in a system (usually by an outsider to that system) and Mode 2 as informal interaction and enquiry (usually by someone from that system). As Mode 2 is a more internalized process, literature accounts of it, are not as common as they are for mode 1 interventions (although see Brocklesby, 1994 for a detailed intervention discussion using Mode 2). In other words, managers implementing Mode 1 also reported it as a mode of thinking about the organisation (Gold, 2001). Part of this difficulty (of internalization) is that the methods available to managers to interpret and communicate in their environment are also relatively opaque. Gold (2001) has argued that this can be addressed through the use of storytelling and metaphors, which are able to relay, colourfully and richly, work events and activities. This is further discussed shortly, when problems of mode 2 implementation are considered. Mode 2 with a practitioner focus is explicitly concerned with problems whereas Mode 1 could be viewed as being more concerned with the methodology. Mode 2 is situation driven, iterative and makes the transition between reality and imagination (implicit in the mind of the manager and which helps to make the methodology more useful and less ‘theoretical’). In this mode, the manager/analyst is expected to have a deep understanding of the organisation’s culture and values and be positioned so as to be able to implement desirable changes needed as a result of wider organisational activities (Brocklesby, 1994). For example, a senior staff member as manager-analyst at a University, would be positioned to be able to initiate organisational changes as required to meet regulatory changes originating from revisions to student funding for the UK Higher Educational system. Mode 2 therefore correctly has also been viewed as a system of learning. Mode 2 has two streams of analysis that warrant manager/analyst attention; the first is the cultural analysis of the organisational context whilst the other is the logic and rational based enquiry using mode 1 stages. Both streams are viewed as interdependent and it is through the process of comparison and reflection that the manager-as-analyst expects to identify feasible and desirable changes (Mingers, 2001). Logic-rational based enquiry tries to consider the current situation, the search for new opportunities and solution development and to find ways to accommodate individual’s needs (Flood, 1999). The cultural analysis stream is part of the intervention process and is a social and political analysis too. Mode 2 allows consideration, therefore, of three contextual factors of the system that the manager-analyst will be aware of:

192
Download free eBooks at bookboon.com

Effective Management Decision Making •	 The Intervention •	 The Social Systems •	 The Political Systems

Chapter 5

The intervention is focused upon the role and needs of the client (as appropriate), the investigators (of the problem) and the owners (of the problem). The social systems factor of the cultural analysis is concerned with roles, values and norms that influence individual behavior in the organisation. The political systems factor is concerned with political interaction, development of mutual support between individuals and the use of power in the organisation (Flood, 1999). A mode 2 view of SSM can also be viewed as part of a larger understanding of social reality, which is created and recreated through social processes (Checkland & Howell, 2000). So in Mode 2, the manager-as-analyst enters the social context of the real situation, negotiates carefully and reflectively in developing that understanding so as to be able to effect ‘work change’ and improvement (Checkland & Howells, 2000). As an undergraduate student it is possible that assessments or the desire to explore SSM in a practical context may be asked for. In those cases, Costley & Armsby (2007) offer guidance on these explored conceptual and practical issues where key aspects of these are: •	 ‘Work’ is a community of practice – with its own cultures, values, status and hierarchy all of which can be related to (student) pay. •	 SSM considers both the complexity of a presented/identified work problem but also supports a methodology to address feasible changes. •	 Experience of student managed work based projects does not usually indicate a difficulty with the conceptual foundation of a given methodology (e.g. SSM) – but that problems occur with the processes of carrying out those concepts. This is considered shortly. Attention to the concerns noted above will be reflected in a more considered and inclusive series of feasible recommendations in assessed submissions and / or professional reports. Systems analyses have found multiple applications (and development) in management. Skarzausklene (2010) for example, uses this holism view to try to identify the key constituent capabilities of effective leaders in organisations (and determining that the key holism features are a blend of envisioning the organisation, designing the organisation and integrating the organisation, into its environment. Brocklesby (1994) has used SSM to develop competence profiles in HRM. In that case, to help identify key skills and abilities required by the organisation in its staffing, to achieve a given strategic objective. Beer (1979) and colleagues (Espejo, 1999) have developed the Viable Systems Model (VSM) of organisational structural change and more recently supportive software to help in that analysis. The VSM method offers a further development of holism in management practice. It is concerned with the resources and relationships necessary to support an organisation’s viability (and in doing so moves the focus of analysis away from traditional hierarchical relationships) (Espejo et al, 1999). One key aim of the VSM method is to capture the “..interplay between the meanings people ascribe to their organisations and the relations they develop to actually produce them…” (Espejo et al, 1999:663). Identity statements are developed to help resolve this interplay. These are passionately not vision statements – but an attempt to capture the realism of organisational activity and purpose and in effect identify ‘what is done’ by the organisation. Identity statements are constructed using the TASCOI acronym (much as with CATWOE/ BATWOVE earlier). In this case:

193
Download free eBooks at bookboon.com

Effective Management Decision Making •	 T – Transformation •	 A-Actors •	 S-Suppliers •	 C – Customers •	 O- Owners •	 I – Interveners

Chapter 5

The Soft Systems Methodology which emerged over the last part of the 20th century, whilst welcomed by managers, has raised additional concerns within organisations. As Stowell (2009) notes, fundamentally the philosophical foundation for soft systems analysis lacks the historical presence and weight of reductionist approaches and hence its uptake as a management practice has not had the gains hoped for by its adherents. For example, with the development of Mode 2, it would be reasonable to ask the question about whether it could be possible to identify a poorly executed SSM analysis, given the high level of internalization of issues in the manager-as-analyst? (Stowell, 2009). Stowell (2009) then describes at length the processes through which this problem can be addressed in practice – namely: 1)	 Manager-as-analyst and system participants declare their Weltanshauung openly. 2)	 Recognising that the application of a systems based methodology will have no prescribed outcomes per se 3)	 Addressing the difficulties of interpretation in the system-as-practice and of relaying that understanding to others – as a problem of recoverability. This means ensuring detailed records are kept of the knowledge created with participants. Champion & Stowell (2003) cited by Stowell (2009) developed the PEArL acronym specifically for this purpose – so as to support reflection on the enquiry process. PEArL is:

This e-book is made with

SetaPDF

SETASIGN

PDF components for PHP developers

www.setasign.com
194
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making P – participants – who was involved/why/for what reason/who was excluded…

Chapter 5

E – engagement – how will P engage? Who is and is not a P? What are the external factors shaping engagement? A – authority – what is the ‘power’ associated with P? (resources? Reputation? Voice?...)Are there any external influences (e.g. political)?Are some methods of E better than others?(i.e. do they have internal and external recognition)? R – relationships – what are the key elements of power between P? How are P managed and by whom? L – learning – what are the intended and actual outcomes of the intervention? What is the manager-as-analyst view of how this was achieved and who has ownership of these outcomes? As a guide to the implementation of a SSM or VSM intervention, reporting on PEArL, is therefore an appropriate step in the research development and reporting process. 4)	 In the process of research - SSM lacks any need for an ex-ante knowledge perception of a problem/ opportunity (which is a comparative weaknesses compared to reductionist research methodologies). To address this Checkland & Howell (1985) cited by Stowell (2009) suggest the manager-as-analyst should state at the start of the work – what the FMA will be for the work (i.e. what the (F)framework of ideas will be in use, what the (M)methodology of enquiry will be in use and what the (A)areas for concern are). Stating these guiding framework elements, will make it clear for a reviewer of the work or to aid in its recoverability. 5)	 The research boundary - is important for any research, but needs consideration for SSM. In particular the manager-as-analyst needs to take care that the research boundary is sufficient to contain the system elements without reducing the necessary richness of interaction between those elements. The boundary therefore ‘holds’ the system of interest. It is recognized that this is a difficult objective to achieve in SSM, but nevertheless, the boundary should be stated. 6)	 Participation and power – in SSM, the manager-as-analyst seeks to participate within, but not contribute to, the views of the observed system of interest. Again this is not an easy aim to sustain and the literature discussions on the issue contain only broad attempts to include the diversity of power interpretations and applications. Finally, Mingers (2001) recounts the conflict between SSM analysts and analysts of critical systems. Where in the latter, concern was expressed that as SSM focused upon the values and beliefs maintained by individuals within the organisation it did not pay enough attention to the origin of those values and beliefs originated from. Stowell (2009) notes that soft systems research can be therefore loosely defined as heremeneutics in action (i.e. the construction and development of meaning) and where therefore organisational systems do not ‘exist waiting to be discovered’ – they originate from the individual.

5.5	Summary
This chapter has considered the duality of management decision making by a focus upon the assumptions of credence decomposition (problem reductionism). In short, many problems and opportunities managed by organisations are embedded in a social and political context, which also contributes to the understanding of those issues. Deriving a model for that issue which is reliant upon a decompositional approach seeking to identify relevant sub units of the problem, which can then be solved, to understand the larger problem presented, loses the richness of that decision context. The recognition of this problem, helped general system theory to evolve into a range of system based methodologies, which attempt to view the holon of a problem.

195
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 5

A holon is a word used to describe a system that has emergent properties (which cannot be easily identified by an identification and consideration of the components of that system) that allow it to adapt to changing environments through changing structures and communication processes (Boarder, 2001). At its core therefore, management decision making is the requirement of managers to constantly address holons in their working careers or in other words, to address the duality of decision making. Peter Checkland and colleagues at Lancaster University in the 1960s and 1970s, pioneered the Soft Systems Methodologies which have latterly been developed internationally. These methods allow for learning of and in an organisation to occur with a system aim of generating an improved and concensually supported accommodation of change in the organisation. A survey of the adoption and success of SSM methods during the 1990s revealed high levels of satisfaction within a large cross section of managers and analysts (Mingers, 2001), but also that SSM tended to be used in conjunction with other analytical methodologies to address organisational difficulties. Issues of implementation and the use of SSM were presented in this chapter with particular foci on PEArL and FMA as helpful aides in increasing the recoverability of SSM research. In the next chapter we explore further the reasons for the apparent divergence in world views of problems expressed by individuals and groups of individuals, in organisations and in particular, try to identify methods to manage and mitigate some of this divergence.

5.6	

Key terms and glossary

Complex problems – are problems that cannot be reduced to more focused sub-problems as in doing so, the richness of their interaction (and hence the understanding of the problem) would be lost. Credence decomposition – describes the practice of reducing a problem into a smaller subset collection of problems, that can be resolved to allow the original larger problem to be solved. Entropy – is a concept that describes the order or disorder of a given system First order model – describes the initial sequence of activities that must be undertaken to achieve a given aim of a root definition. General Systems Theory – is the broad heading applied to methodologies that seek to identify the agreed ‘facts’ of the situation and the individual values in the problem domain, issue interdependency in the problem or decision to be taken. Hermeneutics – is a concept used to describe the development of meanings in a given situation over time. Holon – is a concept that describes both the whole and the individual in a given system. Identity statement – is a statement that seeks to capture ‘what is done’ in a system (or by an organisation). They are a variation of a root definition. Interpretive paradigm – describes how reality is constructed by subjective perception and therefore predictions cannot be made using for example, statistical data.

196
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 5

Logical incrementalism – describes a method of decision making focused upon small changes to a given solution, to apply it to a new problem – but with an overall aim that small decision steps are working towards achieving. Recoverability – describes the need to be able to, as much as possible, replicate a given interpretive analysis by describing key activities undertaken. Reflexivity – is the concept that describes how an individual in a system, shapes how that system performs or is understood. Rich picture – is a key stage in Checkland’s SSM concerned with representing the breadth of understanding and knowledge concerning a problem space and situation. It is a visual collection of words, icons, images and relationships that describes hard and soft data on a given problem. Root definition – is a stage from the SSM which describes a primary aim and objective of a given system Second order model – describes the subsequent series of activities that may be required to fulfill one of the stages of the first order model. SSM Mode 1 – was the first development of Peter Checkland’s holistic problem solving methodology and was focused upon an external intervention in a given problem process. SSM Mode 2 – was a later development of Peter Checkland’s holistic problem solving methodology, but focused upon managers (with knowledge of a given decision context) being able to intervene within that context using the stages of SSM 1 Mode as appropriate. Whole brain thinking – describes how problem solving methodologies can embrace both the rational and creative cognitive functions of the human brain. Worldview / Weltanshaunng – Is the concept that describes the perceptional constructs of an issue or concern for an individual, which supports the sensemaking of that situation. It allows schemas and heuristics to be applied (see chapter 6)

197
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 6

Chapter 6
6.1	 The individual in decision making: Heuristics in Management Decision Making:
Moving on from the frameworks presented by Soft Systems Methods as guides to structure organisational intervention and research (Checkland, 1985), if we are to consider the systems view of problems and decisions, then we need to also need to consider how individuals can interpret the decision context. This chapter therefore considers how individuals can both interpret a decision context and what problems emerge when we look at that interpretation. Smith and Sharma (2002) note, for example, that specifically considering the value and emotions associated with organisational staff has historically been perceived as a weakness in the development of a consistent strategic aim and its implementation (Maccoby, 1976 cited by Carr, 2002). Yet, as chapter 5 has illustrated, management methods need to embrace these organisational features fully as well. This becomes quite self evident when the evidence is reviewed that organisational structure and processes can simply be the extension of the ‘self ’ of the leader(s) (Kets de Vries and Miller, 1991). In addressing the first issue of interpreting the decision context, we can adopt an ideographic perspective of the individual – or an inductive view, which views individuals as unique – but in doing this we then have the difficulty of developing sufficiently generalized (or nomothetic) views of individuals to allow managers to structure the decision context. The earlier three phased model of chapter 1 would be an example of a nomothetic model for example. Similarly, we can look for nomothetic themes in individuals to help understand what may be common amongst how individuals make decisions. Three distinctive methodologies have thus been developed to look at this question. These are :

In the past four years we have drilled

81,000 km
That’s more than twice around the world.
Who are we?
We are the world’s leading oilfield services company. Working globally—often in remote and challenging locations—we invent, design, engineer, manufacture, apply, and maintain technology to help customers find and produce oil and gas safely.

Who are we looking for?
We offer countless opportunities in the following domains: n Engineering, Research, and Operations n Geoscience and Petrotechnical n Commercial and Business If you are a self-motivated graduate looking for a dynamic career, apply to join our team.

What will you be?

careers.slb.com

198
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making 1.	 descriptive methods 2.	 explanatory method 3.	 prescriptive methods

Chapter 6

These three methods have been used by four dominant but interdependent knowledge bases of understanding individual actions (or their epistemologies) (Gross, 1996): 1.	 Psychodynamic 2.	 Behavioural 3.	 Humanistic 4.	 Cognitive (including neurobiology) All four approaches claim to offer the understanding of the individual’s decision making processes, but also they raise questions of being able to identify and observe these processes – for example, behavioural epistemologies offer stronger evidence of these process as they can be measured and observed in practice, unlike cognitive epistemologies. Hence we also need to consider the appropriateness and validity of each view and how evidence for that view is gathered. We will consider each of the epistemologies in turn.

Views of decision making
The psychoanalytical view sees the individual, as opposed to the external environment or other factor, as dominant in determining human behavior. The premise of this view is that unconscious and irrational processes play an active role in individual decision making and organisational activities (Jarrett & Kellner,1996). There are therefore affective and behavioural factors shaping individual development, which are believed to become more evident as risk and anxiety associated with a given decision, increases (Jarrett & Kellner, 1996). Early insights presented to understand this anxiety in decision making were developed by Sigmund Freud, who argued that behavior of individuals is derived from non-conscious aspects of individuality, including biological development. This development generates three contributing non-conscious facets of individuality which have been labeled (Carr, 2002): •	 The Id – also more famously known as the ‘pleasure principle’. For Freud this aspect of individuality represented the unfettered child, devoid of any constraints, who acts hedonistically at all possible opportunities. It also therefore encompasses self interested actions and behaviours which could be negatively viewed by others. •	 The Ego – was the facet which is inherited from our immediate family and their sets of value on right or wrong, good or bad. For Freud, it provided a moral framework for making informed decisions through logic, memory and judgement to appropriately satisfy human needs and wants (and when to do so). •	 The Superego – was the facet which described the societal set of cultures and values within which individuals live and share community.

199
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 6

This view argues that it is these facets in conflict, which drive individual behaviours’ and actions as individuals seek to minimize this conflict and internal mental discomfort (this is also called cognitive dissonance (acting because we feel uncomfortable holding two conflicting ideas at the same time)). It is argued that the extent of the individual’s awareness of these facets and of these facets in others, which contributes to effective communication and decision making. The Johari window (Luft & Ingham,1955 cited by Hase et al, 1999) is a practical outcome of considering the rigidities between effective communication in individuals (and is discussed in more detail in chapter 7). From this perspective of decision making, unfamiliar situations can provoke actions and behaviours that may lead the individual to implement childhood coping mechanisms that have been recalled from the sub conscious. Jung also proposed that there was a deeper layer of unconsciousness, from which humans derive primordial ideas, images and emotions (Carr, 2002), from which common global myths emerge amongst many apparently disparate branches of human society. These were called the Jungian archetypes from the collective unconcious, although common usage of the term ‘archetype’ has lessened its correct interpretation. For Jung, the importance lies with between the shape of the archetype and the content of the archetype (i.e. what story is used to identify the message of the archetype). For Jung, the individual’s personality development was a continual process. Wilfren Bion and George Homans are well known researchers who have explored this belief. Whilst their primary area of interest has been in the understanding of group decision making, their insights also help our understanding of individual actions and decision making. For George Homans (1950), the actions of individuals are shaped by the perception of the task to be resolved, the individual satisfaction to be derived in resolving that task and the personal development that results from that. Individuals contributing towards a decision in a group are influenced by how cohesive that group is and the interdependency between the three areas of task, satisfaction and personal relevance. Satisfaction for example increases by an individual conforming more to group norms and gaining that social approval. For example, identity has both a distinctive individual quality and a social quality – people wish to belong to a social norm with an accepted status but also be recognised as unique within that environment. The influence upon a motivation to perform a behaviour is derived from both a self identity and a social identity construct (Terry et al. 1999; Anderson & Warren 2005). In short, people work in groups for the group and for themselves. Wilfred Bion (1961) developed the Freudian perspective (and latterly the Berne view (discussed shortly)), where individual conflicts and/or agreements between tasks and belief in those tasks causes members to not focus on the task at hand in group decision making. Bion recognized that groups of individuals tasked with resolving a decision have two important aspects to manage – the task at hand and assumptions about the required actions and behaviours to achieve that task. Under conditions of stress, anxiety and uncertainty, these assumptions of behavior can block the effectiveness of decision making by the individual in the group (McKenna & Martin-Smith, 2005), as individuals in the group swing between these assumptions in guiding their behavior. The identified assumptions in practice are: 1.	 A dependency assumption – that the group seeks a charismatic leader to resolve the task 2.	 Fight-Flight assumption – that the group acts to fight or flee an enemy (or construction of an enemy) and the perception of a win or loss by the group

200
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 6

3.	 Pairing assumption – that the group members hope an expert amongst their membership will step forward to provide the answers needed 4.	 Oneness assumption – that the group will seek and join an external force to resolve the task and allowing the group to continue passively (this is a fourth Bion ‘assumption’ identified by Torquet (and cited by Stacey (2003) cited by McKenna & Martin-Smith, 2005). Freud’s early view of the origins of individual behavior through unconscious cognitive conflict was then developed by From Wilfred Penfield (1951) and later Eric Berne (post 1968), in the area of Transaction Analysis (TA). It has also been explored in other management subject areas such as entrepreneurialism by Manfred Kets de Vries and his studies on entrepreneurial social deviants and misfits and Elizabeth Chell in her studies of critical incident theory (whereby early events in individual’s lives have impact upon their future development). Transaction Analysis, has become an important area of both psychoanalysis study and management consultancy. Berne developed early transaction analysis by proposing that individuals are comprised of three mental modes: the child ego state – where behaviours, thoughts and feelings are recalled and replayed from childhood, the parent ego state – where behaviours, thoughts and feelings are copied and learned from parental figures and the adult ego state – where behaviours, thoughts and feelings are direct responses to the individual’s situational context (the here and now). This was developed in to the so called ‘descriptive model’. This development views the Adult state now as the ‘Accounting Mode’- through which individuals are able to interpret communications and choose how to respond to those communications. These are called ‘strokes’. In both aspects of the Parent and Child state, strokes can be positive or negative and invite appropriate responses to those strokes, through the Adult state. For example, a negative child state stroke which is driven by the desire to express their freedom and ‘play’, should draw a stroked response through the Adult accounting mode of the negative punitive parent, to control the child. Berne argued this is the basis of effective communication, the selection of appropriate responses. Equally, ineffective communication can then occur by a poorly or inappropriately chosen response to a given stroke. It has been noted in the discussion, that extending beyond the psychoanalytical view of decision making, we need to consider the task, the situational context and the relevance to the individual of those factors. For example, as an organisation develops and grows, there is a need to ensure an appropriate fit between the decision maker’s style and source of judgement and that growth. •	 In the emergent and entrepreneurial stage, an organisation may be more accepting of a behaviour – even tantrums(the Child state) - in senior managers due to the unformed basic assumption set, unformed organisational power structure and relatively wild free-flowing environment (McKenna & MartinSmith,2005). •	 In a mature stage of growth with strong, functional basic assumptions and culture- an organisation through its staff, would not accept petty or pedantic behaviour, except in individuals with very high levels of personal power (the Parent) – in other words, the organisational members could not reconcile the behaviour with the accepted organisational culture(McKenna & Martin-Smith,2005). •	 In a transformational stage where boundary breaking decisions which oppose the basic assumptions and culture are often required to “drag” the organisation into a new environment, these decisions would also depend greatly on the power position of the decision maker (Adult)(McKenna & Martin-Smith, 2005).

201
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 6

The judgement context – schemata
Friedman (2004) asks the question about why manager’s often have misperceived understanding of their organisational problems or issues. We will discuss the issue of ineffective communication between individual decision makers in chapter 7 with the Johari window, but we can note for this discussion that one outcome of this weakness is then that manager’s tend to make decisions which have unintended consequences for their organisation. This misperception can arise due to (Friedman, 2004): •	 Cognitive bias - can arise due to an individual’s experience, learning environment and organisational culture and context. There may also be neurological imperatives shaping these biases too. Later in this chapter we consider one well known area of cognitive bias, that of heuristics •	 Mental models and schemata – these are the ways in which cause and effect are interpreted by the manager and therefore shape how that individual manager believes a problem should be resolved. This then leads to the selection of appropriate judgements to use – called schemata. These are cognitive processes (further discussed below) which for example can direct how and individual searches for a solution, how information is organized, how information is sorted and used etc. •	 Emotions – have tended to be viewed negatively in an organisational context, although ample evidence of their importance as factors shaping cognitive structures (chapters 5,6 and 7). •	 Perseverance of ineffective beliefs- occurs when managers resist changing their beliefs to correspond to actual observed evidence. This resistance can have many sources, from questioning the validity of evidence presented, to how we attribute success or failure to ourselves or our external environment.

202
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Chapter 6

McKenna & Martin-Smith (2005) further note that individual schemata (discussed shortly), from which cognitive actions are chosen and applied (heuristics) can be affected by perceptions of those individuals (such as the locus of power in an organisation) which may not be externally perceived as rational. Consider the situations in Box. 6.1 for example: Box 6.1 Irrational rationality? Lewin et al (1990) make numerous references to instances where trade union action to strike can appear irrational to observers, but is in fact rational to the union (despite counter claims actions are based upon incorrect forecasts of benefits and costs) as the action still seeks to maximize self interested behavior. Tiwana (2006) discuss the observed phenomenon that failing software projects are often in receipt of escalated resources, yet from the project manager’s point of view, this escalation can be in pursuit of a variety of other real options which can capture much of the original project premise value. Moon et al (2003) similarly cite that project escalation of a failing activity can also originate (at least in part) from a group bias dynamic (see chapter 7 for a further discussion). Vander Shee (2008:244) notes “…although irrationality is commonplace, it does not necessarily mean that we are helpless. Once we understand when and where we make erroneous decisions, we can try to be more vigilant, force ourselves to think differently about these decisions, or use technology to overcome our inherent shortcomings.” One of the key aims of this text is to present in an accessible format, insight on the causes of these erroneous decisions so that a manager is better able to anticipate and mitigate the symptoms which generate false judgements. The behavioural view of decision making rejects the ‘internal mind’ of psychoanalysis (the nomothetic view) and instead argues that observation is the important focus as this can be measured and observed. Decision makers are rational and analytical and use dispassionate processes which may include ethical and moral standards (Mckenna & Martin-Smith, 2005). It also argues that individuals are shaped by their experiences and by environmental forces, whereby they are defined through a series of collected learned responses acquired throughout all ages of development (Gross, 1996). Thus this view seeks to make a ‘better claim to ‘truth’. Hence, experience is now perceived as the key factor that motivates actions. Classically, this has been also explored through reflexive & stimulus / response and operant conditioning (voluntary & reinforcement). One observed difficulty of this view is the reliance upon past experience as a guide to future decision making actions can shape and limit those future decisions, when there is limited prior experience of a given situation. This has been previously identified with the incrementalist approach to decision making discussed in chapter 5 and moreover, the preceding chapter and this one, have sought to emphasise that decision making is not a linear nor rational process in general (McKenna & Martin-Smith, 2005). Smith & Sharma (2002) note that fundamentally, human beings are social animals and hence the reason why meetings between individuals in organisations play such a prominent role in most decision making activities (although this does not negate the need in many people to understand how formal meetings work within an organisational context and the importance of self disclosure (see chapter 7 for a further discussion)).

203
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 6

The cognitive view of decision making proposes a synthesis of the preceding views to understand individual decision making behaviours, where in other words, decision actions are shaped by our experiences, behaviour, environment and an individual’s processing abilities (Shackleton et al, 1990: Gross, 1996: McKenna & Martin-Smith. 2005). The biological development of this view argues behaviour is determined through genetic, neurobiological and physiological factors and which takes time to develop in individuals, as they grow and mature (Gross, 1996). From a cognitive view, decision actions can therefore occur through cognitive dissonance for example holding conflicting moral views ‘ I am a good person’, yet using the internet to pay for an assessed submission, or knowing that smoking is unhealthy yet being a smoker and still seeking to enjoy a healthy lifestyle. They can also occur through interaction with others and effective communications or as noted, through consideration of the relative access to power and influence in a given situation. People, in this school of thought are viewed as being information processors and therefore have limitations upon those abilities. It is a subjective view which further proposes that any external reality is subject to interpretation by the individual through mental constructs, schemas and narratives, all of which take time to develop. This viewpoint has emerged in the preceding discussions and which is explored in greater detail shortly. Shackleton et al (1990) and Hardin III (1999) give succinct summaries of the development of this approach to decision making with in particular, Hardin III (1999) stressing Herbert Simon’s original work on the limitations of human cognitive processing, but latterly also identifying developments that integrate this with the problems of short term memory and the decision making process (the judgement aspect as discussed in chapter 1). Shackleton et al (1990) present the Driver & Rowe (1987) cognitive decision style model, which is mentioned here as it is a helpful summary of the decision making issues raised so far and which identifies: •	 the directive style of decision making – reflects individuals who act promptly, based upon organisational/ ethical/moral rules and covet power and authority. •	 the analytical side of decision making – reflects individuals focused upon creativity and innovation on resolving problems or exploiting opportunities. •	 the conceptual side of decision making – reflects individuals with a high orientation for achievement, a desire for independency and artistic preferences. •	 the behavioural side of decision making – reflects individuals seeking affiliation, easy and effective communications and using coalitions of interest to achieve preferred outcomes. We can also note that in Chapter 5, the systems view and SSM approach to problem analysis, solution development and selection for decision making similarly reflected left and right brain thinking, which is also notably at the core of the four aspects of the Driver & Rowe model of decision making styles. As a result of these insights, management interest in the development of cognitive schemas and heuristics has increased. Heuristics are discussed in more depth shortly, but in simple terms, they refer to organized elementary information processes (Simon, 1978 cited by Hardin III(1999)) and schemas are then describing the problem space within which those heuristics derive their meaning (i.e. the associated knowledge for a given management context). In short, this view of decision making sees the manager as interpreting the task at hand (the problem domain), developing a problem space for its resolution and deploying appropriate information processes (heuristics).

204
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 6

In addition to this cognitive subjective focus, recent neurobiological research has identified relationships between biology and conceptual decision making of individuals. For example, interesting research has highlighted that economic decisions extend beyond cost/benefit analysis (see Naqvi, Shiv and Bechara, 2006). Further evidence of the importance of neurobiology to individual decision making has been found by considering the impact of ventromedial prefrontal cortex damage research (vmPFC) on individual decision making. Individuals with vmPFC were observed to make more frequent decisions that resulted in personal losses/injury compared with uninjured individuals, but did not seem to learn from those experiences, so as to reduce their losses in the future. The injury of vmPFC did not affect an individual’s intellect, problem solving abilities or memory but did seem to affect that individual’s ability to use emotions in their decision making. Evidence suggests that reasoning about moral dilemmas activates structures including the vmPFC and which is amplified when there are negative consequences for others. One final view of decision making to reflect on, is the humanist view of individual decision making, which progresses our understanding beyond cognition, to consider broader issues of the ‘World View idea’ and ‘the whole individual’ discussed in chapter 5. It is another branch of human psychology, but one that argues motivations and actions come from the need to self actualize and freely pursue and realize personal ambition and aims (McKenna & Martin-Smith, 2005). Purposeful actions therefore are a mix of experiences, meanings and choices. Individuals are driven by the need to meet self development aims and difficulties emerge in their actions, when those aims cannot be achieved (Gross, 1996).

Find and follow us: http://twitter.com/bioradlscareers www.linkedin.com/groupsDirectory, search for Bio-Rad Life Sciences Careers http://bio-radlifesciencescareersblog.blogspot.com

John Randall, PhD Senior Marketing Manager, Bio-Plex Business Unit

Bio-Rad is a longtime leader in the life science research industry and has been voted one of the Best Places to Work by our employees in the San Francisco Bay Area. Bring out your best in one of our many positions in research and development, sales, marketing, operations, and software development. Opportunities await — share your passion at Bio-Rad!

www.bio-rad.com/careers

205
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Chapter 6

6.2	

More on the cognitive view – schematas and heuristics

After this broad overview of key factors shaping effective individual decision making, the discussion now develops further the cognitive view of decision making. We noted that in chapter 2, methods of making decisions under uncertainty were presented, where probabilities attached to actual event outcomes were unknown (and in which case each event was identified as equally likely) – so that decisions could be made through the optimistic criteria (Laplace), evaluated so as to generate the best worst outcome – the conservative criteria (Wald) or evaluated so as to generate the least worst outcome through a regret analysis (the minimax regret (Savage) criteria). In all cases therefore, inevitably we do ascribe some probability likelihood to the range of outcomes. We now consider how to manage those situations where we have no available data at all, on the likelihood of outcomes. As this chapter was being written, there was significant publicity for an otherwise little known human psychologist Daniel Kahneman (or rather as much as there ever can be for a human and cognitive psychologist in the public press) (Burkeman, 2011: Lewis, 2011). In the learned academic environment however, Daniel Kahneman and Amos Tverskey are highly regarded for their pioneering work on cognitive psychology, which was briefly introduced in chapter 5. Indeed, it earned them the Nobel Prize in economics. The recent publicity was driven by the forthcoming publication (2011) of Kahneman’s first ‘mass market’ text on decision making (Thinking fast and slow). Their work was undertaken largely between 1971 and 1984 and which focused upon the reasons for why individuals reach decisions, and in particular how information is interpreted by them (i.e. their problem space, schema and heuristics). Schmidt (2011) identifies four types of schema employed by individuals in making decisions: •	 Person Schema – mental constructs concerning the attributes of a particular individual. •	 Event schema - are the mental constructs of the ways in which tasks and problems are approached. The task at hand shapes the selection of an appropriate problem methodology. •	 Role schema - are the mental constructs that are used to present normative understanding of individual role expectations (i.e. how we expect an individual occupying a certain role to behave). •	 Self-schema – are mental constructs that are maintained by the individual of the present situation and past experiences. This contributes to an individual’s self-image and where for example, self-efficacy is a type of self schema that applies to a particular task. Schmidt (2011) further clarifies the role of schemas as evaluative (i.e. where there is a mental comparison between individual actions for a given function or job), role playing (where they generate understanding about how individuals should act in a given situation), identity (where they categorize individuals by the roles they do) and prediction (where by mentally placing individuals in certain roles, they are then expected to behave in those roles). Clearly, this view of the source of decision making judgements is also manifest in behaviours of individuals as was also identified in the discussion of transactional analysis earlier and expected scripts (role plays) of individuals involved in a communication transactions (Berne, 1996).

206
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 6

The origin of schemas deployed for the problem space comes from individual experiences. Carr (2002) has also proposed that Jungian archetypes offer an origin for individual schemata too. It is reasonable to argue that there may be a required sequence of experiences required to develop a particular view of a given decision making context from these origins (i.e. constructs require maturation) and they may be acquired directly (as discussed) or through the use of a communication vehicle (such as metaphors or storytelling (see chapter 5 and the use of storytelling within organisations for Mode 2 SSM)). As the SSM and holism discussion further stressed in chapter 5, the understanding of the problem domain (and here problem space) needs to be an iterative and ongoing process of learning. Without this as Schmidt (2011) states, the accurate diagnosis of a problem (or exploitation of an opportunity) can be blocked. Furthermore Hardin III (1999) argues that individual bias in decision making may occur through the misapplication of schema and/or under developed knowledge of the problem domain. From a management perspective then, how that process of determining an action is made, is subject to bias and prejudice based upon information uncertainty, scarcity and individual interpretation. We noted in the first chapter from the Garbage Can model of decision making from Cohen et al (1972) that it is highly unlikely that all the necessary information, resources and contexts will be convergent in an organisation to make an optimal decision. Hardin III (1999) presents the work of Shanteau (1992) and his ‘Theory of Expert Competence’ to help clarify the necessary decision making inputs and which also identifies the sources of bias in individual (expert) decision making that require management attention. These are: 1.	 The decision maker needs to have problem domain knowledge (codified and tacit) 2.	 A robust leadership style including self confidence and belief 3.	 A cognitive ability to identify key information from within the problem domain 4.	 To be able to use heuristics and data simplification procedures to resolve complex scenarios 5.	 Be able to define clearly the issue / problem to be addressed and judged. As we have noted from chapter 5 however, it is unlikely that all management problems will be clearly delineated and packaged so that the decision maker is able to determine a full and detailed understanding of the task. In these situations, it is likely that bias and incorrect use of heuristics (through incomplete understanding of the problem space) will be the result (with sub-optimal decision outcomes). Now we begin to identify what consequences this lack of an understanding of the problem domain and space have upon effective decision making. Lewis (2011) makes the very valid point, that the more you begin to consider why a human endeavour to achieve a given goal fails or does not succeed as hoped, the more you begin to discover the work of Kahneman and Tversky.

207
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 6

For example, Gigerenzer (1999) and his colleagues asked four randomly chosen groups of decision makers (in this case graduate students of finance and economics from both countries and similar sized groups of Munich and Chicago shoppers) to select from a listing of 798 American and German companies, which ones they recognised. From this listing, two portfolios of companies were selected (one for the US and for German companies). Where the volunteers were choosing from their own country firm listing, a 90% recognition across the sample people was enough to warrant that firm’s inclusion in the resulting stock portfolio. However, for the shoppers from both countries looking at the company names from the other country (i.e. not their own), this common citational requirement had to dip as low as 10% to ensure that sufficient companies were in fact selected for the stock portfolio. When the value of the company portofolios created were then compared six months later, the selection chosen by the shoppers in Munich had grown an impressive 47%, 10% higher than the German DAX overall, over the same time period. More importantly, all four portofolios where the firms chosen had been from the other country, had outperformed those country’s stock market measures. To ensure that chance had no significant impact upon the selection of companys in the portfolio lists, a control listing of random firms performed more poorly than the selected portfolio’s in 7 out of 8 instances. Clearly, the selection of stocks by individuals from their non home country was a marked success in terms of the value of those stocks – why? The accepted viewpoint on why this has happened, is found in the works of Daniel Kahneman and Amos Tversky and their studies of the ‘availability heuristic’ - which has been used by the individuals concerned. Heuristics are mental (cognitive) functions that allow individuals to make decisions under a context of uncertainty or scarcity. When presented with a new situation, an individual will attempt to recall similar examples and draw relevant insights from that recollection. The more recent or vivid a relevant memory is the more of an impact it can have on the current situation being considered or decided upon. As with the example of the selection of stockmarket companies, this can be a marked improvement on random selection as an answer to a problem. However it can also result in bias (when for example an event is recalled precisely because it is rare). Being able to easily recall an event does not mean it is more or less likely to actually occur. As Goodwin & Wright (2004) state, the fear of crime is disproportionately more observed within some parts of society than actual incidents of crime in those parts of society do make clear. Recent press and the language and tone of the press, is an important variable here that shapes our ability to recollect this information. Furthermore, a prejudice towards viewing the activities of the organisation may result in illusory correlations between the contribution of those activities. For example, let’s assume an organisation has established a manufacturing plant in Vietnam. Through having early quality problems with semi finished goods shipped to the UK, that plant may then suffer an illusory correlation that all their semi finished goods are of poor quality within the organisation. Yee et al (2008) for example cited Iaffaldano and Muchinsky (1985), who had described that employee satisfaction and job performance was an illusory correlation. In other words, we expect there to be a correlation between the two variables, but there is no empirical evidence of that. Equally Grant (2010) in discussing the contribution of innovations to the profitability of organisations, identifies that where a correlation might be expected between the two activities, there is in fact very limited data for this. It is important therefore for managers to consider the persistent influence that preconceived views and the immediacy of recall for an event, can have on how effective the availability heuristic is.

208
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 6

This heuristic can manifest in a variety of forms – for example in the earlier discussion through individuals being able to identify an organisation through a recollection of that firm (say through a TV report or other announcement). This can explain for example, why increased marketing in a recessionary period (which might seem counter intuitive) may generate positive results for an organisation as shoppers are more easily able to recollect that marketed organisation. Srinivasan et al (2005) for example, in a survey of 154 senior marketing executives, identified a positive correlation between a proactive marketing approach in a recessionary period and brand developed products and services. Indeed Aaaker(2009) suggests that aggressive marketing during a recession also tends to generate improved sales which continues after the end of the recession as the gains tend to be ‘sticky and persist’. More importantly, Aaker(2009) citing work by Kamber (1992) identified that recessionary and post recessionary success was not dependent upon the existing competitive strength of the organisation. Overall, there is less competition for the attention of consumers and organisations that engage in proactive marketing, raise their profile and consumers find it easier to identify with and recall that organisation’s name and/or product/brand at the point of the buying decision. This can therefore offer a significant opportunity for the prepared organisation, willing to exploit the opportunity of the recessionary period. Another simple example of the availability heuristic, is to ask an individual whether there are more words in the English language which have k as the third letter or as the first? The ease with which an individual can recall words beginning with k, does not mean that in reality there are more words beginning with k (as the opposite is in fact true). Nonetheless, this does not stop most individuals choosing the wrong answer here. Nor does it stop governments and the media exploiting the human preference towards viewing more recent events as more important than distant events and manipulating that to preferred outcomes. Clearly, this heuristic (and others discussed), allow individuals to make rapid decisions and can be effective in doing so. Yet accurate statistical data will be superior in every event, if it is available. However, much more important than this, is the consequence of the implications of identifying that all individuals possess these heuristic attributes and therefore the very foundation of classical economics – that it is comprised of rational, selfish agents, who’s tastes do not change – is flawed (Lewis, 2011). Behavioural economics was the resultant new discipline that emerged from the convergence of these views. In total, over 133 different forms and types of heuristics have been revealed by cognitive research (Goodwin & Wright, 2004). Several more will now be illustrated that are particularly relevant for managers.

6.3	

More on heuristics

Hardin III (1999) and Goodwin & Wright (2004) identify that the 3 earliest generic types of heuristic identified from the work of Kahneman & Tversky were: 1)	 The availability heuristic 2)	 The representativeness heuristic 3)	 Anchoring and adjustment heuristic The availability heuristic has been discussed above and we now consider the other two generic types of heuristic.

209
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 6

The representativeness heuristic is a cognitive process through which individuals consider how similar a given event/ person/process is to a family of events/persons/processes (in other words, is event X representative of other known events) with a tendency to view a sample as being more representative of a parent population that it actually is. So if you were to meet ‘Linda’ who works in the USA and described herself as an outgoing individual with interests in climate change and global warming– when faced with the question – do you think Linda is either a lawyer, or a lawyer with a support for environmentalism – what would you answer? Rationally you should identify that it is far more likely Linda is a lawyer. Selecting the second option is logically incorrect (as to be this, she must also be a lawyer). Also it is statistically far more likely that she is a lawyer in the USA as an occupation without any additional focus. This heuristic is a cognitive process through which an individual seeks to identify given characteristics which associated stereotypes. The human brain is particularly gifted in identifying and seeking patterns in data (Marques de Sa, 2001) which can then be erroneously interpreted as evidence of causal relationships (Roberts, 2004). Goodwin & Wright (2004) identify that the representativeness heuristic has a number of key perceptional biases associated with it. These include ignoring the base rate frequencies associated with an estimated probability. For example, consider this description in Box 6.2:

678'<)25<2850$67(5©6'(*5((
&KDOPHUV 8QLYHUVLW\ RI 7HFKQRORJ\ FRQGXFWV UHVHDUFK DQG HGXFDWLRQ LQ HQJLQHHU LQJ DQG QDWXUDO VFLHQFHV DUFKLWHFWXUH WHFKQRORJ\UHODWHG PDWKHPDWLFDO VFLHQFHV DQG QDXWLFDO VFLHQFHV %HKLQG DOO WKDW &KDOPHUV DFFRPSOLVKHV WKH DLP SHUVLVWV IRU FRQWULEXWLQJ WR D VXVWDLQDEOH IXWXUH ¤ ERWK QDWLRQDOO\ DQG JOREDOO\ 9LVLW XV RQ &KDOPHUVVH RU 1H[W 6WRS &KDOPHUV RQ IDFHERRN

210
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making Box 6.2:

Chapter 6

Janet was a quiet, self assuming individual but of high intelligence. She has a strong need for order, structure and ensuring details are recorded and identified. Whilst not the most creative person, her writing style is fluid and tidy with clear elements of control and discipline apparent. She has a strong drive for competency in her skills and in others she works with. This personality description has been chosen from a random group comprising 30 librarians and 70 TV presenters. What is the probability that Janet is a librarian or a TV presenter? If the view was taken that she was a librarian, then this answer ignores the stated fact of the base frequencies of the information presented (that this was in fact only 30% likely). It was much more likely that she was a TV presenter. The author has used variants of this question with both undergraduate and postgraduate classes with the usual result that most answers chosen are biased upon the perception of the stereotyped description. Goodwin & Wright (2004) further identify that bias in the representativeness heuristic can arise from expecting sequences of events to appear random, where rationally, none should be expected. In tossing a coin ten times, the two sequences of HHHHTTTTHH and HTTHTHTTHT are both equally likely, although the latter sequence seems more random. The primary risk for an organisation is that managers respond too abruptly to perceived patterns in, for example, sales data. We also know from chapter 2 and 3, that events such as coin tossing are independent of their previous history, so having had a sequence of ten heads being tossed still means that the next toss is equally likely to be a head or a tail. However, it is not uncommon that individuals expect chance to be self correcting (the so called Gambler’s fallacy argument, that is a gambler has had a losing streak, they are ‘due’ a winning streak ). A further bias in the representativeness heuristic is that of regression to the mean. In this instance, individuals maintain a perception that when an event follows another unlikely event, that subsequent events will continue to be unlikely. Consider for example the question posed in Goodwin & Wright (2004) – think about the likelihood of boys versus girls being born at a small versus large hospital? In particular, would a small or large hospital record more days in which more than 60% of newborns are boys? The choices available are (1):The larger hospital would see more days on average where more than 60% of newborns are boys? (2): The smaller hospital would see more days on average where more than60% of newborns are boys? (3): Both hospitals would expect on average to see about the same proportion of male and female newborns on any given day. The correct answer would be (2) – as this is the smaller sample size where you would expect to see more skewed results in a statistical distribution. The issue of an expected gender balance does not reflect the more likely statistical outcome for this sample size.

211
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 6

The third generic heuristic outlined by Good & Wright (2004) and Hardin III (1999) is that of the anchoring and adjustment heuristic. In chapters 2 and 3, we explored the quantification of event likelihood as probabilities. This developed from the rational (RAT) models with externally verified data, through to estimated probabilities in say data forecasting. It is at this stage in the management decision making process, that the risk of this heuristic being erroneously used is apparent. For example, estimating the duration of time needed to complete a task is often based upon previous experience (the anchor value) which is then changed to reflect perceived difference in this new task when compared with older completed tasks (the adjustment). However, it is common that there is insufficient adjustment of the anchor to reflect the likely (in this case time) outcome. In classes taught by the author, an interpretation of the classic Tversky & Kahneman presentation of this problem, can be used: Activity 6.1: The Anchoring and Adjustment heuristic Class individuals are presented with the following problem statement: Go to the tutor and draw a number at random from the blind bowl. Read the number and then replace it back into the bowl. Now answer the question ‘How many countries of Africa are members of the UN?’. It is very unlikely that the class student would know the answer to this specific question and by selecting a number at random, the student is presented with an anchor upon which to adjust their recorded answer. The bowl contains a large selection of number lots to draw from, but in fact only features two different numbers (in this case 10 and 45). The use of this heuristic by students then tends to result in two groups of answers clustered around the low teens with another clustered around the 40-50s. A further bias in the anchoring and adjustment heuristic, is that of overestimating the likelihood of conjunctive events. For example, the likelihood that the reader secures a first class degree and wins the lottery can result in individuals overestimating the likelihood of the second event as they have anchored that probability to the first event (Goodwin & Wright, 2004). Let’s take another hypothetical example, if we assume we have some fortune telling apparatus and from that we can deduce the following: the chance of securing a first class degree is 90%, then the chance of securing a Master’s Degree is 90%, then the chance of securing an excellent job is 90% and then the chance of securing promotion in that job within the first year is 90% - we might assume that within a few short years , to have secured a highly successful career as we have anchored on the achievement of one outcome and adjusted the likelihood of the subsequent outcome. However, when the probabilities are actually determined (recall chapter 2), the actual probability of securing these outcomes are presented in this manner is 65% (or 0.94). Similarly individuals tend to underestimate probabilities of disjunctive events occurring. For example, the likelihood that you will as a manager experience either poor sale or bad marketing press in say a week. As before, individuals will have a tendency to anchor on one of the associated events and as a result, under estimate the likelihood of the other event then occurring. The classic question posed in Good & Wright (2004: 249) is that of: “10 people work in an office and each has a 5% probability that at least one of these people will leave within the next year. Estimate the probability that at least one person will leave within the next twelve months (assume that the chances of any one person leaving is independent of whether the others stay or leave)”

212
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 6

What would your answer be? If you had said 5% you would be wrong – the actual answer is just over 50%. An incorrect answer suggests that it is anchored to the 5% and was insufficiently adjusted to reflect the actual combinations of potential leavers: i.e.: Probability of Person A OR B OR C OR D OR E OR F OR G OR H OR I OR J leaves = (10x0.05)=50% But of course you may have 2 or more people leave the office at the same time which also then needs to be determined. Hence Probability (2 or more people leave)=0.05x0.05= 0.0025, Probability (3 or more people leave)=(0.05)3 = 0.000125 …etc – hence the sum of these other probabilities would be (through to at least 9 people leaving) = 0.00262. Hence the probability of at least one person leaving is 0.5+0.00262 = 50.26%. This chapter will close with one further reference to one final important heuristic identified by Tversky & Kahnemann called – Prospect Theory. This is the observed tendency of individuals to become more risk averse when situations involving gains, but risk seeking when situations are likely to lead to losses (Chapman, 2006). In other words, If a decision is framed in terms of gains, then individuals will tend to avoid risk and protect a small gain rather than risk it for a larger gain. Equally, if a decision is framed in terms of losses, individuals will tend to gamble more to avoid a large loss, rather than accept a smaller loss. It is an important heuristic, that also manifests at the collective level in group decision making dynamics (see chapter 7).

Linköping University – innovative, highly ranked, European
Interested in Engineering and its various branches? Kickstart your career with an English-taught master’s degree.

Click here!

213
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Chapter 6

6.4	Summary
This chapter has expanded on the issues presented in chapter 5, namely the contribution of the individual to the decision making process. It has sought to present a broad overview of the issues that shape individual perception, behaviour and action when making decisions and in doing so, give to the reader a broad framework to identify potential sources of erroneous decision making outcomes. Significant emphasis has been placed on the role and function of cognitive process in the decision making process and the selection of appropriate processes to make judgements. From a managerial perspective, to mitigate the incidence of bias in individual decision making activities, individuals can practise the following: •	 Being aware of multiple cognitive issues shaping decision outcomes – and trying to encourage and adopt multiple perspectives. •	 Acting deliberately negatively towards initial decision outcomes (by acting as Devil’s Advocate) •	 Always question assumptions made in reaching a decision outcome and validate any inferences made •	 Do not neglect the unpopular decision outcome •	 Try to make incremental decisions rather than seek (and fail) to achieve optimal outcomes •	 Rely on probabilistic relationships and statistics to guide the decision making process

6.5	

Key terms and glossary

Cognitive dissonance – describes how individuals are able to hold opposing viewpoints simultaneously. Heuristic – is a cognitive process that is employed to allow decision making to occur where there is uncertainty over data and / or its interpretation. Problem space – describes the individual’s environment (incl. schemas), task and situation which give value and meaning to the decision selection. Schemas – are the mental constructs that allow interpretation to be made of an individual’s environment and support the selection and implementation of differing cognitive heuristics.

214
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 7

Chapter 7
7.1	  he role of groups in decision making and long term decision making methods T and analysis
This chapter continues with some of the issues raised in chapter 6 regarding how individuals evaluate options and apply judgement to reach a decision, but in the context of a group activity. It also considers decision making methods, which extend beyond the traditional strategic duration typical of most organisations, to consider decision making in the future. Collectively that body of work is known as ‘futures methodologies’. To begin with, we might define a group based on the ideas of Lewin, that it is comprised of individuals with some interdependency with each other (Gallos, 2006). In general, group decision making has historically been viewed from two perspectives of either viewing the group as a collection of individuals working together, or that through the group, an additional dynamic is achieved (Goncala & Duguid, 2008). In short, the early view was either to see the group as merely a collection of individuals or that there is a ‘group mind’ that emerges through the co-operation of those individuals (Allport (1924) & LeBon(1895) cited by Goncala & Duguid, 2008). These early views began to crystallize in the latter half of the twentieth century, with Lewin’s work of field theory and group dynamics and the views that there was an interdependency between a series of factors which reflect both individual, group and environmental issues.

215
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making We could further attempt to categorise these approaches into three broad threads:

Chapter 7

1.	 The first focused upon group communication methods – comprising the linear/functional theories of group decision making. These views are concerned with how groups achieve their tasks and tend to be progress and stage driven models. This has been a popular approach to understanding group decision making as it is quite prescriptive in its views of decision making. Its most famous writers are Bob Tuckman (1965) and Aubrey Fisher (1980). There have been >100 observed models of group development in the literature (as variations of the generic stage models) which normally reflect key stages described as: Formation (forming), Control (norming), Work (norming), Ending (performing). Fisher’s (1980) approach describes Decision Emergence Theory (DET) where decisions emerge from group members’ verbal interactions but still are formed within stage development. Fisher’s work posits the four stages of Orientation (clarification, orientation, roles…), Conflict (proposals criticized, openness…), Emergence (consensual compromise), Reinforcement (commitment and unity towards the decision). The weaknesses of these approaches is that they are overly descriptive . 2.	 The second are interactional theories of group decision making – where the focus is upon the interaction of the individual members in the group and how this shapes the progress towards an outcome. The concepts underpinning these ideas were broadly presented in Chapter 6 and are concerned with individual interaction in a group context (including emotions and human bias). Key insights are from George Homans and his work on group behaviour (1950) and Wilfred Bion’s (1961) psychoanalytical work on individual interaction. A key outcome of this work was the finding that the more cohesive a group is, the more valuable the social approval or activity the members exchange with one another and the greater the average frequency of interaction of members. This is important for the effectiveness of lobby groups and the more individual’s conform to emergent group norms, the more individuals feel part of the group dynamic. Additional work in this thread comes from Warren Bennis & Herbert Shepard (1956-61), who focused upon conflict and emotion in groups (dependency and independency), Edgar Schein and informal groups (1990-1993), who focused upon the role of informal groups and ‘cliques’ generally. 3.	 The third are structuration theories of group decision making – where the focus is upon the context of the individuals, the group and the problem presented and how this generates and sustains the emergent structures of decision making. Structuration approaches focus on the pre-context of individuals and background factors, the task, the group, the situation. It is an important view as it considers that reality is formed and reformed through group generative rules – much as was presented and discussed in chapter 5 with the SSM importance of a ‘worldview’. Similarly, Gilson & Shalley (2004) also identify a range of factors that shape effective group decision making. Firstly is the design of the task for the group activity, including the relevance of the task to the individual, their individual attitude towards it and their willingness to participate in that decision making process (and with, therefore, each other) and the necessity for the organization to provide a supportive climate and culture. Secondly are group characteristics – that individual members have necessary experience and knowledge to complete the task assigned to the group; that group members have a secure tenure with the organisation and that group members are willing to socialize with each other. Thirdly, if group members are given an expectation of being creative in their group decision making, then this antecedent is more likely to encourage the group to become creative. Additionally, creating an interdependency between individuals in the group is more likely to encourage new and better ways to help colleagues in the group activity. The number of staff involved should also be relatively small but also ensure sufficient knowledge and experience overlap between those involved (Howell, 2010). 216
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 7

As conceptual theory developed, so did different management methods to exploit group decision making potential. Brahm & Kleiner (1996) provide a succinct summary of some of the more important of these methods – which are summarized in table 7.1 overleaf. Groups and their use in and by organisations has become very much the norm since the late 1970s. Most important decisions tend now to be taken by groups in organisations (with some notable exceptions).
Method Description A free thinking and freedom oriented verbal interaction method, first developed by Osborn (1962). Advantages Disadvantages Ideas are generally not screened (hence useful as a precursor to other subsequent methods), requires time, space and that staff involved are of similar position. Staff can ‘claim’ ideas not wishing others to be considered / developed from them. Requires a key leader to manager this issue*. Not all staff are comfortable with verbal writing. Only useful for large groups. Its unstructured nature can limit the results generated, some staff may not be comfortable expressing views publically and having many small groups formed from a large group requires a final co-ordination which requires a facilitator to manage effectively. Requires supervisors to be trained as group leaders and that at some point they will act as gatekeepers of recommended decisions (which could deflate group morale). Requires the organisation to embrace the practice of Quality Circles. May require additional training and social skill development.

Brainstorming

Develops a large range of ideas. Informality helps creativity, it’s cheap and fast

Brainwriting

A silent method, where staff write down their ideas – which others use to develop further ideas.

Can generate more ideas that just brainstorming (but usually not of the same quality), does not need a facilitator (but see *). Supports staff who may be reluctant to articulate ideas for fear of ridicule, can work with large groups of disparate status members. Works best when the problem is clear, the group size is small and manageable, when groups are working within near proximity, work within a time limit and operate informally. Can also be used in a range of contexts (political, economic, social etc).

Buzz Sessions

A method of dividing large groups into smaller groups for the purposes of discussion.

Quality Circles

Seeks to involve employees within the decision making process (as they will be more committed to the outcome).

Involves staff on a long term basis in slowly changing groups – enhancing commitment. Helps identify relevant operational concerns with solutions, and provides some distance between the staff and their discussed problems. Higher employee cohesion.

217
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 7

Nominal Group Technique

A combinatory method of brainstorming and brainwriting.

By using written and oral input –weaknesses of other methods are mitigated. This can reduce conflict and tension in group dynamics. Increases creativity, promotes equality of participation, reducing the effect of dominant personalities and promote concensus. Has been adapted to an IT context.

Lacks flexibility – only able to resolve one problem at a time. Requires time to prepare and cannot be a spontaneous method.

Table 7.1: Comparison of major group decision making methods Sources: Braham & Kleiner (1996): Lago et al (2007)

Peniwati (2007) further presents a very detailed overview of the criteria that can be used to judge the effectiveness of group decision making methods. These are summarized into a 16 point series of criteria (table 7.2):
Group maintenance: Leadership Effectiveness Group maintenance: Learning Problem Abstraction: Scope Problem Abstraction: Development of alternatives Structure: Breadth Structure: Depth Analysis: Faithfulness of Judgements Analysis: Breadth and Depth of Analysis (what if ) Fairness: Cardinal separation of alternatives Fairness: Prioritizing of group members Fairness: Consideration of other actors and stakeholders Scientific and mathematical generality Applicability to intangibles Psychophysical applicability Applicability to conflict resolution Validity of the outcome (prediction)
Table 7.2: Peniwati’s (2007) listing of assessment criteria to judge the effectiveness of Group Decision Making Methods

The methods presented in table 7.1 however, do not consider when they should be used by an organisation. This is a key concern cited by Lowell (2010), that judgement in the use of group decision making methods is arguably not as important (as this can be learned and practiced) as being able to determine when to apply a given method. In other words, making organisational decisions when there is still some ambiguity about future issues (but not so much as to make decision making a ‘guessing’ process).

218
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 7

7.2	

Group decision making – is it really better?

Evidence of whether the group is more or less effective than individual decision making, however, is not unequivocal (Moon et al, 2003). For example, recent conceptual development of group decision making methods have highlighted a number of key issues discussed in this chapter. Firstly, Goncalo and Duguid (2008) consider the role of attribution theory as an important group dynamic which shapes how effective the group is in resolving decisions and uncertainty. Attribution theory claims that individuals are likely to take credit for their success whilst attributing failure to external circumstances (Rogof et al, 2004). This is called the self serving bias. Recent research evidence suggest that a similar attribution develops within the group dynamic of group decision making – that groups and organisations have a tendency to take credit for their success and attribute failure to their external environment (Goncalo & Duguid, 2008). It has been argued that this tendency in individuals, when operating at the group level may help to encourage divergent thinking (Goncalo & Duguid, 2008). . Divergent thinking will tend to generate a more diverse range of potential solutions to presented group problems and help mitigate the risk of groupthink (or convergent thinking). We consider groupthink as a dynamic group phenomemon shortly. The attributions of past successful decisions resolved and brought to the group discussion environment, can however have one of two outcomes. Firstly, it may lead to actual convergent thinking as group members build upon those attributed successes and narrow their range of possible options (as other options are too time consuming and risky to develop). This can lead to a continued group discussion for solutions but within a narrower and more focused theme. However, there is the counter intuitive argument that individual attributions of prior successful solutions, may actually generate a strong group dynamic to identify group solutions. Ongoing research is focused upon identifying those factors which can result in this group outcome (Goncalo & Duguid, 2008).

219
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Chapter 7

Other issues that seem to shape group effectiveness when compared with individual effectiveness in decision making, is the prior exposure of individuals to the aim of the group task (Moon et al, 2003). For example, recent research suggests that groups tasked with being creative, perform more effectively when they are expected to be creative (Gilson & Shalley, 2004).

7.3	

Group communication

It was noted in Chapter 6, that communication between individuals is structured by rigidities between individuals. One well known framework to understand this is the so called Johari window (Luft ,1969) . Perhaps most famously, the speech given by US Defense Secretary Donald Rumsfeld of February 12th, 2002 is illustrative of this: “As we know, there are known knowns, there are things we know we know. We also know, there are known unknowns, that is to say - we know there are some things we do not know. But there are also unknown unknowns, the ones we don’t know, we don’t know.”

/ŬŶŽǁ zŽƵŬŶŽǁ

/ĚŽŶ͛ƚŬŶŽǁ zŽƵŬŶŽǁ

/ŬŶŽǁ zŽƵĚŽŶ͛ƚŬŶŽǁ

/ĚŽŶ͛ƚŬŶŽǁ zŽƵĚŽŶ͛ƚŬŶŽǁ

Figure 7.1 The Johari window and the action of group communication. Figure 7.1; The Johari Window

220
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 7

The Johari window has proven to be a useful instrument to understand communication between individuals. Much has been written on this model, although key issues relate to the level of information disclosure given to others unknown to us upon their meeting (the top left hand corner). As disclosure of information and understanding takes place much may be revealed by observing the reactions of other (top right hand corner or the bottom left hand corner). Of course there may also be uncertainty regarding what is known to all those involved in a group communication (the bottom right hand quadrant and obliquely referred to by the Rumsfeld quotation). In general, following the discussion in chapter 6 on psychoanalysis, disclosure is perceived to be a healthy cognitive pursuit, although we also noticed in chapter 5 and chapter 6, that disclosure of information can result in emergent dependencies with and to others, which can negatively affect decision outcomes. The model can also be used dynamically, where multiple windows interact as individuals work together in a group activity. Friedman (2004) further identifies that many organisational difficulties do emerge through manager misperception of a causal relationship in a problem, then acting to correct that problem and therefore creating unintended consequences in the organisation. Friedman (2004) further cites that there is a perseverance of belief here, whereby managers act rigidly preferring to use familiar solutions to problems, rather than explore the unknowns in a problem through communication (such as proposed by the Johari window).

7.4	

Convergent thinking emergence in groups- The Abilene Paradox

A concern with the effectiveness of group decision making and in particular how the group can generate, under certain circumstances, flawed decision outcomes, can be sourced partly to the decline of the protestant work ethic, but also as we have discussed to the range of individual concerns and biases discussed in chapter 6. However, there are additional difficulties that can emerge, in certain contexts (and groupthink is one potential outcome that can arise from some situational contexts and it should not be thought of as the most likely defect of group decision making), when individuals work in groups to resolve a problem. Consider for example the case of the ‘Abilene Paradox’ (Harvey, 1998: 2008) (Box 7.1). Box 7.1: “On a hot afternoon visiting in Coleman, Texas, the family is comfortably playing dominoes on a porch, until the father-in-law suggests that they take a trip to Abilene [53 miles north] for dinner. The wife says, “Sounds like a great idea.” The husband, despite having reservations because the drive is long and hot, thinks that his preferences must be out-of-step with the group and says, “Sounds good to me. I just hope your mother wants to go.” The mother-in-law then says, “Of course I want to go. I haven’t been to Abilene in a long time.” The drive is hot, dusty, and long. When they arrive at the cafeteria, the food is as bad as the drive. They arrive back home four hours later, exhausted. One of them dishonestly says, “It was a great trip, wasn’t it?” The mother-in-law says that, actually, she would rather have stayed home, but went along since the other three were so enthusiastic. The husband says, “I wasn’t delighted to be doing what we were doing. I only went to satisfy the rest of you.” The wife says, “I just went along to keep you happy. I would have had to be crazy to want to go out in the heat like that.” The father-in-law then says that he only suggested it because he thought the others might be bored.

221
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 7

The group sits back, perplexed that they together decided to take a trip which none of them wanted. They each would have preferred to sit comfortably, but did not admit to it when they still had time to enjoy the afternoon.” This is a famous example of another failure of group decision making which is related to, but not the same as groupthink (discussed shortly). In this case, this is an example of a non conflict oriented problems where organisations can take decisions seemingly in contrast to what they are actually aiming to do. In short, as Harvey (1988) states there is an organisational inability to manage agreement. This is evidenced by a number of organisational symptoms: 1)	 Members in the group activity agree as individuals, with an issue or experience facing them 2.	 Members in the group activity agree as individuals, what would need to be undertaken to address the issue or experience facing them 3.	 Members of the group activity as individuals, fail to communicate effectively with each other regarding the issue or experience facing them – and in doing so, a misperceived construct of the issue or experience facing them is generated. 4.	 Members of the group activity perpetuate and sustain the misperceived constructs leading them to take actions which individually contradict their views of the issue or experience facing them, but collectively seem to support the emergent group concensus. 5)	 Upon realization that an action was not undertaken for either individual or organisational benefit, anger and dissatisfaction arise and which can reoccur if the issues concerning the poor communication, are not addressed by the group.

26 destinations 4 continents
Bartending is your ticket to the world

GET STARTED

222
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Chapter 7

Harvey (1988) identifies a number of individual psychological drivers and factors from the group dynamic for this paradox (the occurrence of an event which seems to contradict expected logic). 1)	 Action anxiety – that individuals feel anxiety as they prepare to take actions to initiate what they feel needs to be done in a given situation. In some situations then this can result in individuals choosing to not act in this manner, but instead accept alternative (and inferior) actions to what they believe. Action anxiety arises therefore from: a)	 Negative fantasies – that confront the issue with the perceived correct course action will surface other difficulties that need resolution and which are perceived to be equally, if not more so, unpalatable. Negative fantasies arise from: •	 Real risks – that individuals perceive the impact of negative fantasies will result in personal injury or loss and is to be avoided. •	 Fear of separation – that individuals who choose to state what they believe to be right in a given situation, have the real risk of being perceived by others as different and ‘not a team player – thus they could be alienated or separated from the group. b)	 Risk reversal – by following the actions in (1,a, I and ii above), individuals do in fact run the high risk of being separated from the group (and which Harvey (1988) identifies as the paradox within the paradox). This is believed to occur as organisations (and people therein), by virtue of the lack of stressed emotion (chapter 6) as valuable to the operation of the organisation, find it very difficult to connect with other parts (people) of the organisation. Instead, when the ‘Abilene paradox’ manifests in organisation, stress, anger and irritation with colleagues occurs. To address the paradox in organisations, Harvey (1988) recommends that there is a significant culture change required where highly charged language is negated (and in doing so, the perceived existential risk associated with fear fantasies is also negated). So words and supportive actions such as victim, collusion, conflict, confrontation, reality and knowledge are redeveloped and redefined so that effective and open communication is developed. For example Schulz –Hardt et al (2002) stress the importance of viewing conflict as a productive factor in organisations, to mitigate biased information seeking in group decision making.

7.5	

Convergent thinking emergence in groups- The Groupthink phenomenon

Group decision making defects can also arise through other defects in the group dynamic. Perhaps the most famous example of this comes from Janis (1982) with ‘Groupthink’. In this particular context, the defect in the decision making process arises when concurrence thinking between group members arises, before a problem or solution has been sufficiently analysed or evaluated (Chapman, 2006). Groupthink defects do not arise with all group decision making activities, just as the previously discussed ‘Abilene Paradox’ management outcomes, can also arise for certain organisational contexts. For Janis (1972 cited by Chapman, 2006), groupthink defects emerge when group members are operating in provocative situations with high levels of moral dilemmas or risks of materials losses. Group members strive for emotion agreement between them, which leads to concurrent thinking and high levels of cohesiveness between those group members. This though, is not a sufficient context for the emergence of groupthink – as in essence all groups reaching meaningful decision outcomes, must have reached some form of cohesiveness.

223
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 7

One difference in the working of groups, comes from the decline in importance of the protestant work ethic, which has given way to a social ethic valuing social interdependency and interaction. This lays the foundation for so called ‘risky shift’ of the group through ‘cultural amplifiers’. Risky shift is the group dynamic phenomenon whereby the group is willing to accept decision solutions that embrace higher levels of risk that might otherwise be supported by individuals themselves in those groups. It has a number of identified key contributors including (Freedheim, 2003): •	 Familiarisation theory – where as the group discusses an issue, the increased familiarity of the issue to the group, reduces the uncertainty associated with the issue and increases scope for embracing more radical solutions decisions. •	 Leadership hypothesis – proposes that leaders will emerge in group decision making who are more familiar and knowledgeable towards the task, have greater confidence and assertiveness in themselves and in it. They are as a result, more comfortable with risk . •	 The diffusion of responsibility hypothesis – proposes that individuals in groups are less averse to risky solutions as they feel less responsible for their outcome adoption Janis proposed a range of group antecedent factors (presented in table 7.3 below) that are likely to lead to high levels of concurrency in the thinking of group members and which then might result in the problems highlighted above.
Anxiety defence mode and groupthink symptom Control – illusion of invulnerability

Defence mechanism

Purpose as response to anxiety Keeping control of people and events

Control compensation

Addressing weaknesses in one area by attending to areas of greater strength. Selection, manipulation and explanation of the facts to allay fears . Not seeing what one does not wish to see. Mutual reassurance of support and agreement Doubts pushed out of conscious thought Feelings of concern or dread not acknowledge by self: others discouraged from expressing them as a means of protecting the group Others dissuaded from expressing doubts that might upset the status quo Inability or refusal to question the moral position of the group or to acknowledge other values or positions as a means of avoiding value conflict Rather than acknowledging own fears of responsibilities, attributing these to others, often in a derogatory manner that hints of outgroup inferiority or weakness

Collective rationlisation Illusion of unanimity Denial – self censorship Self appointed mindguards

Rationalisation Fantasy Repression Denial Control

Pressure on dissenters Escape – belief in the morality of the group

Suppression control

Regression

Stereotyping of outgroups

Projection displacement

Table 7.3 – Symptoms of Groupthink and anxiety associations Source: Chapman (2006): Manz & Neck (1997)

224
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 7

For an excellent contemporary discussion of the emergence of groupthink due to the convergence of these antecedent factors, Badie (2010) has analysed the US policy and political position with regards to the conflict with Iraq and the ‘War on Terror’. Chapman (2006) outlines two types of Groupthink phenomenon – Type 1 which is a pessimistic view about the possibility for the tasked group to resolve the problem and arises from ‘collective avoidance’ by the group (stressed induced to avoid failure). Type 2 groupthink is the opposite of this, that the group adopts an overly optimistic view of their ability to solve the tasked problem. It has been argued (see Chapman, 2006 for a full discussion) that prospect theory (chapter 6) plays a key role in shaping risk perceptions through expected losses and gains that determine judgement. Both contexts can generate the symptoms and flawed outcomes from groupthink. However, there remains some discussion of the full veracity of the original Janis groupthink framework. Koerber & Neck (2003) for example discuss the Whyte model of the original Janis Groupthink phenomenon, which seeks to replace the argument that group cohesiveness shapes the emergence of groupthink, with an argument of collective efficacy as a necessary but not sufficient antecedent condition. This is a type 2 argument, where the decision makers in the group have a collective belief about the ability of the group to resolve the tasked problem. In addition, Chapman (2006) also proposes that anxiety between group members tends to result in higher levels of concurrency between group members. Anxiety is a stress induced emotion that arises when individuals anticipate exposure to danger or risk, and is similar to fear but focused upon a future context, rather than an immediate one. This negative emotion has a detrimental impact upon the individual’s capacity to make informed and appropriate decisions. This has been labeled as Decision Conflict Theory (Janis and Mann (1977) cited by Chapman, 2006), where stress negatively affects information processing of individuals. Similarly the Mood Maintenance Hypothesis (Isen & Patrick(1983) cited by Chapman (2006)) argues that decision makers in a positive mood tend to be risk averse and vice versa. The desire of decision makers to reduce their anxiety triggers inappropriate cognitive processes and in conjunction with others, flawed decision outcomes. Much as with the discussion on addressing the Abilene Paradox, to avoid this problem, managers must find ways to surface negative emotions – which as has been already noted, was stressed as an important aim for contemporary organisations to address. Other advice for mitigating the risks of groupthink emerging are to within groups (Manz & Neck, 1996): •	 Encourage divergent views within the group •	 Support the open expression of concerns and ideas •	 Developing an awareness of limitations and threats •	 Recognising each group member’s unique value •	 Recognition of the importance of views from outside the group •	 A frequent discussion of collective doubts •	 Pursuing the adoption of non stereotypical views •	 Recognition of the ethical and moral consequences of decisions

225
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 7

One proposed group decision making methodology which seeks to recognize both individual decision making factors and the role and function of the group dynamic that emerges when individuals work collectively on decisions is the Motivated Information Process in Groups model (MIP-G) model (De Dreu & Beersma, 2010). There is a recognition and acceptance that individuals make decisions with varying cognitive structures and procedures and in a similar fashion, the group dynamic also reflects shared antecedent and cognitive structures (such as ‘majority voting’). However, the MIP-G model also states that when group members have low confidence in the correctness of their understanding of a problem, they will initiate systemic information processing. This is called epistemic motivation. The reverse is also claimed to be valid, that when group members do have confidence in their understanding and ability to resolve a tasked problem, they are more likely to use familiar schemata and heuristics to resolve the situation. In the MIP-G model, co-operative motivation amongst group members supports epistemic motivation and deeper level thinking and a more useful range of decision outcomes.

7.6	

Futures forecasting and decision making

The last section of this chapter is to introduce briefly some additional concepts that are not usually found in management decision making texts, but which offer a range of additional insights for the manager seeking to make effective decisions in the long term. This includes a body of work called ‘Futures Methodologies’, and which are in essence concerned with supporting decision making significantly into the future (i.e. beyond the range of what may normally be the concern of a strategic decision maker).

.

226
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

Chapter 7

The first of these reconsiders an issue presented in chapter 2 regarding data forecasting. It was noted in that chapter that some management data can exhibit seasonal variations, but the driver for those motivations was not examined. We can now consider this more broadly as a business ‘cycle’. These are combined pressures and forces that have (varying) periodicity in the economy. A number of them have been identified as: •	 The Joseph Kitchin cycle (1861-1932) – concerned with the acquisition and development of organisational inventory (3-4 years and is otherwise called the well known ‘boom / bust’ cycle). It is believed that information lacks and the diffusion of it, also contributes to this business cycle. •	 The Clement Juglar cycle (1819-1905) – concerned with the increase and decrease in fixed investment in plant equipment and machinery (and argued to persist over a 7-11 year period) •	 The Simon (Smith) Kuznets cycle (1901-1985) – is concerned with the migration and investment in construction and capital build projects (and is argued to persist over a 15-25 year period). His work laid the foundation for development economics and revealed problems with Keynesian economics in the long term. •	 The (Nikolai) Kondratieff cycle (or Long Wave Cycle or K wave) –is concerned with significant structural change in an economy, such as that brought about by new innovations diffusing through society with strong positive externalities (this is argued to persist over a 48-60 year period). They have long term changes for the fabric of society, although clear empirical evidence remains unclear and is not widely accepted as an economic trend. •	 There are also been identified innovation cycles, such as Gerhard Mench’s metamorphosis theory (1979) – whereby an economic recession and depression trigger growth (argued to be undertaken over an18 year cycle and which focus upon the exhaustion of a given innovation in society, which is then replaced by another innovation(s)). He views economic development as occurring in S shaped pulses rather than long K waves. So there is stable growth followed by periods of instability, uncertainty and crisis. Utterback & Abernathy’s (1974) theories on innovation processes and their rates of change. After Kitchin’s well known boom and bust cycle, the next most familiar of these cycles is that of the Russian Economist - Nikolai Kondratiev. He was required to investigate the Great Depression in the capitalist economy by Joseph Stalin and prove it would be the fall of that method of market management. By considering the rise and fall in identified key commodity prices over a several hundred years, he did manage to prove that the Depression would be a significant negative factor for capitalism, but also that it would recover from this. Unhappily, this news was not greeted positively by Stalin and Kondratiev was fired from his posting and ultimately, dispatched to Siberia. If a manager was aware of their economic environment’s ‘position’ in the ‘cycle’ - it would provide indicative strategic decision making guidance. For example, for the Kitchin cycle, on the upswing (application side) the manager can anticipate changes in capacity and could change company gearing, review inventory levels and assess market share development. There would also be scope for opportunities for the service of those innovative industries. At the peak of the upswing in the cycle, the changes in productive capacity slow, there is a change corporate gearing, a need to reduce inventory and consolidate. As the ‘wave breaks’ and the economy enters the downswing (learning is exhausted and new innovations cluster), the manager needs to avoid sensitive markets, organise for low growth, expect the impact of new technology to become evident and there should be a corporate priority to maintain high liquidity.

227
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 7

Other futures forecasting methodologies include the Delphi method. This is concerned with using the views of acknowledges experts to consider the impact of future trends and developments in an economy, to anticipate long term investment for organisations and governments. The methodology involves identifying participants (say between 5-20 people) who then receive the circulated subject of study. The facilitator then receives comments on the perceived importance of identified factors and from these constructs a questionnaire, which is then recirculated and returned (all undertaken anonymously). The areas of analysis of areas of convergence and divergence are noted and the questionnaire is reconfigured focusing upon that convergence (or it could also be divergence) and again it is recirculated. This is repeated until general consensus is obtained (typically involving anything between 2-10 rounds). The method is therefore one of successive iterations of a constructed and reconstructed questionnaire. The primary motive is to seek to find concensus amongst a group of experts which is likely to be better than any single individual opinion. There are also variations on this methodology (where the search can be to identify consistent divergence in views). Delphi is an effective futures forecasting methodology when problems are not clearly analytical – but subjective, which require contributions from many diverse and / or difficult to ‘get together’ individuals, when many individuals are required and resource constraints limit group meetings. There may also be resistance to face-to-face meetings between some experts which this method avoids.

ƌĂŝŶƐƚŽƌŵŝŶŐ

EŽŵŝŶĂů'ƌŽƵƉ dĞĐŚŶŝƋƵĞ

&ŽĐƵƐ'ƌŽƵƉ;ƐͿ

,/', /ŶĨŽƌŵĂŶƚƚŽ ŝŶĨŽƌŵĂƚŝŽŶ ĐŽŵŵƵŶŝĐĂƚŝŽŶ

ůĞĐƚƌŽŶŝĐ ďƌĂŝŶƐƚŽƌŵŝŶŐ

ůĞĐƚƌŽŶŝĐEŽŵŝŶĂů 'ƌŽƵƉdĞĐŚŶŝƋƵĞ ǆƚĞŶĚĞĚĞůƉŚŝ ^ƚƵĚǇ

/dďĂƐĞĚ ĐŽŶĨĞƌĞŶĐŝŶŐ

'ƌŽƵƉ/ŶƚĞƌǀŝĞǁ

dƌĂĚŝƚŝŽŶĂůĞůƉŚŝ ^ƚƵĚǇ

>Kt ,/', ZĞƐĞĂƌĐŚĞƌͲ/ŶĨŽƌŵĂŶƚŽŵŵƵŶŝĐĂƚŝŽŶ
Figure 7.2 The Scope of Group Decision Making Methods Source: Adapted from Day & Bobeva (2005)

Day & Bobeva (2005) view the Delphi method and other discussed group decision making methods as noted in Figure 7.2, which can also be undertaken using web based media too. Activity 7.1 presents a thought activity to explore group decision making methods:

228
Download free eBooks at bookboon.com

Effective Management Decision Making Activity 7.1: Selecting a group decision making method

Chapter 7

For each of the following situations recommend a group decision making method and discuss the advantages and disadvantages of that method, for the present situation. 1)	 The Board of a large organisation wish to indentify long term future trends (circa 20 years beyond the present time) in the development of the market(s) for their products. 2)	 The Board has been convened to discuss a pressing new competitor, who has launched a new product into the market. The Board must decide who to invite to an organisational meeting to review implications of this competitive change and how to manage that meeting. Long term forecasting, often called ‘foresight analysis’, has been undertaken for a period for a period of time by both governments and organisations. It was the Japanese government which published the first of their Delphi foresight initiatives in 1971, which is repeated every five years (Cuhls, 2001). It was adopted by many other governments in the 1990s as a way to guide strategic priorities for investment and development (Anderson, 1994). Daheim & Uerz (2006) further discuss the increase in importance of a corporate engagement with these futures methodologies, with a German corporate lead (being taken up by Siemens, Thyssen-Krupp and BASF). In their European survey of organisations which had implement corporate foresight (CF) initiatives, the median age of practice was 10 years and the role of external consultants in the process was in general a necessary, but not sufficient condition for the implementation of CF. Four types of CF are observed in practice with the European sampled firms of: 1)	 ‘The Collecting Post’ – a practice which views CF as a peripheral activity, undertaken by individuals periodically within the organisation. 2)	 ‘The Observatory’ – a practice which views the CF activity as a specialized task, performed by dedicated staff who are effectively networked in the organisation. 3)	 ‘The Think Tank’ – a practice which views CF as an organisational wide activity, with multiple aims and goals. 4)	 ‘The Outsourcer’ – a practice which views CF as an organisational activity involving external consultants, within a changeable project team and given high visibility. For the sampled organisations, the primary aims of the CF activities were identified as supporting strategic decision making, aiding the long term planning of the organisation, developing an ‘early warning system’ of potential difficulties within the organisation and improving the innovation process within the organisation. To close this chapter, we can also acknowledge more unexpected (and controversial) forecasting methodologies such as Science fiction – which is a form of story-telling – and which presents a sequence of events that might result in a given scenario. Quite a few scientists write science fiction (hence drawing upon their knowledge – such as Alastair Reynolds (ex-CERN) although the most famous is Arthur C. Clarke and his prediction of geostationary artificial satellites. Such work is both mind expanding and mindset changing. A more detailed discussion of this area of decision making requires greater development than this small section permits. For the interested reader, further more detailed narrative on futures methodology is provided by Tonn (2003) and Porter (2004).

229
Download free eBooks at bookboon.com

Effective Management Decision Making

Chapter 7

7.7	Summary
This chapter has presented a view of group decision making from the perspective of chapter 6 – that individuals exhibit key cognitive processes and adopt specific mental modes, plus a group dynamic can emerge under certain circumstances, that further misdirects the efforts of effective group decision making. This primarily focuses upon difficulties in both management agreement and disagreement amongst decision makers (as Type I and Type 2 groupthink). Key proposals for responding to and managing these issues were presented in the discussion. Finally, the chapter closed by identifying, for further reading, futures forecasting methodologies and their increased uptake across both international governments but also large corporate organisations.

7.8	Glossary
Delphi method – this is an early method of data and futures forecasting developed by the RAND corporation in 1953. It essentially considers that iterative sampling of expert views can help understand the development of a given problem (or industry for example). It is named after the Greek mythological ‘Oracle of Delphi’. Self Serving Bias – the tendency of a group decision making unit to perceive the success of that group as derived from the group effort rather than an external event(s). Groupthink – the tendency of a group decision making unit to evidence enhanced comfort with risk and risk based decisions and focus upon dominant values and leadership traits than might otherwise be expected from an individual making similar decisions. Key contributing factors are identified as: familiarization theory, leadership hypothesis and the diffusion of responsibility hypothesis. Social conformity – the phenomenon whereby individuals when acting as a group can fear the articulation of apparently dissenting viewpoints as the consequences of that dissension are viewed as more negative than the acceptance of a group activity or decision (even when the outcome is not individually desired). Motivated Information Process in Groups (MIP-G) – described decision making processes in groups which accounts for individual cognitive function, task and situational specificity.

230
Download free eBooks at bookboon.com

Effective Management Decision Making

References

References
1.	 Aaker D,(2009),’Aggressive recession marketing: when does it make sense?’, Marketing News, 30th August 2009, pp.11 2.	 Ahmed P and Shepherd C, (2010) Innovation management : context, strategies, systems and processes’, FTPrentice Hall. 3.	 Anderson D.R., Sweeney D.J. and Williams T.A (1998), Quantitative Methods for Business, 7th edition, Thomson, South Western. 4.	 Anderson D.R., Sweeney D.J., Williams T.A. and Wisniewski M., (2009), An introduction to management science, Cengage Learning, South-Western. 5.	 Anderson, A.R. & Warren. L., 2005, ‘Playing the fool? An aesthetic performance of entrepreneurial identity’, in Steyaert, C. & Hjorth D., (Ed), The politics and aesthetics of entrepreneurship, Edward Elgar, Cheltenham, 148-161. 6.	 Anderson, J (1994),Foresight analysis in research funding agencies: A UK experiment’, Higher Education, 28,pp.39-57 7.	 Assakul P (2003),’Future Studies Methods’, Accessed at http://www.ifm.eng.cam.ac.uk/cig/ documents/030115studymethod.pdf December 2011 8.	 Aston H (2008),’Architects of Alcatel-Lucent merger resign’, Sunday Times , July 29, accessed at http:// business.timesonline.co.uk/tol/business/industry_sectors/telecoms/article4421093.ece July 1st 2011. 9.	 Badie D,(2010),Groupthink, Iraq and the War on Terror, Foreign Policy Analysis, 6,pp.277-296

Think Umeå. Get a Master’s degree!
• modern campus • world class research • 31 000 students • top class teachers • ranked nr 1 by international students Master’s programmes: • Architecture • Industrial Design • Science • Engineering

Sweden www.teknat.umu.se/english

231
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

References

10.	Barney J.B. and Clark D.N.,(2007),Resource Based Theory : Creating and Sustaining Competitive Advantage’, Oxford, OUP. 11.	Baron, J. (2004). Normative models of judgment and decision making. In D. J. Koehler 12.	BBC, 1999, Consider the famous case of ‘The Midas Formula’ (Real Options and Black Scholes Formula)’See a discussion of this here: http://fisher.osu.edu/~butler_267/DAPresent/SanAntonio/MC01-1.pdf (Checked Jan 2010) and here http://www.bbc.co.uk/science/horizon/1999/midas.shtml (Checked January 2010) 13.	Beer S (1979) , The Heart of Enterprise; John Wiley, London and New York. 14.	Berne E (1996), Games people play- the psychology of human relations, Grove Press (reprint). 15.	Berry BJL, Kim H and Kim H-M (1994),’Innovation diffusion and long waves’, Technological forecasting and social change’, 46,pp.289-293 16.	Bing, Wang and Geoff Dyer (2008),’Nuclear pressure groups learn to tread carefully’, Financial Times, July 22nd, page 9 17.	Boarder J (2001), Soft Systems Structures, paper presented at the 2001 Spring Symposium and Tutorials of INCOSE 18.	Bogacz R, Brown E, Moehlis J, Holmes P, and Cohen J D (2006),’ The Physics of Optimal Decision Making: A Formal Analysis of Models of Performance in Two-Alternative Forced-Choice Tasks’, Psychological Review, 113,4,pp.700-765 19.	Brady, F.N.,(2002),Lining Up for Star-Wars Tickets: Some Ruminations On Ethics and Economics Based on An Internet Study of Behavior in Queues, Journal of Business Ethics, Part 2, Vol. 38 Issue 1/2, p157-165 20.	Brahm C and Kleiner B.H.,(1996),Advantages and disadvantages of group decision making approaches, Team Performance Management: An International Journal, 2,1,pp.30-35 21.	Brocklesby J,(1994),’Using soft systems methodology to identify competence requirements in HRM’, International Journal of Manpower, 16,5/6 22.	Burkeman O (2011),’ We’re beautiful devices’, The Guardian, 15/11/11, G2, pages 6-9 23.	Cameron A.C., (2009), Excel 2007 Multiple Regression, Accessed at http://cameron.econ.ucdavis.edu/excel/ ex61multipleregression.html December 2011. 24.	Carland et al (2000),’ The Indefatigible Entrepreneur: A study of the dispositions of multiple venture founders’, Access at http://www.sbaer.uca.edu/research/asbe/2000/21.pdf July 2011 25.	Carr, A, (2002),Jung, archetypes and mirroring in organisational change management, Journal of Organisational Change Management, 15, 5,pp.477-489 26.	Chapman J (2006),Anxiety and defective decision making: an elaboration of the groupthink model, Management Decision, 44,10,pp.1391-1404 27.	Checkland, P (1985),Achieving desirable and feasible change: An application of Soft Systems Methodology’, J.Opl.Res.Soc, 36,9,pp.821-831 28.	Checkland P, (1990), Systems think, systems practice: A 30 year retrospective, Wiley. 29.	Cohen, M D, March J G & Olson J P (1972), A Garbage can model of organisational choice’, Administrative Science Quarterly, 17,1, ,pp.1-25 30.	Costley, C. and Armsby, P.,(2007),Methodologies for undergraduates doing practitioner investigations at work, Journal of Workplace Learning, 19,3,pp.131-145 31.	Cuhls, K (2001),’Foresight with Delphi Studies in Japan’, Technology Analysis and Strategic Management, 13,4. 32.	Cushman M and Venters W (2004),’Making Sense of Rich Pictures’, accessed at : http://eprints.lse. ac.uk/8073/1/Making_sense_of_rich_pictures.pdf Jan 2011 232
Download free eBooks at bookboon.com

Effective Management Decision Making

References

33.	Daheim C. and Uerz G (2006),Corporate foresight in Europe – ready for the next step?, Paper delivered at the Second International Seville Seminar on Future-Oriented Technology Analysis: Impact of FTA Approaches on Policy and Decision-Making – Seville 28-29 September 2006, Accessed at http://www.zpunkt.de/fileadmin/be_user/D_Publikationen/D_Arbeitspapiere/Corporate_Foresight_in_Europe.pdf January 2012 34.	Davern, Finian (2004),Travel Trade Gazette UK & Ireland, Issue 2642, p10 35.	Day J and Bobeva M (2005),’A generic toolkit for the successful management of Delphi studies’, eJournal of Business Research Methods (eJBRM) ,3,2,pp.103-116 And accessed at http://www.ejbrm.com/volume3/issue2 December 2011 36.	De Dreu C and Beersma B (2010), Team confidence, motivated information processing and dynamic group decision making, European Journal of Social Pscyhology, 40,pp.1110-1119 37.	Devezas T C, Linstone H A and Humberto J S S (2005),’ The growth dynamics of the internet and the long wave theory’, Technological forecasting and social change, 72,8,pp.913-935 38.	Eeckhout J (2004), Gibrat’s Law for (All) cities, American Economic Review, Vol. 94, No.5, accessed at http:// www.ssc.upenn.edu/~eeckhout/research/gibrat.pdf July 2011. 39.	Elkins C (1976) ‘The Social Functions of Science Fiction: Some Notes on Methodology’ – accessed at http:// www.eric.ed.gov/ December 2011 40.	Escoubes F (1999), ‘A Framework for managing environmental strategy’, 10,2, Business Strategy Review,10,2, pp.61-66 41.	Espejo R., Bowling D and Hoverstadt P.(1999),The Viable System Model and the Viplan Software, Kybernetes, 28,7,pp.661-678 42.	Flood R L (1999), Rethinking The fifth discipline: learning within the unknowable, Routledge. 43.	Freedheim D.K. (2003),’History of Psychology: Volume 1’, John Wiley and Sons. 44.	Freeman C.,(1984) Strategic Management: A Stakeholder Approach, Pitman,Boston. 45.	Friedman S (2004), Learning to make more effective decisions: changing beliefs as a prelude to action,The Learning organisation, 11,2,pp.110 - 128 46.	Fuller J.A. and Mansour A.H.,(2003),’Operations management and operations research: a historical and relational perspective’, Management Decision, 41,4,pp.422-426 47.	Gallos, J.V.,(2006),Organisation Development: A Jossey-Bass reader 48.	Gigerenzer G (1999),’Better the Devil you know’, New Scientist, 4th September, 1999 49.	Gilson L.L. and Shalley C.E.(2004), ’A little creativity goes a long way: An examination of Teams’ engagement in creative processes’, Journal of Management, 30.p.453 50.	Gold, J (2001),’ Storying Systems: Managing Everyday Flux Using Mode 2 Soft Systems Methodology’, Systemic Practice and Action Research, Vol. 14, No. 5,pp.557-573 51.	Goncalo J.A. and Duguid M.M.,(2008), Hidden consequences of the group serving bias : Causal attributions and the quality of group decision making, Organisational Behaviour and Human Decision Processes, 107, pp. 219-233 52.	Groot B & Franses P H (2008),’Stability through cycles’, Technological forecasting and social change, 75,3,pp.301311 53.	Gross, R., 1996,Psychology: The Science of Mind and Behaviour, Hodder Arnold. 54.	Hardin W III (1999),Behavioural research into heuristics and bias as an academic pursuit, Journal of Property Investment and Finance, 17,4,pp.333-352

233
Download free eBooks at bookboon.com

Effective Management Decision Making 55.	Harrison E F (1999),’ The Managerial Decision Making Process’, 5th Edition, Houghton-Mufflin.

References

56.	Harvey J.B.(1998), The Abilene Paradox – the Management of Agreement, Organisational Dynamics, Summer 1988, pp. 17–43 57.	Harvey, J.B.(2008), The Abilene Paradox, accessed at http://www.abileneparadox.com/ December 2011 58.	Hase, Steward; Alan Davies, Bob Dick (1999). The Johari Window and the Dark Side of Organisations. Southern Cross University. 59.	Hill, Andrew (2011),’ The Hands-on manager trying to revive a struggling giant’, Financial Times, April 12, page 14. 60.	Hollinger P, Parker A and Taylor P (2007),’Pressure rises for Alcatel overhaul’, Financial Times (London 2nd edition), 5th October, pp.26 61.	Howell, B.,(2010),Dynamic Management: Better decisions in uncertain times, McKinsey Quarterly, 1. 62.	Janis I.L.,(1982), Groupthink, 2nd edition, Houghton-Mifflin, Boston, MA. 63.	Jennings D. and Wattam S.,(1998), “Decision Making: An integrated Approach’, FT Prentice Hall 64.	Kersten G E (1999),’Learning organisations in the 5th Long Wave: Management, innovation, knowledge and IT’., International Institute for Applied Systems Analysis “, Austria and Concordia University, Montreal, Canada 65.	Kets de Vries, M and Miller D, (1991),Leadership styles and organisational cultures: the shaping of neurotic organisations, in Kets de Vries, M.(Ed.), Organisations on the couch’, pp.243-263, Jossey-Bass, California. 66.	Knight, F (1921), Risk, Uncertainty and Profit, New York: Houghton Mifflin. 67.	Koerber C.P. and Neck C.P.,(2003),Groupthink and sports: an application of Whyte’s model, International Journal of Contemporary Hospitality Management, 15,1,pp.20-28 68.	Kwak Y H and Ingall L (2007), Exploring monte-carlo simulation applications for project management’, Risk Management, 9, 44 – 57

How could you take your studies to new heights?
By thinking about things that nobody has ever thought about before By writing a dissertation about the highest building on earth With an internship about natural hazards at popular tourist destinations By discussing with doctors, engineers and seismologists By all of the above

From climate change to space travel – as one of the leading reinsurers, we examine risks of all kinds and insure against them. Learn with us how you can drive projects of global significance forwards. Profit from the know-how and network of our staff. Lay the foundation stone for your professional career, while still at university. Find out how you can get involved at Munich Re as a student at munichre.com/career.

234
Download free eBooks at bookboon.com

Click on the ad to read more

Effective Management Decision Making

References

69.	Lago, P.P., Beruvides M.G., Jian, J-Y., Canto, A.M., Sandoval, A. and Taraban, R.,(2007), Structuring Group decisiobn making in a web based environment by using the nominal group technique, Computers and Industrial Engineering, 52,pp.277-295 70.	Langley A, Mintzberg H, Picher P , Posada P and Saint-Macary J (1995),’Opening up Decision Making – the view from the Black Stool’, Organisation Science, 6,3,pp.260-279 71.	Lee M D and Cummins T D R (2004),’Evidence accumulation in decision making: Unifying the ‘take the best’ and the ‘rational’ models’, Psychonomic Bulletin & Review, 11,2,pp.343-352 72.	Levi M (1997),’A model, a method and a map: rational choice in comparative and historical analysis’, Lichbach M.I. and Zuckerman A.S.,(Eds).,’Comparative Politics: rationality, culture and structure’, Cambridge University Press.,pp.19-40 73.	Lewin D, Mitchell, O.S. and Sherer,P.D. (1990), Research frontiers in industrial relations and human resources. 74.	Lewis M (2011),’ The King of Human Error’, Vanity Fair, December, pp.120-128. 75.	Lindblom, C.E. (1959), The Science of Muddling Through, Public Administration Review, 1959, 19, pp. 79-99. 76.	Lotfi V and Pegels C.C., (1996), Decision Support Systems for Operations Management and Management Science, 3rd Edition, Irwin. 77.	Luft, Joseph (1969). “Of Human Interaction,” Palo Alto, CA:National Press, 78.	Lumsdaine, E.; Lumsdaine, M.; (1994), Creative Problem Solving, Potentials, IEEE ,Issue Date: Dec 1994/Jan 1995 ,Volume: 13 Issue:5 ,On page(s): 4 - 9 79.	MacGrath & MacMillan (2000) 80.	Manz C.C. and Neck C.P.(1996),Teamthink: beyond the groupthink syndrome in self-managing work teams, Team Performance Management, 3,1,pp.18-31. 81.	Marques de Sa, J.P.(2001),’Pattern recognition, concepts, methods and applications’, Springer. 82.	McCorvey J.J. (20110),’ When slow is better’, Inc Magazine, 33,5. 83.	Mingers J (2000),An idea ahead of its time: the history and development of soft systems methodology, Systems Practice and Action Research, 13,6,pp.733-755 84.	Mintzberg, H.(1998),Strategy Safari: A Guide through the wilds of strategic management’, Ft-Prentice Hall. 85.	Mitchell R.K., Agle B.R. and Wood D.J.,(1997),’ Towards a Theory of Stakeholder Identificaiton and Salience: Defining the Principle of who and what really counts’, Academy of Management, 22,4,pp.853-886 86.	Mitchell Ronald K, Lowell W. Busenitz, Barbara Bird, Connie Marie Gaglio, Jeffrey S.McMullen, Eric A Morse and J.Brock Smith (2007),’ The central question in entrepreneurial cognition research’, Entrepreneurship Theory and Practice, 31:1:1-27 87.	Mitchell, R.K., Busenitz, L., Lant, T., McDougall, P.P., Morse, E.A., & Smith, B. (2002). Entrepreneurial cognition theory: Rethinking the people side of entrepreneurship research. Entrepreneurship Theory and Practice, 27(2), 93–104. 88.	Modelski G (2001),’ What causes K-waves?’, Technological forecasting and social change, 68,1,pp.75-80 89.	Moon H., Conlon D.E., Humphrey S.E., Quigley N, Devers C.E. and Nowakoski M (2003), Group decision process and incrementalism in organisational decision making, Organisational Behaviour and Human Decision Processes, 92, pp.67-79. 90.	Mountain Associates (2010), OK Modes model of Transaction cost analysis, Accessed at http://www. businessballs.com/transactionalanalysis.htm December 2011 91.	Naqvi, N Shiv B. and Bechara, A,2006, The role of emotion in decision making: a cognitive neuroscience perspective, Current Directions in Psychological Science, 15,5,pp.260-264. 235
Download free eBooks at bookboon.com

Effective Management Decision Making 92.	Oakshott, L, (1998), Quantitative Methods, Letts Study Guides.

References

93.	Olson D L (2001),’Rationality in Information Systems Support to Decision Making’, Information Systems Frontiers, 3,2,pp.239-248 94.	Pascale R.T.,(1988),’ The Honda Effect’, in Quinn J.B., Mintzberg H. and James R.M.,(eds),(1988),’ The Strategy Process’, Prentice-Hall, pp.104-110. 95.	Peniwati K (2007), Criteria for evaluating group decision making methods, Mathematical and Computer Modelling, 46,pp.935-947 96.	Pickard J (2007),’Councils pass ‘bloody awful’ housing plans’, 1st Edition, Financial Times, December 15th page 4. 97.	Porter A.L.(2004),Technology futures analysis: Toward integration of the field and new methods, Technological Forecasting and Social Change, 71,pp.287-303 98.	Roberts T.L. (2004),’Heuristics and bias in information systems project management’, Engineering Management Journal, June 1st. Accessed at http://www.allbusiness.com/management/1028702-1.html. 99.	 Roberts, D (2001),’ The knights’ slow progress: BT shareholders are frustrated by the inability of its leaders to restore momentum, Financial Times [London edition], 22 Mar 2001: 19. 100.	Rogoff Edward G, Syung-Soo Lee, Dong-Churl Suh, (2004),’Who Done it? Attributions by entrepreneurs and experts of the factors that cause and impede small business success’, Journal of Small Business Management, 42: 4:364-376 101.	Rogoff.E.G.,Lee.M.S & Suh.D.C. (2004). Who did it? Attributions by entrepreneurs and experts of the factors that cause and impede small business success. Journal of Small Business Management, 42(4), 364-376. 102.	Schmidt , C.T. (2011), Social cognition and cognitive schema, Accessed at http://www.uri.edu/research/lrc/ scholl/webnotes/Dispositions_Cognitive-Schema.htm December 2011 103.	Schulz-Hardt S, Jochims M and Frey D, (2002),Productive conflict in group decision making: genuine and contrived dissent as strategies to counteract biased information seeking, Organisational Behaviour and Human Decision Processes, 88,pp.563-586 104.	Scott (2000),’Rational Choice Theory’, in Browning G.K. and Halcli A.,(Eds.),‘Understanding contemporary society – theories of the present, SAGE,pp.126-138 105.	Shackleton D, Pitt L and Marks A.S.,1990,Managerial Decision Styles and Machiavellism: A comparative study, Journal of managerial Psychology, 5,1. 106.	Simon H (1951), A formal theory of the employment relation’, Econometrica, 19, pp.293-305 107.	Skarzausklene, A (2010),’Managing complexity: systems thinking as a catalyst of the organisation performance’, Measuring Business Intelligence, 14,4,pp.49-64 108.	Smith P A C and Sharma M (2002),’Rationalizing the promotion of non-rational behaviours in organisations, The Learning Organisation, 9,5,pp.197-201 109.	Southampton University (2011), Data Forecasting, Accessed at http://www.personal.soton.ac.uk/rchc/ Teaching/MATH6011/ForecastingChap3.htm December 2011. 110.	Srinivasan R, RangaswamyA and Lilien G L , (2005),’ Turning adversity into advantage: Does proactive marketing during a recession pay off?, International Journal of Research in Marketing, 22,2,pp.109-125. 111.	Stowell, F (2009),Soft Systems and research, Kybernetes, 38,6,pp.879-896 112.	Teale, M., Dispenza, V., Flynn J. and Currie D.,(2002),’Management Decision Making: Towards an Integrative Approach’., FT-Prentice Hall 113.	Terry, D.J.H, A., Michael, A., & White, K.M.,1999,’ The theory of planned behaviour: Self-identity, social identity and group norms’, British Journal of Social Psychology, 38,3,pp.225-244 236
Download free eBooks at bookboon.com

