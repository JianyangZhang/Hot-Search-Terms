Management	of	large	sets	of	image data
Capture,	Databases,	Image	Processing,	Storage, Visualization Karol	Kozak

Download	free	books	at

Karol Kozak

Management of large sets of image data
Capture, Databases, Image Processing, Storage, Visualization

2
Download free eBooks at bookboon.com

Management of large sets of image data: Capture, Databases, Image Processing, Storage, Visualization 1st edition © 2014 Karol Kozak & bookboon.com ISBN 978-87-403-0726-9

3
Download free eBooks at bookboon.com

Management of large sets of image data

Contents

Contents
1	 2	 Digital image	 History of digital imaging	 6 10 18 20 27 31 33 39 44 49

3	 Amount of produced images – is it danger?	 4	 5	 5.1	 6	 Digital image and privacy	 Digital cameras	 Methods of image capture	 Image formats	

7	 Image Metadata – data about data	 8	 9	 Interactive visualization (IV)	 Basic of image processing	

In the past four years we have drilled

81,000 km
That’s more than twice around the world.
Who are we?
We are the world’s leading oilfield services company. Working globally—often in remote and challenging locations—we invent, design, engineer, manufacture, apply, and maintain technology to help customers find and produce oil and gas safely.

Who are we looking for?
We offer countless opportunities in the following domains: n Engineering, Research, and Operations n Geoscience and Petrotechnical n Commercial and Business If you are a self-motivated graduate looking for a dynamic career, apply to join our team.

What will you be?

careers.slb.com

4
Download free eBooks at bookboon.com

Click on the ad to read more

Management of large sets of image data

Contents

10	

Image Processing software	

62 79 97 100 101 109 109 114 117 118 120 124

11	 Image management and image databases	 12	 Operating system (os) and images	 13	 14	 15	 15.2	 15.3	 15.4	 16	 Graphics processing unit (GPU) 	 Storage and archive	 Images in different disciplines	 Medical imaging	 Astronomical images	 Industrial imaging	 Selection of best digital images	

15.1	Microscopy	

	References:	

5
Download free eBooks at bookboon.com

Click on the ad to read more

Management of large sets of image data

Digital image

1	 Digital image
A digital image is a numeric representation (normally binary) of a two-dimensional image. Depending on whether the image resolution is fixed, it may be of vector or raster type. By itself, the term “digital image” usually refers to raster images or bitmapped images. Digital images may be produced in black and white (bitonal), grayscale, or color. Digital images are composed of very small picture elements – called pixels – lined up in rows and columns. Because pixels are the smallest picture elements, the pixel size determines how much detail will appear in the image: the smaller the pixels, the more pixels can appear in a given area, which results in higher resolution. Pixels are bundles of digital information about the color of a specific spot in the image. Basic image parameters are: pixel per inch, dots per inch, type of image (black & white, color), resolution, image depth, brightness, dynamic range, contrast, saturation, sharpness, image size, artifact. Pixels per inch (PPI) A digital image is composed of samples that the screen displays in pixels. The PPI is display resolution. How an image looks on screen is determined by the resolution of the monitor. Dots per inch (DPI) DPI is a measure of the resolution of a printer referring to how many dots of ink or toner a printer can place within an inch (or centimeter). DPI is printer resolution.

Figure 1. Each pixel has a value from 0 (black) to 255 (white). The possible range of the pixel values depend on the colour depth of the image, here 8 bit = 256 tones or greyscales (source image [55]).

6
Download free eBooks at bookboon.com

Management of large sets of image data

Digital image

Black and white images A black and white image is made up of pixels each of which holds a single number corresponding to the gray level of the image at a particular location. These gray levels span the full range from black to white in a series of very fine steps, normally 256 different grays. Since the eye can barely distinguish about 200 different gray levels, this is enough to give the illusion of a stepless tonal scale as illustrated below: Assuming 256 gray levels, each black and white pixel can be stored in a single byte (8 bits) of memory (Fig. 1). Color image A color image is made up of pixels each of which holds three numbers corresponding to the red, green, and blue levels of the image at a particular location. Red, green, and blue (sometimes referred to as RGB) are the primary colors for mixing light – these so-called additive primary colors are different from the subtractive primary colors used for mixing paints (cyan, magenta, and yellow). Any color can be created by mixing the correct amounts of red, green, and blue light. Assuming 256 levels for each primary, each color pixel can be stored in three bytes (24 bits) of memory. This corresponds to roughly 16.7 million different possible colors. Resolution The more points at which we sample the image by measuring its color, the more detail we can capture. The density of pixels in an image is referred to as its resolution. The higher the resolution, the more information the image contains. If we keep the image size the same and increase the resolution, the image gets sharper and more detailed. Alternatively, with a higher resolution image, we can produce a larger image with the same amount of detail. Image depth Call also BIT DEPTH is determined by the number of bits used to define each pixel. The greater the bit depth, the greater the number of tones (grayscale or color) that can be represented. -- A pixel with a Image depth of 1 has two possible values: black or white -- A pixel with a Image depth of 8 has 28, or 256 possible values -- A pixel with a Image depth of 24 has 224, or approx. 17 million possible values A bitonal image is represented by pixels consisting of 1 bit each, which can represent two tones (typically black and white), using the values 0 for black and 1 for white or vice versa.

7
Download free eBooks at bookboon.com

Management of large sets of image data

Digital image

A grayscale image is composed of pixels represented by multiple bits of information, typically ranging from 2 to 8 bits or more. A greyscale (i.e. monochrome / black & white) image uses one ‘byte’ per pixel (a ‘byte’ being 8 ‘bits’). An 8 bit unit or a ‘byte’, as it is called, can store up to 256 levels of information. In this way we can store up to 256 levels of brightness per pixel – which gives us an ‘8 bit greyscale’. A color image is typically represented by a bit depth ranging from 8 to 24 or higher. With a 24-bit image, the bits are often divided into three groupings: 8 for red, 8 for green, and 8 for blue. Combinations of those bits are used to represent other colors. A colour image is made when each element of the ccd array, in the camera or scanner, samples the level of a particular primary colour – Red, Green or Blue (RGB). The resultant sampling combines the information to create one full colour pixel. This full colour pixel contains three bytes (each one 8mb in depth). Three bytes per pixel (RGB) are needed so 8 × 3 = 24 bits. Brightness Brightness refers to the overall lightness or darkness of the image. Dynamic Range Dynamic range quantifies the range of brightness levels a camera’s sensor can reproduce. If your camera provides a live histogram display, you can check the brightness range in a scene before capturing it by displaying a histogram. If the brightness levels aren’t contained within the width of the graph, the highlights and shadows will be ‘clipped’ and no details will be recorded in them. Dynamic range is the ratio between the largest and smallest possible values of a changeable quantity, such in light. It is measured as a ratio, or as a base-2 (bits or stops) logarithmic value. Dynamic range are use for the luminance range of object being captured by digital camera, or the limits of luminance range that a given digital camera or film can capture, or the opacity range of developed film images, or the reflectance range of images on photographic papers. Contrast Contrast is defined as the separation between the darkest and brightest areas of the image. Increase contrast and you increase the separation between dark and bright, making shadows darker and highlights brighter. Decrease contrast and you bring the shadows up and the highlights down to make them closer to one another. Saturation Saturation is similar to contrast, however instead of increasing the separation between shadows and highlights, we increase the separation between colors.

8
Download free eBooks at bookboon.com

Management of large sets of image data

Digital image

Sharpness Sharpness can be defined as edge contrast, that is, the contrast along edges in a photo. When we increase sharpness, we increase the contrast only along/near edges in the photo while leaving smooth areas of the image alone. Let’s take a look at an example with increased sharpness. Image size The physical size of an image when it is displayed on a computer screen or printed out on paper depends on two factors: the image size and the image resolution. Image size refers to the number of pixels in an image, and is measured by the number of pixels along the horizontal and vertical sides of an image, eg 600 × 400 pixels. This is the easiest (and most accurate) way to think about the size of a digital image: the number of pixels that it contains. Image resolution refers to the density at which pixels are displayed: that is, how many pixels are displayed per inch of screen or paper. File size of image is affected by three factors: pixel dimensions, image format and bit depth. File size and image quality is directly related. Artifact Artifacts refer to distortions within the image as a result of image compression or interpolation.

9
Download free eBooks at bookboon.com

Management of large sets of image data

History of digital imaging

2	 History of digital imaging
Early Digital fax machines such as the Bartlane cable picture transmission system preceded digital cameras and computers by decades. The first picture to be scanned, stored, and recreated in digital pixels was displayed on the Standards Eastern Automatic Computer (SEAC) at NIST (Fig. 2). The advancement of digital imagery continued in the early 1960s, alongside development of the space program and in medical research. Projects at the Jet Propulsion Laboratory, MIT, Bell Labs and the University of Maryland, among others, used digital images to advance satellite imagery, wirephoto standards conversion, medical imaging, videophone technology, character recognition, and photo enhancement. Rapid advances in digital imaging began with the introduction of microprocessors in the early 1970s, alongside progress in related storage and display technologies. The invention of computerized axial tomography (CAT scanning), using x-rays to produce a digital image of a “slice” through a threedimensional object, was of great importance to medical diagnostics. As well as origination of digital images, digitization of analog images allowed the enhancement and restoration of archaeological artifacts and began to be used in fields as diverse as nuclear medicine, astronomy, law enforcement, defence and industry. Advances in microprocessor technology paved the way for the development and marketing of chargecoupled devices (CCDs) for use in a wide range of image capture devices and gradually displaced the use of analog film and tape in photography and videography towards the end of the 20th century. The computing power necessary to process digital image capture also allowed computer-generated digital images to achieve a level of refinement close to photorealism [15]. First Digital Image The first film photo registered by a computer and recreated in pixels – 30,976 to be exact. In 1957, Russell Kirsch, a scientist at what is now the National Institute of Standards and Technology, used a drum scanner connected to the SEAC (Standards Electronic Automatic Computer) to scan an image of his three-month-old son Walden.

10
Download free eBooks at bookboon.com

Management of large sets of image data

History of digital imaging

Figure 2. First digital image.

Digital camera technology is directly related to and evolved from the same technology that recorded television images. In 1951, the first video tape recorder (VTR) captured live images from television cameras by converting the information into electrical impulses (digital) and saving the information onto magnetic tape. Bing Crosby laboratories (the research team funded by Crosby and headed by engineer John Mullin) created the first early VTR and by 1956, VTR technology was perfected (the VR1000 invented by Charles P. Ginsburg and the Ampex Corporation) and in common use by the television industry. Both television/ video cameras and digital cameras use a CCD (Charged Coupled Device) to sense light color and intensity. During the 1960s, NASA converted from using analog to digital signals with their space probes to map the surface of the moon (sending digital images back to earth). Computer technology was also advancing at this time and NASA used computers to enhance the images that the space probes were sending. In 1975, Steve Sasson of Eastman Kodak built and demonstrated the first digital camera, A year later, Bryce Bayer (also of Kodak), invented the RGBG color filter array. These two technologies would be key to the future of digital photography. Initially, their use was restricted to government, scientific and forensic imaging applications. Like many new technologies, because of the high cost to produce them, those markets are where most start out. Initial yields were low from the Kodak silicon lab, producing very few high quality CCD chips, so they were very expensive. Once threre was enough demand and it was turned into a production run unit, only then would the cost drop enough to be affordable to consumers. Scanning and image editing were already being done by large pre-press systems. The development of the personal computer would finally provide the foundation that would enable those digital imaging tools to reach a wider audience. Desktop computers were becoming more and more common on desks in many corporate workplaces. First as spreadsheet and word processing tools, and later as drafting, drawing and paint software evolved, they began to be used in a variety of design and engineering visualization applications.

11
Download free eBooks at bookboon.com

Management of large sets of image data

History of digital imaging

In 1981 Sony had demonstrated the first electronic still video camera, called the Mavica (Fig. 3), which used a CCD color array and wrote still images using analog video recording technology to a 2.5” Still Video Floppy Disk.

Figure 3. Mavica camera.

Find and follow us: http://twitter.com/bioradlscareers www.linkedin.com/groupsDirectory, search for Bio-Rad Life Sciences Careers http://bio-radlifesciencescareersblog.blogspot.com

John Randall, PhD Senior Marketing Manager, Bio-Plex Business Unit

Bio-Rad is a longtime leader in the life science research industry and has been voted one of the Best Places to Work by our employees in the San Francisco Bay Area. Bring out your best in one of our many positions in research and development, sales, marketing, operations, and software development. Opportunities await — share your passion at Bio-Rad!

www.bio-rad.com/careers

12
Download free eBooks at bookboon.com

Click on the ad to read more

Management of large sets of image data

History of digital imaging

You could store 25 full frame video images or 50 field (half resolution) images on one disc. The image quality of these units was extremely poor however, suffering from the limitations of the NTSC composite video signal and the subsequent analog storage (same technology as VCR recording). Images were blurry, full of color noise, artifacts and quality degraded each time it was copied. Kodak and many other consumer electronic companies scrambled to put together development teams to exploit this new technology even though none of them really knew who might actually buy a camera for really low quality picture taking. (Photojournalists it would later be discovered, could live with the poorer quality if they could be the first to publish images of an important event.) First VGA and then 8-bit color display technology began to displace the predominantly monochrome displays of the early personal computer. The first twenty-four (24) bit color displays made their appearance around 1985 with the ATT Targa video cards for IBM PC’s. The Targa card was one of the first color video digitizers for the computer, allowing one to capture video camera images directly into the Targa board’s memory buffer without the loss of quality inflicted by first recording it on an analog medium like videotape. The included software then allowed you to do fantastic things with this incredibly clear and vibrant image that you’d captured with the press of a key or a software button on the display using a mouse pointer. Kodak’s brand new Consumer Electronics Division was having trouble selling the Kodak labeled, Japanese made 8mm video camcorders. For that reason Kodak made a decision to exit the 8mm business in 1986. Kodak instead had pinned their hopes on the potential of this new “industry standard”…. the Still Video Floppy. In the spring of 1986 the Consumer Electronics Division was renamed the Electronic Photography Division. Please note that Electronic does not necessarily mean digital. The new “electronic” photo technology Kodak was about to adopt was an already 50+ year old technology. A low quality, analog TV signal and analog recording technology, which was often further horribly compressed as a composite video signal (the yellow, two wire, video phono plug). That being the recently obselete television broadcast standard that we did away with in favor of the new High Definition digital signal. Fortunately we didn’t have to wait for that to happen first [46]. The popularity of film cameras was beginning to decrease in early 1988 when Fuji introduced the first generation digital camera, called DS-1P, utilizing CMOS sensors (Fig. 4 add image of fuji). The first ever massively sold camera that worked with a home computer via a cable – Apple QuickTake – was released in 1994 (Fig. 5).

13
Download free eBooks at bookboon.com

Management of large sets of image data

History of digital imaging

Figure 4. Fuji DS-1P digital camera

Figure 5. Apple QuickTake

Today, over 40 years after the invention of the CCD sensor, there are millions of cameras stored everywhere – from a Digital SLR, right down to the camera in your mobile phone. The technology is incredibly versatile, and still a hugely important part of photography today.

14
Download free eBooks at bookboon.com

Management of large sets of image data

History of digital imaging

Image processing history Many of the techniques of digital image processing, or digital picture processing as it often was called, were developed in the 1960s at the Jet Propulsion Laboratory, Massachusetts Institute of Technology, Bell Laboratories, University of Maryland, and a few other research facilities, with application to satellite imagery, wire-photo standards conversion, medical imaging, videophone, character recognition, and photograph enhancement. The cost of processing was fairly high, however, with the computing equipment of that era. That changed in the 1970s, when digital image processing proliferated as cheaper computers and dedicated hardware became available. Images then could be processed in real time, for some dedicated problems such as television standards conversion. As general-purpose computers became faster, they started to take over the role of dedicated hardware for all but the most specialized and computer-intensive operations. With the fast computers and signal processors available in the 2000s, digital image processing has become the most common form of image processing and generally, is used because it is not only the most versatile method, but also the cheapest [16].

678'<)25<2850$67(5©6'(*5((
&KDOPHUV 8QLYHUVLW\ RI 7HFKQRORJ\ FRQGXFWV UHVHDUFK DQG HGXFDWLRQ LQ HQJLQHHU LQJ DQG QDWXUDO VFLHQFHV DUFKLWHFWXUH WHFKQRORJ\UHODWHG PDWKHPDWLFDO VFLHQFHV DQG QDXWLFDO VFLHQFHV %HKLQG DOO WKDW &KDOPHUV DFFRPSOLVKHV WKH DLP SHUVLVWV IRU FRQWULEXWLQJ WR D VXVWDLQDEOH IXWXUH ¤ ERWK QDWLRQDOO\ DQG JOREDOO\ 9LVLW XV RQ &KDOPHUVVH RU 1H[W 6WRS &KDOPHUV RQ IDFHERRN

15
Download free eBooks at bookboon.com

Click on the ad to read more

Management of large sets of image data

History of digital imaging

Quotes 1.	 “It can be a trap of the photographer to think that his or her best pictures were the ones that were hardest to get.” – Timothy Allen 2.	 “The digital camera is a great invention because it allows us to reminisce” Instantly. Demetri Martin 3.	 “The photographic image…is a message without a code”., Roland Barthes 4.	 “Virtual image, a point or system of points, on one side of a mirror or lens, which, if it existed, would emit the system of rays which actually exists on the other side of the mirror or lens.”, Clerk Maxwell 5.	 “Be daring, be different, be impractical, be anything that will assert integrity of purpose and imaginative vision against the play-it-safers, the creatures of the commonplace, the slaves of the ordinary.”, Peter Lindbergh 6.	 “The important thing is not the camera but the eye.”, Alfred Eisenstaedt 7.	 “There are two people in every photograph: the photographer and the viewer.”, Ansel Adams 8.	 “To me, photography is the simultaneous recognition, in a fraction of a second, of the significance of an event as well as of a precise organization of forms that give that event its proper expression.”, Henri Cartier-Bresson 9.	 “I always thought good photos were like good jokes. If you have to explain it, it just isn’t that good.”, Anonymous 10.	 “If your photographs aren’t good enough, you’re not close enough.”, Robert Cappa 11.	 “A lot of photographers think that if they buy a better camera they’ll be able to take better photographs. A better camera won’t do a thing for you if you don’t have anything in your head or in your heart.”, Arnold Newman 12.	 “Pictures, regardless of how they are created and recreated, are intended to be looked at. This brings to the forefront not the technology of imaging, which of course is important, but rather what we might call the eyenology (seeing).”, Henri Cartier-Bresson 13.	 “The word ‘art’ is very slippery. It really has no importance in relation to one’s work. I work for the pleasure, for the pleasure of the work, and everything else is a matter for the critics.”, Manuel Alvarez Bravo 14.	 “People say photographs don’t lie, mine do.”, David LaChapelle 15.	 “The single most important component of a camera is the twelve inches behind it.”, Ansel Adams 16.	 “You cannot depend on your eyes if your imagination is out of focus”, Mark Twain 17.	 “The wisdom of the wise and the experience of the ages are perpetuated in quotations.”, Benjamin Disraeli 18.	 “You don’t take a photograph, you make it.”, Ansel Adams 19.	 “Your first 10,000 photographs are your worst.”, Henri Cartier-Bresson 20.	 “Beauty can be seen in all things, seeing and composing the beauty is what separates the snapshot from the photograph.”, Matt Hardy

16
Download free eBooks at bookboon.com

Management of large sets of image data

History of digital imaging

21.	 “Nothing happens when you sit at home. I always make it a point to carry a camera with me at all times…I just shoot at what interests me at that moment.”, Elliott Erwitt 22.	 “Which of my photographs is my favorite? The one I’m going to take tomorrow.”, Imogen Cunningham 23.	 “You’ve got to push yourself harder. You’ve got to start looking for pictures nobody else could take. You’ve got to take the tools you have and probe deeper.”, William Albert Allard 24.	 “Twelve significant photographs in any one year is a good crop.”, Ansel Adams

Linköping University – innovative, highly ranked, European
Interested in Engineering and its various branches? Kickstart your career with an English-taught master’s degree.

Click here!

17
Download free eBooks at bookboon.com

Click on the ad to read more

Management of large sets of image data

Amount of produced images – is it danger?

3	 Amount of produced images – is it danger?
Today we take images for granted. They are our experiments, probes, samples, memories of holidays and parties, of people and places. An explosion of cameras and places to share them (Medical databases, Facebook, Twitter, Instagram) means that our lives today are documented, not by an occasional oxidizing of silver halide but constantly recorded with GPS coordinates and time stamps. However it hasn’t always been like this – the oldest photograph is less than 200 years old. We quantify how many analog photos humans have taken. There is a surprising dearth of direct data, but we can make some reasonable estimates. It is safe to say that at most a few million photos were snapped before the invention of the first consumer camera – Kodak Brownie in 1901. From that time we can use Kodak’s employment statistics as a reasonable proxy for how many photos were taken (Kodak’s dominance of those “Kodak moments” persisted for most of the 20th century). More physical photos needed more physical cameras and rolls of print. Throughout this period photos became more and more mass-market – by 1960 it is estimated that 55% of photos were of babies. From 1984 onwards the Silver Institute and PMIA published estimates of how many physical photos the world was snapping each year (silver halide being an important chemical in film). Year after year these numbers grew, as more people took more photos – the 20th century was the golden age of analog photography peaking at an amazing 85 billion physical photos in 2000 – an incredible 2,500 images per second. At the dawn of the new millennium a new technology (that Kodak itself invented) was reshaping the whole industry – the digital photo. When the first few hundred thousand digital cameras shipped in 1997 their memory was strictly limited (in fact cameras like the Sony Mavica took floppy disks – Fig. 3). Digital cameras are now ubiquitous – it is estimated that 2.5 billion people in the world today have a digital camera. If the average person snaps 150 photos this year that would be a staggering 375 billion photos. That might sound implausible but this year people will upload over 70 billion photos to Facebook, suggesting around 20% of all photos this year will end up there. Already Facebook’s photo collection has a staggering 140 billion photos, that’s over 10,000 times larger than the Library of Congress.

18
Download free eBooks at bookboon.com

Management of large sets of image data

Amount of produced images – is it danger?

Today, world’s largest digital image library is an facebook. Number two is an flickr. Even accounting for population growth the exponential growth of images is incredible (we take 4 times as many photos as 10 year ago). Today every party, birthday, sports game and concert is documented in rich detail. The combination of all these photos is a rich portrait of today, the possibilities of which are illustrated by a tool like “The Moment”. As photos keep growing we take a clearer and clearer snapshot of our lives and world today – in total we have now taken over 3.5 trillion photos. The kind of photos we are taking has changed drastically – analog photos have almost disappeared – but the growth of images continues [1]. The advent of digital cameras brought on a new era in digital images and it’s a prolific one. The number of images we take each year has exploded. In fact, in a recent presentation by Yahoo!, it was claimed that as many as 880 BILLION photos will be taken in 2014 if we continue on the current trend. As for Facebook proper, they get 208,300 photos uploaded every minute. Facebook has always been a bit vague about exactly how many photos they get over a period of time, but they’ve gone on record to say that it’s over six billion photos per month. It’s likely even more than that now. For scale, Yahoo!’s Flickr service has gotten about eight billion photos during its whole run. People love getting comments about their photos and take great inspiration from the images of others. It’s little wonder, with such a vibrant web of personal exchanges, that the genre has been booming, resulting in millions of people taking great photos every day, and experimenting (or “appsperimenting”) with their images in highly creative ways. In the midst of the 3.5 trillion images that have ever been taken it’s easy to forget that the shoebox or album of old photos we have at home is incredibly fragile and special. Every 2 minutes today we snap as many photos as the whole of humanity took in the 1800s. In fact, ten percent of all the photos we have were taken in the past 12 months. And yet, there are still more physical photos hidden in our shoeboxes, hanging on our walls or lost in an album than there are digital photos littering our hard drive. These precious images of the past 200 years tell us who we are and where we come from. So grab hold of that photo of you as a kid or of your grandparents’ wedding and realize just how special it is [1].

19
Download free eBooks at bookboon.com

Management of large sets of image data

Digital image and privacy

4	 Digital image and privacy
Digital image devices (e.g., digital cameras, smartphones, scanners) typically use a standard known as Exchangeable image file format (Exif) that specifies how additional informational data may be stored with images as they are created. When you snap a picture with your digital camera, you may also (depending upon the type of camera and its settings) be capturing information about the date and time you took the photo, the camera type and the settings you used to take the picture, a description of the photograph, and copyright information. All of this information is stored as metadata in the same file that holds your photo. Images often contain a bundle of information and various traces left by digital cameras or photo manipulation software. Exif is a key tool for many professionals. It can detail whether the photographer used a flash, which digital effects were applied to a picture and when the photo was taken. EXIF can also contain the precise GPS coordinates for where a photo was taken. This information is readily accessible and can be plugged into software such as Google Maps – leading some security and photography experts to express concerns about amateurs unknowingly disclosing private information, such as the location of their home.

20
Download free eBooks at bookboon.com

Click on the ad to read more

Management of large sets of image data

Digital image and privacy

One of the types of data that may be stored with images created by devices that use the Exif format is location information. Many mobile phones (and some digital cameras) now have built-in GPS receivers that can record precise information about where a picture was taken in the photograph’s Exif header (commonly known as geotagging). When such photographs are shared with others (by posting them on the Internet, for example), it is possible that viewers can examine the Exif metadata stored with those images to find out information such as where the pictures were taken, and use tools that map the stored GPS information to specific locations (such as a particular house or school). This poses potential privacy and security issues, especially since some users may be completely unaware that their cameras are set up to store location information by default. The storage of location based data, in the form of Latitude and Longitude inside of images is called Geotagging; essentially tagging your photograph with the geographic location. This data is stored inside if the metadata is JPEG images and is useful for tying the photograph to a location. Want to remember exactly where you took those photographs while on vacation? This information is for you. However, most modern digital cameras do not automatically add geolocation (Latitude and Longitude) metadata to pictures. The process for adding the geolocation data either requires specialized add on hardware, or post processing with software on the desktop after the pictures are taken. There is a large exception to this rule: Smartphones. With the proliferation of smart phones that contain GPS locator technology inside, the cameras in these devices are already equipped with the specialized hardware to automatically add geolocation information to the pictures at the time they are taken. Most people don’t realize that the action of automatic geotagging takes place on their smart phones, either because it is enabled by default, not exposed the user as an option, or was asked and then forgotten. As a result, individuals often share too much information about their location, right down to the exact Latitude and Longitude when snapping photos with their smartphone and posting them online. Apple’s and Google’s systems ask each user once or a few times for permission to access their location in order to provide additional services. If they click “OK” on that popup, every photo they take is tagged with GPS coordinates. Smartphones are fast becoming the camera of choice for many people. Cameras on newer phones have come to rival dedicated point-and-shoots, and many smartphone owners carry them just about everywhere. Millions of images are uploaded to Facebook using the company’s iPhone, Android and BlackBerry applications. The iPhone 3G is the most popular shooter among photographers on Yahoo’s Flickr website, according to a report on that site.

21
Download free eBooks at bookboon.com

Management of large sets of image data

Digital image and privacy

Judging by the abundance of pictures in Flickr’s database that include geolocation data in the EXIF, some smartphone owners aren’t thinking twice about opting into their devices’ GPS feature. Doing so can facilitate useful tools. For example, software like iPhoto and Picasa can group images by location and display them on a map. But amateur photographers may not realize that this info stays with the image when it’s uploaded to web image services some other photo-sharing services. Images uploaded to Photobucket by one woman show her children preparing lunch and bathing in a kitchen sink. The location data, which is displayed directly on each photo’s webpage, can be inputted into Google Maps to find a satellite image of her rural home in Edmond, Oklahoma. The woman couldn’t be reached for comment. Anyone could still download the original file using a link from web services and view the location info in Adobe’s Photoshop or in software included with every new Mac and Windows 7 computer. However, this isn’t really a “new” danger (the news video linked above is nearly four years old), and the potential for harm is much less now than it was when the story was originally reported. Where and how you post your photographs on the Internet makes a big difference: If you put up photographs on the Internet by directly copying them to your own web site, then geotagging might be an issue you need to take heed of. But if you use one or more of the currently popular photo-sharing and social media web sites (such as Facebook and Twitter), geotagging is not much of an issue because those sites now automatically strip some or all of the Exif metadata from uploaded pictures in order to protect user privacy. (Other sites may leave Exif metadata intact or provide users with the option of whether or not to make the Exif information from their uploaded photos available to other viewers.) Picture-takers have a number of ways of avoiding storing location information with their photographs or eliminating it from existing pictures. Turning off your device’s GPS feature is the most straightforward way: if your camera or smartphone can’t use GPS to determine where you are when you take a picture, it can’t store that information with your photo. After the fact, you can use an Exif metadata editor to remove or change information stored with your photographs, or you can use a photo editor or converter program to save your photographs in a format that does not support Exif metadata. A January 2014 variant warned that hackers could not only discern your location from posted smart phone photos, but they could use information embedded in those pictures to clone your cell phone:

22
Download free eBooks at bookboon.com

Management of large sets of image data

Digital image and privacy

Share this with everyone you know! Did you know that posting a screenshot of your cellphone on Facebook leave you open to be a victim of IDENTITY THEFT? Every cell phone has a graphic signature embedded in the pixels that can be easily deciphered by hackers with a simple computer program. A screenshot can yield a person’s location information, phone number and unique SIM card ID number. It’s this last bit of information that can allow hackers to easily CLONE YOUR PHONE and then monitor all your text messages, numbers and addresses in your contact list, and even any credit or bank transactions you make from that phone. If you know someone who is posting screenshots of their cell phone, forward this to them and warn them to STOP NOW. We are unaware of any cell phone function that embeds information in photographs that would (by itself) allow hackers to clone the cell phone used to take those photos. Fully Protecting Your Images is Impossible Anything that is publicly available online could be seen by your students and others – including their parents, your colleagues and your manager – so you should exercise caution.

23
Download free eBooks at bookboon.com

Click on the ad to read more

Management of large sets of image data

Digital image and privacy

Before you post any information or images on the internet, consider whether it could cause you embarrassment or potentially damage your reputation or career. Ask your friends to consider the ramifications too before they post or tag any images of you on the internet. Take care when storing images on your mobile – it’s easy to misplace or lose a mobile, which could mean that your photos and videos fall into the wrong hands. Always activate your PIN lock or security protection and don’t leave your mobile unattended. 1) Image Protection with a Watermark: The most common, and possibly strongest, image protection technique is to place a visual copyright watermark on the image. Image watermarks come in a range of styles: from a full copyright symbol across the image to simply small text in the corner of an image containing the copyright symbol, date, and name of the copyright holder. There is a trade off with watermark size: the larger the watermark the less likely one is to download or steal the image, but the larger the watermark the more of a visual distraction it becomes. There are several techniques we can use to make website downloads and image theft much harder. Image watermarks are easy to create in Photoshop: make a new layer with the desired text, adjusting its size and location to best suit your needs. To type the copyright symbol © in text, you can type option+g (mac) or Ctrl+Alt+C (PC). Changing the blending mode and opacity for this layer can create a more artistic and less obstructive watermark: for example ‘soft light’ layer blending can create a nice opaque blend. It is possible to further customize the watermark by using layer blending options such as a dropped shadow and/or beveling. Digimarc, a technology that can be integrated as a Photoshop plug-in, can also produce a digital watermark through the use of digital noise that is read by software. Despite the presence of a watermark however, its survival through image manipulations is not definite, and the lack of a visual symbol may not prevent image theft – only act as proof of copyright owner in case of theft. Image Protection Using Low Resolution: Low resolution images should almost always be used on the internet to protect them against theft. A 550 pixel (in its longest dimension) image saved at 72dpi is a good size for internet use, and will not give downloaders much image to work with: other than internet use, images of this size look quite horrible in just about any other application imaginable. On a photosharing site like flickr, images are resized down to 500 pixels (longest dimension) for basic viewing, thus you can upload images 500 pixels wide and not change how most view the photo. A Note on Color Profiles: Much of the internet currently lacks color management (for more information see my article on Soft Proofing and Color Management for the internet). Uploading images without a color profile will not protect an image from being downloaded, but could potentially prevent it from being used: those wishing to use the image in a color managed, internet independent application may be at a loss if they see a drastic shift in colors and contrast and do not know how (or care to) to rectify it. For those that are worried about image quality due to a missing color profile, the consequences are small: the minority of viewers using color managed browsers will see exactly what those using non-color managed browsers see (if you ultimately wish viewers to see as close to what you see in photoshop, I recommend Soft Proofing your images for the web).

24
Download free eBooks at bookboon.com

Management of large sets of image data

Digital image and privacy

Prevent Downloads Using Tables: ‘Right clicking’ images is a fast shortcut to find, copy, and/or download images. For those that have their own website it is possible to prevent this action by placing images as a background to tables. Hiding Images: It is possible to look for an image by scouring the source code of a webpage in search of a link to the image. These links however, can be hidden in a number of ways by a web designer. First, they can be hidden within the code by url encoding: converting the html code of the image to ascii (hexadecimal). Preventing Bandwidth Theft: Bandwidth theft, also known as hotlinking, is the process by which people use images from your server to display in their own webpages. The end result is that the image is still on your server but linked to and displayed by a remote website. It is incredible how easy this is to do and how often this occurs, and startling how many images found in image searches on google are hotlinked images. So why prevent hotlinking? There are a variety of reasons. As the name implies, other websites are stealing your bandwidth: in most cases the bandwidth is negligable, in a few cases it can add up. Further, when hotlinking occurs it is unknown in what context the image is being used. Lastly, the link to your website could be on a website that performs “black hat” or malicious activity. Prevent Search Engine Indexing: Protecting images from search engine indexing can be accomplished in two ways. First, a META tag can prevent images from getting indexed, but allow the rest of the page to be found in searches. Finding Your Images: In the instance someone uses your image(s) without permission, you may usually never know. For online usage however, it is possible to be pro-active in finding unauthorized usage by simply searching for your images; the greatest tools being image search engines powered by google, yahoo, and MSN. Although this may seem like finding a needle in a haystack, at times it can be a very useful tool. Copyright Images: A copyright is a form of protection provided by law that prevents the use of a piece of work, and the copyright owner the person who originally created that work (please visit the United States Copyright Office for more information). In almost every case, if you push the shutter button you hold the copyright. However, it is always advisable to register a copyright of your images with the United States Library of Congress or a similar entity in the country where you reside. While this will not prevent images from being downloaded and/or stolen, it will give you proof that you are the copyright owner [39].

25
Download free eBooks at bookboon.com

Management of large sets of image data

Digital image and privacy

If you post your images online, it is possible for someone to steal them and use them somewhere else. No matter what you do to protect them. No right-click scripts can be defeated by using a view source bookmarklet and browsing to the image directly. Shrink wrapping the images can be defeated the same way. Watermarks can be removed (with difficulty). Even if you embed your images in a Flash object or something else to protect them, it’s possible to take a screen shot of the desktop as that object is displaying your image. If your image is so valuable that you want to be sure no one ever steals it, then don’t post it online. That is the securest and safest method of protecting your digital images.

26 destinations 4 continents
Bartending is your ticket to the world

GET STARTED

26
Download free eBooks at bookboon.com

Click on the ad to read more

Management of large sets of image data

Digital cameras

5	 Digital cameras
A digital camera is a camera that encodes digital images and videos digitally and stores them for later reproduction. Most cameras sold today are digital, and digital cameras are incorporated into many devices ranging from PDAs and mobile phones (called camera phones) to vehicles. Digital camera technology Digital camera components (Fig. 6) Workflow (Fig. 7, 8): •	 Light passes through the lens and strikes a mirror (green) •	 The mirror reflects the light up to a focusing screen •	 Light passes through the focusing screen and enters a block of glass called a pentaprism (orange) •	 The pentaprism reflects the image so that you can see it in the viewfinder •	 When you take a photo, the mirror flips up and a shutter (blue) opens that exposes the digital sensor (red) to light •	 This is a great example of what-you-see-is-what-you-get. By using the viewfinder you can precisely compose your image and adjust the focus.

Figure 6. The Anatomy of a Digital camera (Picture source [17])

27
Download free eBooks at bookboon.com

Management of large sets of image data

Digital cameras

Figure 7. Workflow in digital camera (Picture source [17]).

Figure 8. When the photographer is framing the shot, light is directed through the lens. It hits a mirror and is reflected up toward the viewfinder. As a result, the photographer’s looking right down. (Picture source [57])

Digital and film cameras share an optical system, typically using a lens with a variable diaphragm to focus light onto an image pickup device (Fig. 8). The diaphragm and shutter admit the correct amount of light to the imager, just as with film but the image pickup device is electronic rather than chemical. However, unlike film cameras, digital cameras can display images on a screen immediately after being recorded, and store and delete images from memory. Many digital cameras can also record moving video with sound. Some digital cameras can crop and stitch pictures and perform other elementary image editing (Fig. 9).

28
Download free eBooks at bookboon.com

Management of large sets of image data

Digital cameras

Figure 9. Process of making an digital image by digital camera. Processor and external computer is responsible for image processing and image data management.

Charge-Coupled Device (CCD) CCD is a semiconductor device that converts optical images into electronic signals. CCDs contain rows and columns of ultra small, light-sensitive mechanisms (pixels) that, when electronically charged and exposed to light, generate electronic pulses that work in conjunction with millions of surrounding pixels to collectively produce a photographic image. CCDs and CMOS (Complementary Metal Oxide Semiconductor) sensors are the dominant technologies for digital imaging. CCD cameras may also produce noise. All noise by nature creates uncertainty in the brightness levels of the pixels in your image. If a picture has a lot of noise in it, you can see this visually. There will be variations in brightness in areas that you expect to have a more uniform brightness. Words commonly used to describe this effect are “grainy” and “gritty.” Some common sources of system noise in CCD imaging include: •	 Readout noise results from collecting, amplifying, and converting pixel data to electrons inside the camera. •	 Dark current results from electrons accumulating in pixels even in the absence of light. •	 Optical dust is debris and bits of dust in the optical path that cast shadows on the CCD detector. •	 Background noise results from sky glow. •	 Reflections are the effects of light that reflects off of various surfaces in the telescope, focuser, or CCD camera, and falls on the CCD detector. •	 Image processing noise results from processing the image using various techniques. Removing the system noise from your images is a form of data reduction.

29
Download free eBooks at bookboon.com

Management of large sets of image data

Digital cameras

Megapixel A megapixel contains 1,000,000 pixels and is the unit of measure used to describe the size of the sensor in a digital camera. Optical Zoom Optical zoom enable to change the magnification ratio, i.e., focal length of the lens by either pushing, pulling or rotating the lens barrel. Unlike variable focal length lenses, zooms are constructed to allow a continuously variable focal length, without disturbing focus. Digital Zoom Unlike an optical zoom, which is an optically lossless function of the camera’s zoom lens, digital zoom takes the central portion of a digital image and crops into it to achieve the effect of a zoom. This means that the existing data is not enhanced or added to, merely displayed at a lower resolution, thereby giving an illusion of an enlarged image.

.

30
Download free eBooks at bookboon.com

Click on the ad to read more

Management of large sets of image data

Digital cameras

5.1	
Acquire

Methods of image capture

Acquire is use to import digital image files into a software application for processing or editing purposes. The term is often applied differently within different types of software [14]. Single-shot capture systems use either one sensor chip with a Bayer filter mosaic, or three separate image sensors (one each for the primary additive colors red, green, and blue) which are exposed to the same image via a beam splitter multi-shot exposes the sensor to the image in a sequence of three or more openings of the lens aperture. There are several methods of application of the multi-shot technique. The most common originally was to use a single image sensor with three filters passed in front of the sensor in sequence to obtain the additive color information. Another multiple shot method is called Microscanning. This method uses a single sensor chip with a Bayer filter and physically moved the sensor on the focus plane of the lens to construct a higher resolution image than the native resolution of the chip. A third version combined the two methods without a Bayer filter on the chip. The third method is called scanning because the sensor moves across the focal plane much like the sensor of an image scanner. Their linear or tri-linear sensors utilize only a single line of photosensors, or three lines for the three colors. Scanning may be accomplished by moving the sensor e.g. when using color co-site sampling or rotating the whole camera; a digital rotating line camera offers images of very high total resolution. The choice of method for a given capture is determined largely by the subject matter. It is usually inappropriate to attempt to capture a subject that moves with anything but a single-shot system. However, the higher color fidelity and larger file sizes and resolutions available with multi-shot and scanning backs make them attractive for commercial photographers working with stationary subjects and large-format photographs. Improvements in single-shot cameras and image file processing at the beginning of the 21st century made single shot cameras almost completely dominant, even in high-end commercial photography. Filter mosaics, interpolation, and aliasing The Bayer arrangement of color filters on the pixel array of an image sensor. Most current consumer digital cameras use a Bayer filter mosaic in combination with an optical anti-aliasing filter to reduce the aliasing due to the reduced sampling of the different primary-color images. A demosaicing algorithm is used to interpolate color information to create a full array of RGB image data.

31
Download free eBooks at bookboon.com

Management of large sets of image data

Digital cameras

Cameras that use a beam-splitter single-shot 3CCD approach, three-filter multi-shot approach, color co-site sampling or Foveon X3 sensor do not use anti-aliasing filters, nor demosaicing. Firmware in the camera, or a software in a raw converter program such as Adobe Camera Raw, interprets the raw data from the sensor to obtain a full color image, because the RGB color model requires three intensity values for each pixel: one each for the red, green, and blue (other color models, when used, also require three or more values per pixel). A single sensor element cannot simultaneously record these three intensities, and so a color filter array (CFA) must be used to selectively filter a particular color for each pixel. The Bayer filter pattern is a repeating 2×2 mosaic pattern of light filters, with green ones at opposite corners and red and blue in the other two positions. The high proportion of green takes advantage of properties of the human visual system, which determines brightness mostly from green and is far more sensitive to brightness than to hue or saturation. Sometimes a 4-color filter pattern is used, often involving two different hues of green. This provides potentially more accurate color, but requires a slightly more complicated interpolation process. The color intensity values not captured for each pixel can be interpolated from the values of adjacent pixels which represent the color being calculated. Exposure Exposure is the phenomenon of light striking the surface of film or digital imaging sensor. The exposure is determined by the volume of light passing through the lens aperture (f/stop) combined with the duration of the exposure (shutter speed). The proper exposure, which is best determined using a light meter, can be established in a number of exposure modes including manual, program (automatic), shutter priority and aperture priority [64]. Autofocus The ability of the camera and lens to keep the subject in focus during the exposure. Autofocus can be continuous, meaning focus is maintained regardless of where it moves within the frame, or Single, meaning the point of focus is locked regardless of where the subject may move [64]. Depth of Focus Depth of focus is the measurement of the area in focus within an image, from the closest point of focus to the furthest point of focus [64].

32
Download free eBooks at bookboon.com

Management of large sets of image data

Image formats

6	 Image formats
Which file format should we use in digital imaging? There are four main file formats that people mostly use when finding or creating images: TIFF, JPEG, GIF and PNG. If people are working with photographic or other continuous tone images, you should be using JPEGs when incorporating images into teaching and learning materials because they are of good quality with a small file size. They are also the most common image file format found on the web. TIFFs, while they have excellent quality, generally have a very large file size which may make presentations run slowly, and GIFs have limited colour capabilities and so are more suitable for icons or very simple images. PNG is a newer format that share characteristics with both TIFFs and JPEGs, but it is still not a very common format. In table 1 common raster faile formats are listed and in table 2 vector file format.
Extension .jpg .jp2 Name Joint Photographic Experts Group Joint Photographic Experts Group Portable Network Graphics Graphics Interchange Format EXR Raw image file Notes Lossy compression format well suited for photographic images While there is a modest increase in compression performance of JPEG 2000 compared to JPEG, the main advantage offered by JPEG 2000 is the significant flexibility of the codestream. Lossless compression image, supporting 16bit sample depth, and Alpha channel 8bit indexed bitmap format, is superceded by PNG on all accounts but animation HDR, High Dynamic Range format, used by movie industry.. Direct memory dump from a digital camera, contains the direct imprint from the imaging sensor without processing with whitepoint and gamma corrections. Different cameras use different extensions, many of them derivatives of TIFF, examples are .nef, .raf and .crw A subset/clarification of TIFF, created by Adobe to provide a standard for storing RAW files, as well as exchanging RAW image data between applications.  TIFF is a flexible, adaptable file format for handling images and data within a single file, by including the header tags (size, definition, image-data arrangement, applied image compression) defining the image’s geometry. Native format of Adobe Photoshop, allows layers and other structural elements GIMP’s native image format.

.png .gif .exr .raw

.dgn .tiff, .tif

Digital Negative Tagged Image File Format Photoshop Document Gimp Project File

.psd .xcf

Tabel 1. Raster File Formats

33
Download free eBooks at bookboon.com

Management of large sets of image data

Image formats

File Extension .ai .eps .ps .pdf .svg .swf

Name Adobe Illustrator Document Encapsulated Postscript PostScript Portable Document Format Scalable Vector Graphics Shockwave Flash

Notes Native format of Adobe Illustrator (based on .eps) Industry standard for including vector graphics in print Vector based printing language, used by many Laser printers, used as electronic paper for scientific purposes. Modernized version of ps, adopted by the general public as ‘electronic print version’ XML based W3C standard, incorporating animation, gaining adoption. Binary vector format, with animation and sound, supported by most major web browsers.

Tabel 2. Vector File Formats.

Another format commonly used in science, industry and other disciplines: RAW Files Ddigital cameras have the option of capturing RAW files, which contain all of the data captured during the exposure in an unedited format. During processing, RAW files can be adjusted far more extensively than images captured in other imaging formats, and can be saved as any format later. The original RAW file remains unaltered and can be reprocessed at any time for other purposes.

Think Umeå. Get a Master’s degree!
• modern campus • world class research • 31 000 students • top class teachers • ranked nr 1 by international students Master’s programmes: • Architecture • Industrial Design • Science • Engineering

Sweden www.teknat.umu.se/english

34
Download free eBooks at bookboon.com

Click on the ad to read more

Management of large sets of image data

Image formats

Bitmap A method of storing digital information by mapping out an image bit by bit. The density of the pixels determines how sharp the image resolution will be. Most image files are bitmapped. Bitmap images are compatible with all types of computers. BMP A bit-mapped file format used by Microsoft Windows. The BMP format supports RGB, indexed-color, grayscale and Bitmap color modes. CMY Color (Cyan, Magenta and Yellow) These three secondary colors can be combined to recreate all other colors. Like CMYK, CMY is used in printing to create the colors seen in a print, though with less density in the blacks as compared to CMYK color. CMY color is used in some of the least expensive desktop printers. CMYK Color (Cyan, Magenta, Yellow, Black) CMYK is the color space used for commercial offset printing. CMYK is also a common working color space for inkjet, laser, dye-sublimation and wax thermal printers. Thumbnails Small, contact sheet-sized image files used to reference or edit digital images. The images that appear on viewer or display of capture device are thumbnail images of the larger file. Time Lapse A series of photographs captured over a period of time. These images can be captured in variable or set time intervals over the course of seconds, minutes, hours, days, weeks, etc. Although several more advanced cameras offer the option of custom function time-lapse imaging, most cameras require optional hard wired or remotely operated triggering devices to capture time-lapse imagery. PNG (Portable Network Graphics) Developed as a patent-free alternative to GIF, this format is used for lossless compression for purposes of displaying images on the World Wide Web. Adopted by the WWW consortium as a replacement for GIF, some older versions of Web browsers may not support PNG images.

35
Download free eBooks at bookboon.com

Management of large sets of image data

Image formats

Floating point Some image formats used in research and by the movie industry store floating point values. Both “normal” 32bit floating point values and a special format called half which uses 16bits/sample. Floating point is useful as a working format because quantization and computational errors are kept to a minimum until the final render. Floating point representations often include HDR, High Dynamic Range. High Dynamic Range images are images that include sampling values that are whiter than white (higher values than 255 for a normal 8bit image). HDR allows representing the light in a scene with a greater degree of precision than LDR, Low Dynamic Range images. DICOM Digital Imaging and Communications in Medicine (DICOM) is a standard for handling, storing, printing, and transmitting information in medical imaging. It includes a file format definition and a network communications protocol. The communication protocol is an application protocol that uses TCP/IP to communicate between systems. DICOM files can be exchanged between two entities that are capable of receiving image and patient data in DICOM format. The National Electrical Manufacturers Association (NEMA) holds the copyright to this standard.

How could you take your studies to new heights?
By thinking about things that nobody has ever thought about before By writing a dissertation about the highest building on earth With an internship about natural hazards at popular tourist destinations By discussing with doctors, engineers and seismologists By all of the above

From climate change to space travel – as one of the leading reinsurers, we examine risks of all kinds and insure against them. Learn with us how you can drive projects of global significance forwards. Profit from the know-how and network of our staff. Lay the foundation stone for your professional career, while still at university. Find out how you can get involved at Munich Re as a student at munichre.com/career.

36
Download free eBooks at bookboon.com

Click on the ad to read more

Management of large sets of image data

Image formats

FITS Flexible Image Transport System (FITS) is an open standard defining a digital file format useful for storage, transmission and processing of scientific and other images. FITS is the most commonly used digital file format in astronomy. Unlike many image formats, FITS is designed specifically for scientific data and hence includes many provisions for describing photometric and spatial calibration information, together with image origin metadata. HDF5 Hierarchical Data Format (HDF, HDF4, or HDF5) is the name of a set of file formats and libraries designed to store and organize large amounts of numerical data. Originally developed at the National Center for Supercomputing Applications, it is currently supported by the non-profit HDF Group, whose mission is to ensure continued development of HDF5 technologies, and the continued accessibility of data currently stored in HDF. HDF5 is a data model, library, and file format for storing and managing data. It supports an unlimited variety of datatypes, and is designed for flexible and efficient I/O and for high volume and complex data. HDF5 is portable and is extensible, allowing applications to evolve in their use of HDF5. The HDF5 Technology suite includes tools and applications for managing, manipulating, viewing, and analyzing data in the HDF5 format. Because an HDF5 file can contain almost any collection of data entities in a single file, it has become the format of choice for organizing heterogeneous collections consisting of very large and complex image datasets (Fig. 10).

Figure 10. HDF5 file with a strictly hierarchical group structure

37
Download free eBooks at bookboon.com

Management of large sets of image data

Image formats

An HDF5 file is a data container, similar to a file system. Within it, user communities or software applications define their organization of image data objects. The basic HDF5 data model is simple, yet extremely versatile in terms of the scope of image data that it can store. It contains two primary objects: groups, which provide the organizing structures, and datasets, which are the basic storage structures. HDF5 groups and datasets may also have attributes attached, a third type of data object consisting of small textual or numeric metadata defined by user applications. It would be desirable to adopt an existing scientific, medical, or computer image format, and simply benefit from the consequences. All image formats have their strengths and weaknesses. They tend to fall into two categories: generic and specialized formats. Generic image formats usually have fixed dimensionality or pixel design. For example, MPEG2 is suitable for many applications as long as it is 2D spatial plus 1D temporal using red-green-blue modality that is lossy compressed for the physiological response of the eye. Alternatively, the specialized image formats suffer the difficulties of the image formats we are already using. For example, DICOM (medical imaging standard) and FITS (astronomical imaging standard) store their pixels as 2D slices, although DICOM does incorporate MPEG2 for video-based imagery. The ability to tile (2D), brick (3D), or chunk (nD) is required to access very large images. Although this is conceptually simple, the software is not, and must be tested carefully or risk that subsequent datasets be corrupted. That risk would be unacceptable for operational software used in data repositories and research. This function and its certification testing are critical features of HDF software that are not readily available in any other format.

Scholarships

Open your mind to new opportunities

With 31,000 students, Linnaeus University is one of the larger universities in Sweden. We are a modern university, known for our strong international profile. Every year more than 1,600 international students from all over the world choose to enjoy the friendly atmosphere and active student life at Linnaeus University. Welcome to join us!

Bachelor programmes in Business & Economics | Computer Science/IT | Design | Mathematics Master programmes in Business & Economics | Behavioural Sciences | Computer Science/IT | Cultural Studies & Social Sciences | Design | Mathematics | Natural Sciences | Technology & Engineering Summer Academy courses

38
Download free eBooks at bookboon.com

Click on the ad to read more

Management of large sets of image data

Image Metadata - data about data

7	 Image Metadata – data about data
A digital image may include metadata that describe how large the picture is, the color depth, the image resolution, when the image was created, and other data. Metadata is stored in image files using a variety of mechanisms. Digital cameras and scanners automatically insert metadata into the images they create. Digital image processing applications like Adobe Photoshop allow users to add or edit metadata to be stored with the image. Annotating digital images with additional metadata is a common practice in image and news gathering applications and for image archiving usages, as well as at the consumer level. Metadata may be written into a digital photo file that will identify who owns it, copyright and contact information, what camera created the file, along with exposure information and descriptive information such as keywords about the image, making the file searchable on the computer and/or the Internet. Some metadata are written by the camera and some is input by the user of camera and/or software after downloading to a computer. However, not all digital cameras enable you to edit metadata. Each image file format has distinct rules regarding how metadata formats must be stored within the file. Within image file formats, metadata can be stored using a variety of common metadata container formats such as Exif/TIFF IFDs, Adobe XMP, Photoshop Image Resources (PSIR) and IPTC – IIM. Each metadata container form at has unique rules regarding how metadata properties must be stored, ordered and encoded within the container. Digital Image Metadata Standards are governed by organizations that develop the following standards. They include, but are not limited to: •	 IPTC Information Interchange Model IIM (International Press Telecommunications Council), •	 IPTC Core Schema for XMP •	 XMP – Extensible Metadata Platform (an ISO standard) •	 Exif – Exchangeable image file format, Maintained by CIPA (Camera & Imaging Products Association) and published by JEITA (Japan Electronics and Information Technology Industries Association) •	 Dublin Core (Dublin Core Metadata Initiative – DCMI) •	 PLUS (Picture Licensing Universal System).

39
Download free eBooks at bookboon.com

Management of large sets of image data

Image Metadata - data about data

Some metadata semantic groupings, such as IPTC’s, are intended for use in specific user workflows and so. Within metadata semantic groupings, there can be dozens of individual metadata properties. Each metadata property can require data of specific types such as strings, numbers or arrays. Some metadata properties are conventionally read-only while other scan be modified by the user. Metadata properties are typically objective but some are subjective. Some useful properties, such as user ratings, have no commonly used standard storage container while others, such as copyright strings, can be stored within many containers with similar but subtly distinct semantics.me, such as Exif ’s, can be stored using multiple metadata container formats. Metadata is an essential part of image based workflows. Cameras capture device metadata while taking pictures. Operating systems and other software subsequently read metadata to build up catalogs and offer effective search capabilities. In addition to this, the user is able to enhance this workflow with their own metadata that may be stored either inside the file or within caching or database systems. In the context of consumer image- based workflows, the existence of different metadata standards leads to interoperability issues when using various devices, operating systems and software tools. Although the majority of metadata properties are unique, there are a number of properties which overlap across several metadata standards and cause interoperability issues as a consequence [36]. There are three metadata formats widely used in the industry: •	 Exif •	 IPTC-IIM •	 XMP EXIF The Exchangeable Image File Format (EXIF) is the standard for image file storage for digital still cameras. It was developed by the Japan Electronic Industry Development Association (JEIDA) as a standard way of storing images created by digital cameras as well as metadata about the images. EXIF image metadata can be stored in TIFF and JPEG format images. Oracle Multimedia supports the extraction of EXIF metadata from TIFF and JPEG file formats. IPTC–IIM The International Press Telecommunications Council-Information Interchange Model (IPTC-IIM) Version 4 is a standard developed jointly by the International Press Telecommunications Council and the Newspaper Association of America. This metadata standard is designed to capture information that is important to the activities of news gathering, reporting, and publishing. These information records are commonly referred to as IPTC tags.

40
Download free eBooks at bookboon.com

Management of large sets of image data

Image Metadata - data about data

The use of embedded IPTC tags in image file formats became widespread with the use of the Adobe Photoshop tool for image editing. IPTC metadata can be stored in TIFF and JPEG format images. Oracle Multimedia supports the extraction of IPTC metadata from TIFF and JPEG file formats. XMP The Extensible Metadata Platform (XMP) is a standard metadata format, developed by Adobe, for the creation, processing, and interchange of metadata in a variety of applications. XMP uses Resource Description Framework (RDF) technology for data modeling. XMP also defines how the data model is serialized (converted to a byte stream), and embedded within an image file. Oracle Multimedia supports the extraction of XMP metadata from GIF, TIFF, and JPEG file formats. Oracle Multimedia also supports writing XMP data packets into GIF, TIFF, and JPEG file formats. Metadata in microscopy: OME metadata Using the OME data model enables applications to support a single metadata format, rather than the multitude of proprietary formats available today [41].

Cyber Crime Innovation

Web-enabled Applications

Are you ready to do what matters when it comes to Technology?

41
Download free eBooks at bookboon.com

Data Analytics

Implementation

Big Data

.NET Implementation

Click on the ad to read more

IT Consultancy

Technology

Information Management

Social Business

Technology Advisory

Enterprise Application

Java

SAP

Cloud Computing

CRM

Enterprise Content Management SQL End-to-End Solution

Management of large sets of image data

Image Metadata - data about data

Every file format has a distinct set of metadata, stored differently. Bio-Formats processes and converts each format’s metadata structures into a standard form called the OME data model, according to the OME-XML specification. We have defined an open exchange format called OME-TIFF that stores its metadata as OME-XML. Any software package that supports OME-TIFF is also compatible with the dozens of formats listed on the Bio-Formats page, because Bio-Formats can convert your files to OMETIFF format. To facilitate support of OME-XML, OME have created a library in Java programming language for reading and writing OME-XML metadata. There are three types of metadata in Bio-Formats, call core metadata, original metadata, and OME metadata. Core metadata only includes things necessary to understand the basic structure of the pixels: image resolution; number of focal planes, time points, channels, and other dimensional axes; byte order; dimension order; color arrangement (RGB, indexed color or separate channels); and thumbnail resolution. Original metadata is information specific to a particular file format. These fields are key/value pairs in the original format, with no guarantee of cross-format naming consistency or compatibility. Nomenclature often differs between formats, as each vendor is free to use their own terminology. OME metadata is information from converted by Bio-Formats into the OME data model. Performing this conversion is the primary purpose of Bio-Formats. Bio-Formats uses its ability to convert proprietary metadata into OME-XML as part of its integration with the OME and OMERO servers – essentially, they are able to populate their databases in a structured way because Bio-Formats sorts the metadata into the proper places. This conversion is nowhere near complete or bug free, but we are constantly working to improve it. We would greatly appreciate any and all input from users concerning missing or improperly converted metadata fields. Metadata handling Dealing with more than one metadata format makes it challenging to determine the correct behaviour for handling the particular property values. The main difficulty is the evolution of metadata representations and standards where older applications are not aware of newer practices. This can happen within a standard, such as the introduction of Unicode storage for IPTC-IIM. It can also happen across standards, such as with the introduction of XMP. Inconsistent implementations across software tools, encoding requirements, as well as size limitations on metadata properties cause additional challenges. The properties described earlier have been identified as the most relevant in the consumer workflows today. However, they also serve another purpose in this document. Nearly all of them are defined in more than one metadata format, so they are good candidates for helping to understand the reconciliation issues between the various formats. In other words, if the problems for these properties are well understood, all other metadata properties can be handled accordingly [36].

42
Download free eBooks at bookboon.com

Management of large sets of image data

Image Metadata - data about data

Embedded metadata All digital files can hold a certain amount of metadata in addition to the information which actually makes up the content of the file. Adobe’s ‘File Info’ was also common, which was a sub-set of IPTC). This data is typically captured by the device at point of capture and is usually technical in nature. Most common image software applications will at least be able to read it, some will offer editing capabilities. More recently developed image formats like JPG2000 and Portable Network graphics (PNG) offer potentially more flexibility for working with embedded metadata. Embedding metadata has the advantage of protecting against loss or unavailability of a central database. For instance, a student can be given or download an image file with no need for an accompanying metadata file and still have access to valuable contextual information. However, a central database is still recommended which can perform advanced and speedy searches across collections without finding and accessing the metadata embedded within many individual files. If embedded metadata is to be used, the challenge of synchronising the two sets of metadata should be met by the overarching management system. Remember to check this functionality if purchasing a digital asset management system as additional bolt-on modules or even separate harvesters/editors may be required, depending on the file types within your collection.

AXA Global Graduate Program
Find out more and apply

43
Download free eBooks at bookboon.com

Click on the ad to read more

Management of large sets of image data

Interactive visualization (IV)

8	 Interactive visualization (IV)
Interactive visualization or interactive visualisation is a branch of graphic visualization in computer science that involves studying how humans interact with computers to create graphic illustrations of information and how this process can be made more efficient. For a visualization to be considered interactive it must satisfy two criteria: •	 Human input: control of some aspect of the visual representation of information, or of the information being represented, must be available to a human, and •	 Response time: changes made by the human must be incorporated into the visualization in a timely manner. In general, interactive visualization is considered a soft real-time task. Technological advancements are constantly increasing the size and complexity of image data resulting from high throughput microscopy. Through an easy-to-use interface, one-click, one-view concept, workflow based architecture, visualization method facilitates the linking of image data with numeric data. Such method utilizes a state-of-the-art data management tool optimized for fast visualization of large scale image data sets. Interactive image data exploration in workflow environment High dimensionality of data sets makes it difficult to find interesting patterns. To cope with high dimensionality, low dimensional projections of the original data set are generated; human perceptual skills are more effective in 2D or 3D displays. Mechanisms have been developed to enable researchers to select low dimensional projections, but most are static or do not allow users to make one view frame and crosslink what is interesting to them. Software applications based on those mechanisms require from users many independent clicks which in consequence change content of main view panel (Fig. 11 A, B). There is, therefore, a need for a mechanism that allows users to choose the property of projections that they are interested in, rapidly examine projections, make filtering and locate interesting view panels to detect patterns, clusters, or outliers [65].

44
Download free eBooks at bookboon.com

Management of large sets of image data

Interactive visualization (IV)

Figure 11. A and B – traditional analytics scenarios for image based data sets. Multiple clicks and distributed views. C. Interactive method: use one view and with one click scientist should get access to all related data sources and analytical tools: images, plots, numerical results, filters. The concept is focus to reduce number of clicks – access to all information should be direct and user actions should minimized with maximum of interactions. D. Star model as software methodology for interactive visualization tools.

In IV data should be visualized at several stages of analysis and linked to raw image data keeping concept of star model (Fig. 11 C, D): one-click, one-view. Unmodified and transformed data sets can be plotted interactively as scatter plots, displayed in histograms, viewed in image viewer/editor or viewed as tables. Entire experiments can be displayed in various overview plots in the context of how they are annotated, and figures and tables can be exported for publication. Data can also be exported for custom analyses (for example, for algorithms that are very expensive of computer power and time) and local development of new analysis methods, and in various defined formats for use in external post analysis applications. Data can be visualized at several stages of analysis. Unmodified images and extracted results data sets can be plotted interactively as scatter plots, displayed in histograms, or viewed as tables (Fig. 12, 13). In IV different types of visualizations can be shown simultaneously. They can be linked to each other, and may or may not be updated dynamically when the corresponding filters on the page are manipulated.

45
Download free eBooks at bookboon.com

Management of large sets of image data

Interactive visualization (IV)

Figure 12. Screenshot of Nauru interactive visualization platform

IV has integrated interactive table which is directly link to plot and filters. The table provide also information about current amount of rows, selected row. In the table user may only display selected (in plot, report tables) rows. Table can be exported.

46
Download free eBooks at bookboon.com

Click on the ad to read more

Management of large sets of image data

Interactive visualization (IV)

Figure 13. Interactive panels to get detail information and images.

IV systems are designed to help users in managing large amount of image data. IV are proposed to be: -- Used as local database system IV can read unlimited amount of images from file storage, manage & filter them, search, interactive visualize and save search setting as profile in project. IV can export desired image collection into new location. -- Used as image storage miner IV can control and monitor amount of images and folders created on hard drive or network share drive. IV can provide full report as a table about current storage conditions: amount of files, folders, storage size risk. IM can export as table in one view entire image storage. IV can inform users about different type of image data, size, duplicates, creation date, creation by, last access. IM provide time based report: amount of images – by year, month day & time statistics -- Used as visual analytics tool IV can visual analyse data by providing interactive plots (scatter, bar, line, pie) and filter mechanism connected to report tables -- Used for interactive image data labelling and teaching module for machine learning

47
Download free eBooks at bookboon.com

Management of large sets of image data

Interactive visualization (IV)

IV can be used for interactive labelling of images by generating in data table a new column with new class. Interactive labelling is used very often for manual data annotation, extending table with image metadata or for preparation of the database for classification using machine learning algorithms . IV do not require programming. The visual, dashboard, icon-based tools support every step of the image handling. The highly responsive interface adapts to the size of data you are processing. Possible usage scenario of interactive visualization tools: •	 Find similar images on image storage (file system or databases) •	 Find identical images on image storage •	 Create a full table of images existing on storage -> export to excel or text file for further analysis using external tools •	 Create a full table of images existing on storage and expand the table by image metadata and parameter -> export to excel or text file for further analysis using external tools •	 Analyse images by content using image metadata •	 Check statistics for different type of images: amount of images for each type •	 Check which folder on my storage contains largest collection of images •	 Check which folder contain largest images •	 Find using scatter plot a group of images of interest: search for patterns •	 Find using scatter plot largest images on storage •	 Search for images using multiparamteric filters •	 Control over time production of images on storage •	 Batch image processing (change size, color, threshold, type: 8; 16; 32 bits, format, resolution, …) -> export processed images to new folder •	 Label images by creating new column in the table •	 Find images by selecting them in scatter plot -> navigate to folders, files •	 Save current imported images, current view, current filter settings, selected images as project and come back to same setting loading the project again. Available software for IV: Image Manager [40] Download and user guide: http://85.214.140.42/images/pdf/IMAGE_MANAGER_USER_GUIDE_1.0_annot.pdf

48
Download free eBooks at bookboon.com

Management of large sets of image data

Basic of image processing

9	 Basic of image processing
Image processing is any form of processing for which the input is an image, the output of image processing may be either an image or a set of characteristics or parameters related to the image. Most image-processing techniques involve treating the image as a two-dimensional signal and applying standard signal-processing techniques to it [16]. Digital Image Analysis is when a computer or electrical device automatically studies an image to obtain useful information from it. Note that the device is often a computer but may also be an electrical circuit, a digital camera or a mobile phone. The applications of digital image analysis are continuously expanding through all areas of science and industry, including: οο medicine, such as detecting cancer in an MRI scan. οο microscopy, such as counting the germs in a swab. οο remote sensing, such as detecting intruders in a house, and producing land cover/land use maps. οο astronomy, such as calculating the size of a planet. οο materials science, such as determining if a metal weld has cracks. οο machine vision, such as to automatically count items in a factory conveyor belt. οο security, such as detecting a person’s eye color or hair color. οο robotics, such as to avoid steering into an obstacle. οο optical character recognition, such as automatic license plate detection. οο assay micro plate reading, such as detecting where a chemical was manufactured. οο metallography, such as determining the mineral content of a rock sample. οο defense οο filtering Image processing refers to processing of a 2D picture by a computer. Basic definitions: An image defined in the “real world” is considered to be a function of two real variables, for example, a(x,y) with a as the amplitude (e.g. brightness) of the image at the real coordinate position (x,y). Modern digital technology has made it possible to manipulate multi-dimensional signals with systems that range from simple digital circuits to advanced parallel computers. The goal of this manipulation can be divided into three categories: •	 Image Processing (image in -> image out) •	 Image Analysis (image in -> measurements out) •	 Image Understanding (image in -> high-level description out)

49
Download free eBooks at bookboon.com

Management of large sets of image data

Basic of image processing

An image may be considered to contain sub-images sometimes referred to as regions-of-interest, ROIs, or simply regions. This concept reflects the fact that images frequently contain collections of objects each of which can be the basis for a region. In a sophisticated image processing system it should be possible to apply specific image processing operations to selected regions. Thus one part of an image (region) might be processed to suppress motion blur while another part might be processed to improve color rendition [16]. Image analysis is primarily data reduction process. As we have seen, images contain enormous amount of data, typically on the order hundreds of kilobytes or even megabytes. Often much of this information is not necessary to solve a specific computer imaging problem, so primary part of the image analysis task is to determine exactly what information is necessary. Image analysis is used both computer vision and image processing. The image analysis process can be broken down into three primary stages: System Model •	 Preprocessing •	 Data Reduction •	 Features Analysis

I’M WITH ZF. ENGINEER AND EASY RIDER.
www.im-with-zf.com

CH ARLES JENKIN

S

Scan the code and find out more about me and what I do at ZF:

Quality Engineer ZF Friedrichshafen

AG

50
Download free eBooks at bookboon.com

Click on the ad to read more

Management of large sets of image data

Basic of image processing

Preprocessing: Is used to remove noise and eliminate irrelevant, visually unnecessary information. Noise is unwanted information that can result from the image acquisition process, other preprocessing steps might include: •	 Gray-level or spatial quantization (reducing the number of bits per pixel or the image size) •	 Finding regions of interest for further processing. •	 Extracting regions of interest •	 Image Representation •	 Performing basic algebraic operation on image •	 Enhancing specific image features •	 Reducing data in resolution and brightness •	 Sampling •	 Scaling / Resampling •	 Slicing of images •	 Lens correction •	 Image orientation •	 Cropping an image •	 Image size alteration •	 Layers •	 Lookup tables •	 Changing Contrast •	 Scaling image down •	 Scaling image up •	 Red-Eye Reduction •	 Color Temperature Preprocessing is a stage where the requirements are typically obvious and simple, such as removal of artifacts from images or eliminating of image information that is not required for the application. For example, in one application we needed to eliminate borders from the images that have been digitized from film. Another example of preprocessing step involves a robotics gripper that needs to pick and place an object ; for this we reduce a gray-level image to binary (two valued) image that contains all the information necessary to discern the object ‘s outlines. Sampling When measuring the value for a pixel, one takes the average color of an area around the location of the pixel. A simplistic model is sampling a square, this is called a box filter, a more physically accurate measurement is to calculate a weighted Gaussian average (giving the value exactly at the pixel coordinates a high weight, and lower weight to the area around it). When perceiving a bitmap image the human eye should blend the pixel values together, recreating an illusion of the continuous image it represents.

51
Download free eBooks at bookboon.com

Management of large sets of image data

Basic of image processing

Scaling / Resampling Is used to create an image with different dimensions from what we have we scale the image. A different name for scaling is resampling, when resampling algorithms try to reconstruct the original continous image and create a new sample grid. Slicing of images A more recent tool in digital image editing software is the image slicer. Parts of images for graphical user interfaces or web pages are easily sliced, labeled and saved separately from whole images so the parts can be handled individually by the display medium. This is useful to allow dynamic swapping via interactivity or animating parts of an image in the final presentation. Lens correction Image manipulation packages have functions to correct images for various lens distortions including pincushion, fisheye and barrel distortions. The corrections are in most cases subtle, but can improve the appearance of some photographs. Image orientation Image orientation (from left to right): original, -30° CCW rotation, and flipped.

If it really matters, make it happen – with a career at Siemens.

siemens.com/careers

52
Download free eBooks at bookboon.com

Click on the ad to read more

Management of large sets of image data

Basic of image processing

Image editors are capable of altering an image to be rotated in any direction and to any degree. Mirror images can be created and images can be horizontally flipped or vertically flopped. A small rotation of several degrees is often enough to level the horizon, correct verticality (of a building, for example), or both. Rotated images usually require cropping afterwards, in order to remove the resulting gaps at the image edges. Cropping an image Digital editors are used to crop images. Cropping creates a new image by selecting a desired rectangular portion from the image being cropped. The unwanted part of the image is discarded. Image cropping does not reduce the resolution of the area cropped. Best results are obtained when the original image has a high resolution. A primary reason for cropping is to improve the image composition in the new image. Image size alteration Image editors can resize images in a process often called image scaling, making them larger, or smaller. High image resolution cameras can produce large images which are often reduced in size for Internet use. Image editor programs use a mathematical process called resampling to calculate new pixel values whose spacing is larger or smaller than the original pixel values. Images for Internet use are kept small, say 640 × 480 pixels which would equal 0.3 megapixels. Layers Another feature common to many graphics applications is that of Layers, which are analogous to sheets of transparent acetate (each containing separate elements that make up a combined picture), stacked on top of each other, each capable of being individually positioned, altered and blended with the layers below, without affecting any of the elements on the other layers. This is a fundamental workflow which has become the norm for the majority of programs on the market today, and enables maximum flexibility for the user while maintaining non-destructive editing principles and ease of use. Lookup tables In order to see the image on the computer monitor, the image pixel values must be mapped, one-to-one, to screen pixel values via a Look Up Table or LUT. A transfer function determines what screen values correspond to image pixel intensity values at all coordinates in the image. A LUT is used to transform the input data into a more desirable output format. For example, a grayscale picture of the planet Saturn will be transformed into a color image to emphasize the differences in its rings. A classic example of reducing run-time computations using lookup tables is to obtain the result of a trigonometry calculation, such as the sine of a value. Calculating trigonometric functions can substantially slow a computing application. The same application can finish much sooner when it first precalculates the sine of a number of values, for example for each whole number of degrees (The table can be defined as static variables at compile time, reducing repeated run time costs).

53
Download free eBooks at bookboon.com

Management of large sets of image data

Basic of image processing

Changing Contrast The contrast, or total range of gray values from black to white, can be modified by changing the way the image values are mapped to the screen values, or by increasing the breadth of intensity values to cover the full range from 0 to 255. By changing the transfer function and the subsequent LUT, image contrast can be modified. Mapping image values to a broader range of screen values can enhance images that consist mainly of low intensity values. To do so, the transfer function (curve) would have a greater slope in the area of low pixel values and a lower slope in high image intensity values; thus stretching the limited source intensity values to a broader range of output values. Scaling image down The process of reducing the raster dimensions is called decimation, this can be done by averaging the values of source pixels contributing to each output pixel. Scaling image up When we increase the image size we actually want to create sample points between the original sample points in the original raster, this is done by interpolation the values in the sample grid, effectivly guessing the values of the unknown pixels. Red-Eye Reduction A method of reducing or eliminating red-eye from flash photographs by using a short burst of light, or pre-flash, to momentarily “stop-down” the pupils of the subject’s eyes prior to the actual flash exposure. Some cameras have a built-in pre-flash that fires several times to coax the pupils into contracting, before making the final flash and image capture. Red-eye can also be eliminated electronically after the fact in many photo-editing programs. Many digital cameras contain software applications that electronically eliminate red eye in camera [64]. Color Temperature A linear scale for measuring the color of ambient light with warm (yellow) light measured in lower numbers and cool (blue) light measured in higher numbers. Measured in terms of “degrees Kelvin,”* daylight (midday) is approximately 5600-degrees Kelvin, a candle is approximately 800 degrees, an incandescent lamp is approximately 2800 degrees, a photoflood lamp is 3200 to 3400 degrees, and a midday blue sky is approximately 10,000-degrees Kelvin [64].

54
Download free eBooks at bookboon.com

Management of large sets of image data

Basic of image processing

Image Reduction Image reduction allows you to clean up images to better reveal the data in them. You can in effect take pictures of some of the noise in your imaging equipment, and then use those pictures to remove much of the noise. Noise is the error in the brightness levels in an image. Noise can occur from a variety of sources. Some noise is random and unpredictable. Such noise can be limited, but never removed. This type of noise is defined as the level of uncertainty in brightness levels. For example, if you took four images of an object, you might see brightness levels of 98, 97, 103, and 102. That variation in brightness reflects the error level. Image Enhancement Image enhancement is the process of adjusting digital images so that the results are more suitable for display or further image analysis. For example, you can remove noise, sharpen, or brighten an image, making it easier to identify key features. Enhancement processes improve scanning quality but their use raises concerns about fidelity and authenticity. Many institutions argue against enhancing master images, limiting it to access files only. Typical enhancement features in scanner software or image editing tools include descreening, despeckling, deskewing, sharpening, use of custom filters, and bit-depth adjustment. Here are several examples of image enhancement processes.

www.sylvania.com

We do not reinvent the wheel we reinvent light.
Fascinating lighting offers an infinite spectrum of possibilities: Innovative technologies and new markets provide both opportunities and challenges. An environment in which your expertise is in high demand. Enjoy the supportive working atmosphere within our global group and benefit from international career paths. Implement sustainable ideas in close cooperation with other specialists and contribute to influencing our future. Come and join us in reinventing light every day.

Light is OSRAM

55
Download free eBooks at bookboon.com

Click on the ad to read more

Management of large sets of image data

Basic of image processing

The aim of image enhancement is to improve the interpretability or perception of information in images for human viewers, or to provide ‘better’ input for other automated image processing techniques. Image enhancement techniques can be divided into two broad categories: •	 Spatial domain methods, which operate directly on pixels, and •	 Frequency domain methods, which operate on the Fourier transform of an image Methods of image enhancement (filters): •	 Filtering with morphological operators •	 Histogram equalization •	 Noise removal using a Wiener filter •	 Linear contrast adjustment •	 Median filtering •	 Unsharp mask filtering •	 Decorrelation stretch Filtering with morphological operators Morphological filtering is a theory developed in the 1960s for the analysis and processing of images. It defines a series of operators which transform an image by probing it with a predefined shape element. The way this shape element intersects the neighborhood of a pixel determines the result of the operation. Eroding and dilating images using morphological filters As with all other morphological filters, the two filters of this recipe operate on the set of pixels (or neighborhood) around each pixel, as defined by the structuring element. Recall that when applied to a given pixel, the anchor point of the structuring element is aligned with this pixel location, and all pixels intersecting the structuring element are included in the current set. Erosion replaces the current pixel with the minimum pixel value found in the defined pixel set. Dilation is the complementary operator, and it replaces the current pixel with the maximum pixel value found in the defined pixel set. Since the input binary image contains only black (0) and white (255) pixels, each pixel is replaced by either a white or black pixel. Histogram equalization Grey-level or image histograms provide a variety of useful information about the intensity or brightness of a digital image. In a typical histogram, the pixels are quantified for each grey level of an 8-bit image. The horizontal axis is scaled from 0 to 255 and the number of pixels representing each grey level is graphed on the vertical axis. Statistical manipulation of the histogram data allows the comparison of images in terms of their contrast and intensity.

56
Download free eBooks at bookboon.com

Management of large sets of image data

Basic of image processing

Histogram equalization is a method in image processing of contrast adjustment using the image’s histogram. This method usually increases the global contrast of many images, especially when the usable data of the image is represented by close contrast values. Through this adjustment, the intensities can be better distributed on the histogram. This allows for areas of lower local contrast to gain a higher contrast. Histogram equalization accomplishes this by effectively spreading out the most frequent intensity values. Histogram equalization maximize the image contrast by applying a gray level transform which tries to flatten the resulting histogram. It turns out that the gray level transform that we are seeking is simply a scaled version of the original image’s cumulative histogram. Noise removal using a Wiener filter Wiener filter is a filter used to produce an estimate of a desired or target random process by linear time-invariant filtering an observed noisy process, assuming known stationary signal and noise spectra, and additive noise. Wiener filter is use to deblur an image. Wiener filter can be used effectively when the frequency characteristics of the image and additive noise are known, to at least some degree. In the absence of noise, the Wiener filter reduces to the ideal inverse filter. Unsharp mask filtering (Spatial filter) The unsharped mask is then combined with the negative image, creating an image that is less blurry than the original. The resulting image, although clearer, may be a less accurate representation of the image’s subject. Typically three settings control digital unsharp masking: Amount is listed as a percentage, and controls the magnitude of each overshoot (how much darker and how much lighter the edge borders become). This can also be thought of as how much contrast is added at the edges. It does not affect the width of the edge rims. Radius affects the size of the edges to be enhanced or how wide the edge rims become, so a smaller radius enhances smaller-scale detail. Higher Radius values can cause halos at the edges, a detectable faint light rim around objects. Fine detail needs a smaller Radius. Radius and Amount interact; reducing one allows more of the other. Threshold controls the minimum brightness change that will be sharpened or how far apart adjacent tonal values have to be before the filter does anything. This lack of action is important to prevent smooth areas from becoming speckled. The threshold setting can be used to sharpen more-pronounced edges, while leaving subtler edges untouched. Low values should sharpen more because fewer areas are excluded. Higher threshold values exclude areas of lower contrast.

57
Download free eBooks at bookboon.com

Management of large sets of image data

Basic of image processing

Linear contrast adjustment The Contrast Adjustment block adjusts the contrast of an image by linearly scaling the pixel values between upper and lower limits. Pixel values that are above or below this range are saturated to the upper or lower limit value, respectively. Median filtering In image processing, it is often desirable to be able to perform some kind of noise reduction on an image. The median filter is a nonlinear digital filtering technique, often used to remove noise. Such noise reduction is a typical pre-processing step to improve the results of later processing (for example, edge detection on an image). Median filtering is very widely used in digital image processing because, under certain conditions, it preserves edges while removing noise [35]. Decorrelation stretch Decorrelation techniques can be used to enhance or stretch, colour differences found in each pixel of an image. Decorrelation stretching enhances the color separation of an image with significant band-band correlation. The exaggerated colors improve visual interpretation and make feature discrimination easier. Decorrelation Stretch is use to remove the high correlation commonly found in multispectral data sets and to produce a more colorful color composite image. The highly correlated data sets often produce quite bland color images. Decorrelation stretching requires three bands for input. These bands should be stretched byte data or may be selected from an open color display. Image Analysis (image in -> measurements out) might include: οο Segmentation separates an image into subregions οο Simple intensity thresholding – several methods (e.g. Otsu) to estimate threshold οο Spot/particle detection – intensity, size and shape οο Edge detection (e.g. Sobel) & Morphological image processing* (erosion, dilation) οο Watershed calculation, Voronoi diagram, Ultimate eroded points Image segmentation is the process of partitioning a digital image into multiple segments (sets of pixels, also known as superpixels). The goal of segmentation is to simplify and/or change the representation of an image into something that is more meaningful and easier to analyze. Image segmentation is typically used to locate objects and boundaries (lines, curves, etc.) in images. More precisely, image segmentation is the process of assigning a label to every pixel in an image such that pixels with the same label share certain visual characteristics.

58
Download free eBooks at bookboon.com

Management of large sets of image data

Basic of image processing

The result of image segmentation is a set of segments that collectively cover the entire image, or a set of contours extracted from the image (see edge detection). Each of the pixels in a region are similar with respect to some characteristic or computed property, such as color, intensity, or texture. Adjacent regions are significantly different with respect to the same characteristic(s). Example of image segmentation method using imageJ software (Fig. 14): 1)	 Process->Subtract Background->Rolling Ball 2)	 Image Calculator (original image – duplicate image with 250 mean blur = image used in next step) 3)	 Guassian blur of 2 4)	 Autothreshold/binarize 5)	 Watershed 6)	 Particle Analyzer

At Navigant, there is no limit to the impact you can have. As you envision your future and all the wonderful rewards your exceptional talents will bring, we offer this simple guiding principle: It’s not what we do. It’s how we do it.

Impact matters.
navigant.com

©2013 Navigant Consulting, Inc. All rights reserved. Navigant Consulting is not a certified public accounting firm and does not provide audit, attest, or public accounting services. See navigant.com/licensing for a complete listing of private investigator licenses.

59
Download free eBooks at bookboon.com

Click on the ad to read more

Management of large sets of image data

Basic of image processing

Figure 14. Segmentation of human cells

Particle analysis Automatic particle analysis requires a “binary”, black and white, image. A threshold range is set to tell the objects of interest apart from the background. All pixels in the image whose values lie under the threshold are converted to black and all pixels with values above the threshold are converted to white, or vice-versa. Voronoi diagram Voronoi diagram is a way of dividing space into a number of regions. A set of points (called seeds, sites, or generators) is specified beforehand and for each seed there will be a corresponding region consisting of all points closer to that seed than to any other. The regions are called Voronoi cells. Thresholding The simplest method of image segmentation is called the thresholding method. This method is based on a clip-level (or a threshold value) to turn a gray-scale image into a binary image. The key of this method is to select the threshold value (or values when multiple-levels are selected). Several popular methods are used in industry including the maximum entropy method, Otsu’s method (maximum variance), and k-means clustering.

60
Download free eBooks at bookboon.com

Management of large sets of image data

Basic of image processing

Edge detection Edge detection is a well-developed field on its own within image processing. Region boundaries and edges are closely related, since there is often a sharp adjustment in intensity at the region boundaries. Edge detection techniques have therefore been used as the base of another segmentation technique. The edges identified by edge detection are often disconnected. To segment an object from an image however, one needs closed region boundaries. The desired edges are the boundaries between such objects. Watershed transformation The watershed transformation considers the gradient magnitude of an image as a topographic surface. Pixels having the highest gradient magnitude intensities (GMIs) correspond to watershed lines, which represent the region boundaries. Water placed on any pixel enclosed by a common watershed line flows downhill to a common local intensity minimum (LIM). Pixels draining to a common minimum form a catch basin, which represents a segment. Data Reduction: Involves either reducing the data in the spatial domain or transforming it into another domain called the frequency domain , and then extraction features for the analysis process. Features Analysis: The features extracted by the data reduction process are examine and evaluated for their use in the application. After preprocessing we can perform segmentation on the image in the spatial domain or convert it into the frequency domain via a mathematical transform. After these processes we may choose to filter the image. This filtering process further reduces the data and allows us to extract the feature that we may require for analysis.

61
Download free eBooks at bookboon.com

Management of large sets of image data

Image Processing software

10	 Image Processing software
Automated image processing Free tools: οο ImageJ / Fiji – versatile 2D+ image analysis tool with many plugins ImageJ is a public domain, Java-based image processing program developed at the National Institutes of Health. ImageJ was designed with an open architecture that provides extensibility via Java plugins and recordable macros Custom acquisition, analysis and processing plugins can be developed using ImageJ’s built-in editor and a Java compiler. οο CellProfiler – quantitatively measure cell phenotypes in large datasets (+ Worm Toolbox) CellProfiler is free, open-source software designed to enable biologists without training in computer vision or programming to quantitatively measure phenotypes from thousands of images automatically. Advanced algorithms for image analysis are available as individual modules that can be placed in sequential order together to form a pipeline; the pipeline is then used to identify and measure biological objects and features in images, particularly those obtained through fluorescence microscopy.

Do you have to be a banker to work in investment banking?
Agile minds value ideas as well as experience Global Graduate Programs
Ours is a complex, fast-moving, global business. There’s no time for traditional thinking, and no space for complacency. Instead, we believe that success comes from many perspectives — and that an inclusive workforce goes hand in hand with delivering innovative solutions for our clients. It’s why we employ 135 different nationalities. It’s why we’ve taken proactive steps to increase female representation at the highest levels. And it’s just one of the reasons why you’ll find the working culture here so refreshing. Discover something different at db.com/careers

Deutsche Bank db.com/careers

62
Download free eBooks at bookboon.com

Click on the ad to read more

Management of large sets of image data

Image Processing software

οο KNIME – data analysis workflow for large datasets / batch KNIME, the Konstanz Information Miner, is an open source data analytics, reporting and integration platform. KNIME integrates various components for machine learning and data mining through its modular data pipelining concept. A graphical user interface allows assembly of nodes for data preprocessing (ETL: Extraction, Transformation, Loading), for modeling and data analysis and visualization. οο Priism/IVE, Priithon & Editor – 2D image processing/analysis (DV/OMX) οο Icy, Vaa3D, BioimageXD – 3D image visualization & analysis Icy is a free, open source image processing platform developed by the Quantitative Image Analysis Unit at Institut Pasteur, Paris, France. This platform comprises two components: a Java-based software for image processing, and a community website that gathers communitycontributed resources including third-party plug-ins. The software is available as a downloadable application for Microsoft Windows, Mac OS X and Linux running Java version 6 or later. οο Image Manager Image Manager (IM) is a free (for academic user) digital image data manager and image processing program. Using IM you can now visualize, organize, browse, process, analyse your large image repositories (files, databases) from microscope, radiology, astronomy, etc in quick, simple steps. It can read many image formats including TIFF, GIF, JPEG, BMP, DICOM, FITS and ‘raw’. It can handle unlimited amount of images in interactive user Interface. IM can be installed on every computer and can analyse image storage in real time, providing sophisticated reports οο 3DSlicer 3D Slicer (Slicer) is a free, open source software package for image analysis and scientific visualization. Slicer is used in a variety of medical applications, including autism, multiple sclerosis, systemic lupus erythematosus, prostate cancer, schizophrenia, orthopedic biomechanics, COPD, cardiovascular disease and neurosurgery. οο Analysis of Functional NeuroImages Analysis of Functional NeuroImages (AFNI) is an open-source environment for processing and displaying functional MRI data – a technique for mapping human brain activity. AFNI is an agglomeration of programs that can be used interactively or flexibly assembled for batch processing using shell script. The term AFNI refers both to the entire suite and to a particular interactive program often used for visualization. AFNI is actively developed by the NIMH Scientific and Statistical Computing Core and its capabilities are continually expanding. οο ANIMAL (image processing) ANIMAL (first implementation: 1988 – revised: 2004) is an interactive environment for image processing that is oriented toward the rapid prototyping, testing, and modification of algorithms. To create ANIMAL (AN IMage ALgebra), XLISP of David Betz was extended with some new types: sockets, arrays, images, masks, and drawables.

63
Download free eBooks at bookboon.com

Management of large sets of image data

Image Processing software

οο CellCognition CellCognition is a free open-source computational framework for quantitative analysis of highthroughput fluorescence microscopy (time-lapse) images in the field of bioimage informatics and systems microscopy. οο CellProfiler CellProfiler is free, open-source software designed to enable biologists without training in computer vision or programming to quantitatively measure phenotypes from thousands of images automatically. Advanced algorithms for image analysis are available as individual modules that can be placed in sequential order together to form a pipeline; the pipeline is then used to identify and measure biological objects and features in images, particularly those obtained through fluorescence microscopy. οο CVIPtools CVIPtools (Computer Vision and Image Processing Tools) is an Open Source image processing software. It is free for use with Windows, and previous versions are available for UNIX. It is an excellent, interactive program for image processing and computer vision οο Endrov Endrov is an open-source plugin architecture aimed for image analysis and data processing. Being based on Java, it is portable and can both be run locally and as an applet. It grew out of the need for an advanced open source software that can cope with complex spatio-temporal image data, mainly obtained from microscopes in biological research. It lends much of the philosophy from ImageJ but aims to supersede it by having a more modern design. οο OpenCV OpenCV (Open Source Computer Vision Library) is a library of programming functions mainly aimed at real-time computer vision, developed by Intel, and now supported by Willow Garage and Itseez. It is free for use under the open source BSD license. The library is cross-platform. It focuses mainly on real-time image processing. If the library finds Intel’s Integrated Performance Primitives on the system, it will use these proprietary optimized routines to accelerate itself. οο Openlab Openlab is a software package for performing 2D microscope image processing and integrating and controlling a diverse array of instrumentation in a laboratory environment. It consists of a core application which can manage large arrays of image data, and a series of separate plugin modules which implement a variety of tasks according to the needs of the end user. The customer is able to tailor the system to exactly their own requirements, but use a coherent GUI to operate the entire system. οο Netpbm Netpbm is an open source package of graphics programs and a programming library, used mainly in the Unix world. It is included in all major open source Unix-like operating system distributions and also works on other Unix-like operating systems, Windows, Mac OS X, and other platforms.

64
Download free eBooks at bookboon.com

Management of large sets of image data

Image Processing software

οο GemIdent GemIdent is an interactive image recognition program that identifies regions of interest in images and photographs. It is specifically designed for images with few colors, where the objects of interest look alike with small variation. For example, color image segmentation of: οο Oranges from a tree οο Stained cells from microscopic images οο Ginkgo CADx Ginkgo CADx is a multiplatform (Windows, Linux, Mac OS X) DICOM viewer (*.dcm) and dicomizer (convert different files to DICOM). Ginkgo CADx is licensed under LGPL license, being and open source project with an Open core approach. The goal of Ginkgo CADx project is to develop an open source professional DICOM workstation. οο IDL (programming language) IDL, short for Interactive Data Language, is a programming language used for data analysis. It is popular in particular areas of science, such as astronomy and medical imaging. IDL shares a common syntax with PV-Wave and originated from the same codebase, though the languages have subsequently diverged in detail. οο Ilastik ilastik is a user-friendly free open source software for image classification and segmentation. No previous experience in image processing is required to run the software.

Real drive. Unreal destination.

As an intern, you’re eager to put what you’ve learned to the test. At Ernst & Young, you’ll have the perfect testing ground. There are plenty of real work challenges. Along with real-time feedback from mentors and leaders. You’ll also get to test what you learn. Even better, you’ll get experience to learn where your career may lead. Visit ey.com/internships. See More | Opportunities

© 2012 Ernst & Young LLP. All Rights Reserved.

65
Download free eBooks at bookboon.com

Click on the ad to read more

Management of large sets of image data

Image Processing software

οο Image SXM Image SXM is an image analysis software specialized in scanning microscope images. It is based on the public domain NIH Image (now ImageJ from the National Institutes of Health) and extended to handle scanning microscope images, especially of the SxM formats (SAM, SCM, SEM, SFM, SLM, SNOM, SPM, STM), hence its name. οο ImageNets ImageNets is an open source and platform independent (Windows & Linux) framework for rapid prototyping of Machine Vision algorithms. With the GUI ImageNet Designer, no programming knowledge is required to perform operations on images. οο Insight Segmentation and Registration Toolkit ITK is a cross-platform, open-source application development framework widely used for the development of image segmentation and image registration programs. Segmentation is the process of identifying and classifying data found in a digitally sampled representation. Typically the sampled representation is an image acquired from such medical instrumentation as CT or MRI scanners. οο Integrated Software for Imagers and Spectrometers Integrated Software for Imagers and Spectrometers, also abbreviated to Isis, is a specialized software package developed by the USGS to process images and spectra collected by current and past NASA planetary missions sent to Earth’s Moon, Mars, Jupiter, Saturn, and other solar system bodies. οο Integrating Vision Toolkit The Integrating Vision Toolkit (IVT) is a powerful and fast C++ computer vision library with an easy-to-use object-oriented architecture. It offers its own multi-platform GUI toolkit. οο InVesalius InVesalius is a free medical software used to reconstruct structures of the human body. Based on two-dimensional images, acquired using Computed tomography or Magnetic resonance imaging equipment, the software generates virtual three-dimensional models correspondent to anatomical parts of the human body. After reconstructing three-dimensionally DICOM images, the software allows the generation of STL (stereolithography) files. These files can be used for Rapid Prototyping. οο ITK-SNAP ITK-SNAP is an interactive software application that allows users to navigate three-dimensional medical images, manually delineate anatomical regions of interest, and perform automatic image segmentation. The software was designed with the audience of clinical and basic science researchers in mind, and emphasis has been placed on having a user-friendly interface and maintaining a limited feature set to prevent feature creep. ITK-SNAP is most frequently used to work with magnetic resonance imaging (MRI) and computed tomography (CT) data sets.

66
Download free eBooks at bookboon.com

Management of large sets of image data

Image Processing software

οο Mango (software) Mango (Multi-Image Analysis GUI) is a non-commercial software for viewing, editing and analyzing volumetric medical images. Mango is written in Java, and distributed freely in precompiled versions for Linux, Mac OS and Microsoft Windows. It supports NIFTI, ANALYZE, NEMA and DICOM formats, and is able to load and save 2D, 3D and 4D images. Mango provides tools for creation and editing of regions of interest (ROI) within the images, surface rendering, image stacking (overlaying), filtering in space domain and histogram analysis, among other functions that can be used in neuroimaging analysis for scientific (non-clinical) purposes. οο MicroDicom MicroDicom is a free DICOM viewer for Windows. MicroDicom is application for primary processing and preservation of medical images in DICOM format. You can open DICOM images produced by medical equipment (MRI, PET, CT,…). You can also open other image formats – BMP, GIF, JPEG, PNG, TIFF,…. It is equipped with most common tools for manipulation of DICOM images and it has an intuitive user interface. It also has the advantage of being free for use and accessible to everyone. οο NeatVision Neatvision was a free for non-commercial use Java based image analysis and software development environment, which provides high level access to a wide range of image processing algorithms through well defined and easy to use graphical interface. NeatVision contains over 290 image manipulation, processing and analysis algorithms. Users can extend the core NeatVision library using the developers interface, a plug-in which features, automatic source code generation, compilation with full error feedback and dynamic algorithm updates. οο Scribe Scribe is software used for automatic book scanning and image processing. οο VIGRA VIGRA is the abbreviation for “Vision with Generic Algorithms”. It is a free open source computer vision library which focuses on customizable algorithms and data structures. VIGRA component can be easily adapted to specific needs of target application without compromising execution speed, by using template techniques similar to those in the C++ Standard Template Library. οο VIPS (software) VIPS is an open source image processing software package. It is particularly good with large images, works with multi-core processors, working with colour, scientific analysis and general research & development. It was developed during and is the product of several European research projects (VASARI, MARC, ACOHIR, Viseum) which were primarily about Imaging art, but which demanded a new approach to image processing.

67
Download free eBooks at bookboon.com

Management of large sets of image data

Image Processing software

Compared to most image processing libraries VIPS needs little RAM and runs quickly, especially on machines with more than one CPU. This is primarily due to its architecture which automatically parallelises the image workflows. οο VisualAp VisualAp is a visual framework for building applications and emulate systems. VisualAp is cross-platform as it is a 100% Java application. This application is able to perform audio processing, image processing, text and other process-driven emulation. VisualAp provides a visual framework based on lightweight visual components (proclets) that implements specific tasks. Users can extend the capabilities of VisualAp via user-written proclets. Custom analysis and processing proclets can be developed using Eclipse. οο VXL VXL, the ‘Vision-something-Library,’ is a collection of open source C++ libraries for Computer Vision. The idea is to replace X with one of many letters, i.e. G (VGL) is a geometry library, N (VNL) is a numerics library, I (VIL) is an image processing library, etc. These libraries can be used for general scientific computing as well as computer vision. Commercial tools: οο Volocity – 3D visualization & analysis package, spinning disk οο SoftWoRx – API Deltavision, deconvolution, SI reconstruction

The stuff you'll need to make a good living

STUDY. PLAY.

The stuff that makes life worth living

NORWAY. YOUR IDEAL STUDY DESTINATION.
WWW.STUDYINNORWAY.NO FACEBOOK.COM/STUDYINNORWAY

68
Download free eBooks at bookboon.com

Click on the ad to read more

Management of large sets of image data

Image Processing software

οο Matlab MATLAB (matrix laboratory) is a numerical computing environment and fourth-generation programming language. Developed by MathWorks, MATLAB allows matrix manipulations, plotting of functions and data, implementation of algorithms, creation of user interfaces, and interfacing with programs written in other languages, including C, C++, Java, and Fortran. οο Huygens, AutoQuant – Deconvolution software Huygens software refers to different multiplatform microscope image processing packages from Scientific Volume Imaging, made for restoring 2D and 3D microscopy images or time series and analyzing and visualizing them. The restoration is based on different deconvolution algorithms, that permit the recovery of objects from images that are degraded by blurring and noise. In microscopy the blurring is largely due to diffraction limited imaging by the instrument; the noise is usually photon noise. οο Nauru Nauru is digital image data manager and image processing program. Using Nauru you can now visualize, organize, browse, process, analyse your large image repositories (files, databases) from microscope, radiology, astronomy , etc in quick, simple steps. It can read many image formats including TIFF, GIF, JPEG, BMP, DICOM, FITS and ‘raw’. It can handle unlimited amount of images in interactive user interface. Nauru can be installed on every computer and can analyse image storage in real time, providing sophisticated reports. οο Amira (Software) Amira (pronounce: Ah-meer-ah) is a software platform for 3D and 4D data visualization, processing, and analysis. It is being actively developed by FEI Visualization Sciences Group, Bordeaux, France and the Zuse Institute Berlin (ZIB), Germany. οο AutoCollage 2008 AutoCollage 2008 is a Microsoft photomontage desktop application. The software creates a collage of representative elements from a set of images. It is able to detect faces and recognize objects. The software was developed by Microsoft Research labs in Cambridge, England and launched on September 4, 2008. οο Avizo (software) Avizo (pronounce: ‘a-VEE-zo’) is a general-purpose commercial software application for scientific and industrial data visualization and analysis. Avizo is developed by FEI Visualization Sciences Group and was originally designed and developed by the Visualization and Data Analysis Group at Zuse Institute Berlin (ZIB) under the name Amira. Avizo was commercially released in November 2007. οο Bitplane – Imaris Bitplane is a provider of software for 3D and 4D image analysis for the life sciences. Founded in December 1992, Bitplane operates out of three offices in Belfast, United Kingdom, Zürich, Switzerland and South Windsor, Connecticut, USA.

69
Download free eBooks at bookboon.com

Management of large sets of image data

Image Processing software

οο Bsoft Bsoft is a collection of programs and a platform for development of software for image and molecular processing in structural biology. Problems in structural biology are approached with a highly modular design, allowing fast development of new algorithms without the burden of issues such as file I/O. It provides an easily accessible interface, a resource that can be and has been used in other packages. οο CoLocalizer Pro CoLocalizer Pro is a scientific software application, developed by CoLocalization Research Software, that allows researchers to analyze colocalization in the images obtained using fluorescence microscopy. Due to high popularity of Macintosh computers in medicine and biology, it is designed specifically for Mac OS X operating system. To bring the full advantage of Mac OS X, the software is written exclusively in Cocoa. CoLocalizer Pro is used in universities and research institutions worldwide. Lite version of the software, CoLocalizer Express, is geared toward students and beginners. οο Mathematica Mathematica is a computational software program used in many scientific, engineering, mathematical and computing fields, based on symbolic mathematics. It was conceived by Stephen Wolfram and is developed by Wolfram Research of Champaign, Illinois. οο PanoramaMaker Panorama Maker is panorama software for both Microsoft Windows and Mac OS. Its basic function is to put photo with overlapped areas into the software and then it comes out to be a long slim panorama picture that naturally stitches previous photos together seamlessly. Here the long slim panorama is named as “horizontal panorama” in the program, and creating “vertical panorama”, “360 degree panorama”, “tile panorama” as long as the original photos reach the stitching requirements. The latest version supports of 3D output. οο ImageApp ImageApp is a vision image processing library, providing multiple functions to help process and obtain information from images. ImageApp is developed by Sun systems and is an open source java IDE designed for machine vision and image processing. ImageApp uses a Java image library named JAI (Java Advanced Imaging) and a graph API called JGraph. οο MeVisLab MeVisLab is a cross-platform application framework for medical image processing and scientific visualization. It includes advanced algorithms for image registration, segmentation, and quantitative morphological and functional image analysis. An IDE for graphical programming and rapid user interface prototyping is available. MeVisLab is written in C++ and uses the Qt framework for graphical user interfaces. It is available cross-platform on Windows, Linux, and Mac OS X. The software development is done in cooperation between MeVis Medical Solutions AG and Fraunhofer MEVIS.

70
Download free eBooks at bookboon.com

Management of large sets of image data

Image Processing software

οο MountainsMap MountainsMap is a surface imaging and metrology software published by the company Digital Surf. Its main application is micro-topography, the science of studying surface texture and form in 3D at the microscopic scale. The software is used mainly with stylus-based or optical profilometers, optical microscopes and scanning probe microscopes. οο PhotoModeler PhotoModeler is a software application that performs image-based modeling and close range photogrammetry – producing 3D models and measurements from photography. Close Range Photogrammetry (CRP) is a sub-set of photogrammetry and is differentiated from Aerial Photogrammetry by the type of input photographs. In CRP, photographs are taken from the ground, or from aerial positions that are at a closer range to the subject than typical Aerial Photogrammetry. οο PurVIEW PurVIEW is an integrated image display and viewing plug-in software package that incorporates stereoscopic viewing technology for ESRI ArcGIS 9 (or later version). Essentially, PurVIEW is a photogrammetry-based data capture workstation that extend the ArcGIS environment. It converts Arc- desktops into precise stereo-viewing windows for geo-referenced aerial or space-borne imagery. Digitizing features directly yields positional accuracy comparable with photogrammetric mapping.

I joined MITAS because I wanted real responsibili� I joined MITAS because I wanted real responsibili�

Maersk.com/Mitas www.discovermitas.com

�e Graduate Programme for Engineers and Geoscientists

�e G for Engine

Ma

Real work International Internationa al opportunities �ree wo work or placements

Month 16 I was a construction Mo supervisor ina const I was the North Sea super advising and the No he helping foremen advis s solve problems Real work he helping fo International Internationa al opportunities �ree wo work or placements s solve pr
Click on the ad to read more

71
Download free eBooks at bookboon.com

Management of large sets of image data

Image Processing software

οο ScanIP ScanIP is an image processing software package developed by Simpleware Ltd. ScanIP visualises and processes 3D image data from MRI, CT, or Microtomography and other modalities to create simulation-ready CAD, CFD, FE, and 3D printing models. Data can be visualised in 3D and 2D modes after import, and can be processed using powerful segmentation tools and filters to obtain regions of interest and optimise image quality. οο SOCET SET SOCET SET is a software application that performs functions related to photogrammetry. It is developed and published by BAE Systems. SOCET SET was the first commercial digital photogrammetry software program. Prior to SOCET SET, all photogrammetry programs were primarily analog or custom systems built for government agencies. οο Warpalizer Warpalizer is a professional warp and blend software application made by Univisual Technologies AB in Sweden, for use in simulators. Warpalizer can be used for projection on to surfaces that need image geometry correction (warping) to improve the image from the projector, on a cylindrical screen for example. When used with multiple projectors Warpalizer uses edge blending to display the images from the different projectors without seams. Image editing Adobe Photoshop is one of the most popular products that has an cultural significance far beyond editing images. Adobe’s recent announcement that everything beyond Photoshop CS6 will need to be rented as part of its Creative Cloud lineup has caused a fair amount of disquiet (some of which has been pretty loud), but Photoshop isn’t the only game in town, and never has been. In this article we’ll be taking a quick look at ten other pieces of image manipulation software that you might not know about, but which are well worth exploring. ACD Systems ACDSee Pro 6 and ACDSee Photo Editor οο $60 for ACDSee Pro 6; $30 for Photo Editor 6 οο Operating requirements: Windows XP SP3 or later; ACDSee Pro 3 ($85) available for Mac OS X οο www.acdsee.com/acdsee-pro-6 οο www.acdsee.com/acdsee-photo-editor-6 ACDSee Pro 6 offers RAW processing, image tagging and organization tools, and exposure/color enhancements, while Photo Editor 6 is the more-Photoshop-like tool for layer-based, pixel-level edits. ACDSee Pro 6 doesn’t offer many of the facial-recognition, geotagging, and distortion-correction whistles and bells of Lightroom and Aperture, but both pieces of software offer extensive RAW-format support out of the box. Mac users beware though – you’ll have to make do with ACDSee Pro 3 for now.

72
Download free eBooks at bookboon.com

Management of large sets of image data

Image Processing software

Adobe Photoshop Lightroom 4 οο $150 for standalone version; available as part of $50/month Creative Cloud subscription οο Operating requirements: 64-bit Macs running Mac OS X 10.6.8 or later; Windows Vista or later οο www.adobe.com Lightroom isn’t a Photoshop alternative per se. Launched as a RAW workflow tool, Adobe has been steadily updating it through four iterations (a beta version of 5 is also available) and if you don’t need to slice and dice your images too intensively, it’s a great alternative to ‘full strength’ Photoshop. If you’re a RAW shooter you may already be using Lightroom as part of your workflow, as it covers a few of Photoshop’s weakest points: organizing your photos, tagging your photos, and applying quick fixes and enhancements. If you work mainly with RAW files and need a program to quickly process your images, adjust exposure, remove noise, and apply the same adjustments to a batch of images, Lightroom may be all you need – which was precisely why Adobe created it. Lightroom 4 is available for $150 as a standalone boxed package or via a $50/month Creative Cloud subscription. (Of course, if you’re already paying $50 per month for Lightroom via a Creative Cloud subscription, you’ll have Photoshop CS6 as part of the package, too.) Apple Aperture 3 οο $80 οο Operating requirements: Mac OS X 10.7.5 or later οο itunes.apple.com/aperture Aperture is more of a Lightroom alternative than a Photoshop CS6 alternative, and if you’re a Mac user it might be just what you need. Aperture blends advanced features such as RAW processing, manual retouching, custom-printing elements, and tagging/organization tools with novice-friendly options such as facial recognition, geotagging, and one-click filters. Unlike Lightroom, Aperture does not offer built-in lens-distortion correction out of the box, but there are several Aperture plugins available on Apple’s site that offer that and many other features. Earlier versions of Aperture were notoriously system-intensive, requiring a lot of processing power to run, but Aperture 3 is much improved. At $80 it’s hard to find much to complain about.

73
Download free eBooks at bookboon.com

Management of large sets of image data

Image Processing software

Corel PaintShop Pro X5 οο $60 for Basic edition; $70 for Ultimate edition οο Operating requirements: Windows XP SP3 and later οο www.corel.com If you’re looking specifically for a boxed-software Photoshop alternative for Windows, Corel PaintShop Pro X5 is one of the most-popular packages in that realm. It’s arguably the best option in this roundup for graphic artists, as it can create vector graphics and offers interoperability with Photoshop’s own brush tools. As you’d expect, it also features a full array of photo-editing tools as well, including layers, filters, one-click HDR and other filters, retouch tools, and much more RAW-format support than any of the free packages (including 16-bit RAW). The ‘Ultimate’ edition of PaintShop Pro X5 costs just $10 more than the standard version, and it includes Nik Color Efx Pro 3.0 filters (which costs around $150 by itself, so that’s quite a deal) and additional enhancement tools for portrait photographers.

Need help with your dissertation?
Get in-depth feedback & advice from experts in your topic area. Find out what you can do to improve the quality of your dissertation!

Get Help Now

Go to www.helpmyassignment.co.uk for more info

74
Download free eBooks at bookboon.com

Click on the ad to read more

Management of large sets of image data

Image Processing software

DxO Optics Pro 8 οο $170 for Standard edition; $300 for Elite edition οο Compatible with: Windows XP (SP3) and later for 32-bit support; Windows Vista and later for 32- and 64-bit support; Mac OS X 10.6 and later οο www.dxo.com If you want automated, tailored-to-your-camera lens correction in your RAW-processing software, look no further than DxO Optics Pro. This Lightroom alternative features an extensive database of camera/lens combinations, which you can activate as ‘modules’ to automate lens-correction, chromatic aberration, sharpening, vignetting, and noise-reduction fixes. The Standard edition’s database of lens/ camera combinations is built to support everything from RAW-capable point-and-shoot cameras to consumer-level DSLRs, while the Elite edition is a better fit for those shooting with a full-frame DSLR or other professional-level kits. Optics Pro 8 takes some getting used to, but it’s an incredibly powerful tool and its lens corrections really do have to be seen to be believed. Be prepared to wait for new cameras and lens modules to be added, though DxO is getting better in this regard. GIMP 2.8 οο Free οο Compatible with: Windows XP and later; Mac OS X; Linux; Unix; BSD οο www.gimp.org GIMP is an open-source project that costs absolutely nothing. It does an admirable job of replicating Photoshop’s feature set when it comes to recomposing and manipulating your photos, applying effects, and cropping and resizing images. GIMP supports editing PSD files, and its arsenal of tools is without equal for the price: Filters; brush tools; text tools; layers; distortion and color-correction tools; and plenty of cropping, resizing, and effects options. Although it shares a surprising amount of features with the much-higher-priced Photoshop, GIMP is nowhere near as much of a resource hog. The most common gripes with GIMP are that it isn’t as polished or easy to use as Photoshop, nor does it match up to Adobe’s editing software when it comes to advanced features and color management (no 16-/32-bit RAW or CMYK support, for example). GIMP has a healthy selection of plug-ins that make its feature set even more Photoshop-like, including content-aware healing tools, extensive RAW-format support, and even a modified version that looks and acts more like Photoshop, if you get homesick.

75
Download free eBooks at bookboon.com

Management of large sets of image data

Image Processing software

Paint.net οο Free οο Compatible with: Windows XP SP3 and later οο www.getpaint.net Paint.net is a free Windows-only program that’s often mentioned alongside GIMP (it’s free, for one thing) but avid users give it an edge in terms of learning curve; if you know your way around Microsoft Paint, you should get the hang of Paint.net pretty quickly. Paint.net’s palette of basic selection and paint tools are nearly identical to those found in Microsoft Paint, but it ups the ante with Photoshop-like support for layers, filters, and effects. It’s also similar to GIMP in terms of extensive plug-in development, and those add-ons will be essential to more-advanced users. You’ll need to download and install plug-ins in order to edit PSD files and work with RAW images, for example. For basic JPEG photo edits on a Windows machine, Paint.net might be your simplest, cheapest option. Anyway, it’s free – why not? Phase One Capture One Pro 7 οο $300 οο Compatible with: Windows Vista (SP2) and later (64-bit system required); Mac OS X 10.6.8 or later οο www.phaseone.com In our Raw Converter Showdown earlier this year, Phase One’s Capture One Pro 7 emerged as the top pick for studio and fashion photographers thanks to its excellent support for tethered shooting, including in-application camera adjustments and live-view capabilities. There are plenty of reasons for enthusiast photographers to consider this RAW-processing package, too though, including excellent organization tools, speedy performance, and a unique focus-peaking preview that helps you identify the sharpest shots in your batch of photos. Like Lightroom and DxO Optics Pro, it also offers an extensive selection of noise-reduction, lens-correction, color-correction, and custom-printing tools. If its relatively high cost gives you pause, you can always download a free trial version and see how you get on.

76
Download free eBooks at bookboon.com

Management of large sets of image data

Image Processing software

Pixelmator 2.2 ‘Blueberry’ οο Normally $30, currently (May 2013) on sale for $15 οο Compatible with: Mac OS X 10.7 or later οο www.pixelmator.com Pixelmator is another full-featured editing tool, and it’s probably a safer option for Mac users thanks to its user-friendliness. Pixelmator only runs on Mac OS X, and while it isn’t free, it’s a bargain at $30 – and an even better bargain at its current (May 2013) sale price of $15. Think of Pixelmator as the anti-GIMP in terms of interface: It’s easier to use and much easier on the eyes. For basic to semi-advanced image-editing needs (color correction, brushes, layers, masks, filters, text tools, and a content-aware healing tool), it has the bases covered. Like GIMP, it also supports editing PSD files, so you can work with any projects you’ve already started in Photoshop. You won’t get everything you’ll find in Photoshop, of course: It’s more restrictive in terms of scripting/automating tasks, color management, and RAW support; basically, you’ll need to make sure your camera’s RAW files are supported by Mac OS X itself (if you’re running the latest version of OS X these updates are pretty frequent). Pixlr Editor οο Free οο Compatible with: Runs in web browser; Flash required οο www.pixlr.com You won’t get RAW support with Pixlr, and you will need Adobe Flash to make it work. If those aren’t deal-breakers for you, this in-browser editor offers an impressive amount of image-editing power without the need to download, install, or pay for anything. The Pixlr Editor offers the usual array of paint, blur, cropping, color-adjusting, and text tools, but you also have a context-aware spot-healing tool and an assortment of pre-set filters (HDR, tilt-shift, and color gradients among them) at your disposal. Along with the ability to open and edit PSD files (you can’t save as PSD, however), one of Pixlr Editor’s best features is its Google Drive integration. You can add Pixlr Editor to your list of connected Google Drive apps, allowing you to edit images from your Google Drive folder and save them to your Drive without ever leaving your browser.

77
Download free eBooks at bookboon.com

Management of large sets of image data

Image Processing software

Adobe Photoshop Elements 11 οο $99 οο Compatible with: Windows XP (SP3) and later, Mac OS X v10.6 and later. οο www.adobe.com Photoshop Elements 11 is the latest version of Adobe’s cut-down version of ‘full strength’ Photoshop, and it’s definitely the best yet. Traditionally, Elements was very much the poor cousin of its more expensive relatives, but over the past few years Adobe has been quietly and steadily adding to its feature set to the point where it’s now a very powerful editing tool in its own right. Although the interface (especially browsing) is different enough to be confusing for someone used to Photoshop CS6, Elements 11 contains almost all of the essential image browsing and manipulation features that photographers need. There are still limitations, but far fewer than there were in the past. For a breakdown of the differences between Elements and Photoshop CS6, this page on Adobe’s help forums is pretty comprehensive.

78
Download free eBooks at bookboon.com

Click on the ad to read more

Management of large sets of image data

Image management and image database

11	 Image management and image databases
An image database is a computer system for browsing, searching and retrieving images from a large file system of digital images. Most traditional and common methods of image databases utilize some method of adding metadata such as captioning’, keywords, or descriptions to the images so that retrieval can be performed over the annotation words. Manual image annotation is time-consuming, laborious and expensive; to address this, there has been a large amount of research done on automatic image annotation. To search for images, a user may provide query terms such as keyword, image file/link, or click on some image, and the system will return images “similar” to the query. The similarity used for search criteria could be meta tags, color distribution in images, region/shape attributes. Image databases are also known as query by image content is the application of computer vision techniques to the image retrieval problem, that is, the problem of searching for digital images in large file systems Content-based image retrieval is opposed to concept-based approaches. Image database means that the search analyzes the contents of the image rather than the metadata such as keywords, tags, or descriptions associated with the image. The term “content” in this context might refer to colors, shapes, textures, or any other information that can be derived from the image itself. Database management systems (DBMSs) are specially designed software applications that interact with the user, other applications, and the database itself to capture and analyze data. A general-purpose DBMS is a software system designed to allow the definition, creation, querying, update, and administration of databases. Well-known DBMSs include MySQL, MariaDB, PostgreSQL, SQLite, Microsoft SQL Server, Oracle, SAP, dBASE, FoxPro, IBM DB2, LibreOffice Base and FileMaker Pro. A database is not generally portable across different DBMS, but different DBMSs can interoperate by using standards such as SQL and ODBC or JDBC to allow a single application to work with more than one database [12].

79
Download free eBooks at bookboon.com

Management of large sets of image data

Image management and image databases

Databases vs File system There are applications that manage many terabytes of images. It is confirmed that storing file paths in the database to be best. Benefits to store the images in the database: οο You are storing images that are changing dynamically, eg. invoices and you wanted to get an invoice from specific date? οο The government wants you to maintain 6 years of history οο Images stored in the database do not require a different backup strategy. Images stored on filesystem do οο It is easier to control access to the images if they are in a database. Any administrator can access any folder on disk. It takes a really determined administrator to go snooping in a database to extract the images οο databases win out where transactional integrity between the image and metadata are important. οο it is more complex to manage integrity between database metadata and file system data οο it is difficult (within the context of a web application) to guarantee data has been flushed to disk on the filesystem There are a couple of issues to store images in database: •	 database storage is usually more expensive than file system storage •	 you can super-accelerate file system access with standard off the shelf products οο for example, many web servers use the operating system’s system call to asynchronously send a file directly from the file system to the network interface. Images stored in a database don’t benefit from this optimization. •	 Require additional code to extract and stream the images •	 Latency may be slower than direct file access •	 Heavier load on the database server •	 web servers need no special coding or processing to access images in the file system

80
Download free eBooks at bookboon.com

Management of large sets of image data

Image management and image databases

Available Image Databases: Biological Databases The vast amounts of experiments, images, metadata, and extractable features in systems biology require relational databases. In HCS, there is an intrinsic need for user-friendly, scalable, and powerful information management systems. Data management platforms should enable users to collect, integrate, share, and publish data. In the scope of interoperability, these platforms should also be able to connect to data processing pipelines and workflow systems. The benefit of using open source databases is extendibility and the possibility of platform customization.

Brain power

By 2020, wind could provide one-tenth of our planet’s electricity needs. Already today, SKF’s innovative knowhow is crucial to running a large proportion of the world’s wind turbines. Up to 25 % of the generating costs relate to maintenance. These can be reduced dramatically thanks to our systems for on-line condition monitoring and automatic lubrication. We help make it more economical to create cleaner, cheaper energy out of thin air. By sharing our experience, expertise, and creativity, industries can boost performance beyond expectations. Therefore we need the best employees who can meet this challenge!

The Power of Knowledge Engineering

Plug into The Power of Knowledge Engineering. Visit us at www.skf.com/knowledge

81
Download free eBooks at bookboon.com

Click on the ad to read more

Management of large sets of image data

Image management and image databases

The Bio-Image Semantic Query User Environment (Bisque) [7] was developed for the exchange and exploration of biological images. The Bisque system supports several areas from image capture to image analysis and query. This platform is centered on a database of images and metadata. The integrated analysis tools allow high-level semantic queries to be made as well as comparisons of image content. Bisque was specifically designed to provide researchers with organizational and quantitative analysis tools for time-resolved multichannel 3D screening data. Images and metadata are organized with tags (i.e., name – value pairs) associated with an image. Typically, users locate images of interest by browsing through collections or searching with specific queries. The system has an integrated web image browser for the filtering, sorting, and ordering of images. The image organizer performs advanced sorting by hierarchical tag ordering. In addition, users can extend Bisque with data model and analysis extensions in order to adapt the system to local needs. The extensibility of Bisque stems from the following 2 core concepts: flexible metadata facility and an open web-based architecture. The Open Microscopy Environment (OME) [41] project leverages imaging projects by focusing on the underlying need for common file formats. OME provides Bio-Formats, a tool that fully parses more than 120 proprietary image formats and converts proprietary metadata to the OME-XML data model. The OME-TIFF format is a container format for Tiff images with OME-XML metadata and the most widely used image format in community-driven projects. To ensure data integrity, Bio-Formats converts the proprietary file format metadata into a table of key-value pairs that is subsequently stored as an annotation on the imported image in the relational database OMERO. OMERO was created to provide a single unified data management platform for image data generators and users. Briefly, OMERO uses a number of storage mechanisms for images and metadata and provides an application programming interface for using remoting image analysis tools that are based on C++, Python, Matlab, or Java. Recently added functionality also allows for organizing quantitative features in tables. In biological High Content Screening (HCS), it is crucial to keep track of quantitative features. OpenBIS (http://www.cisd.ethz.ch/software/openBIS) is a framework for constructing user-friendly, scalable, and powerful information systems for HCS data and metadata. OpenBIS allows users to collect, integrate, share, and publish image-based data and connect to data processing pipelines. This framework, which is built on a hierarchical structure ranging from project management layers to sample specific datasets, is easily extensible and specialized but not limited to imaging projects. OpenBIS is a flexible platform for handling images, structured metadata (e.g., sample annotations), and unstructured data (e.g., attached files), and is scalable to very large data. A combination of databases with workflow systems such as KNIME [5] can enable the integration of functionalities beyond the scope of classical image databases. For example, the KNIME node 1Click1View (1C1V) was developed to facilitate a link between large-scale image data sets from HCS and numeric data. At the level of screening plates, 1C1V can be used to visualize quantitative features in form of heatmaps.

82
Download free eBooks at bookboon.com

Management of large sets of image data

Image management and image databases

Below we listed largest repositories of digital images sorted by discipline categories: Biological/Medical 1.	 Annotated Spine CT Database for Benchmarking of Vertebrae Localization, 125 patients, 242 scans (Ben Glockern) 2.	 Computed Tomography Emphysema Database (Lauge Sorensen) 3.	 Dermoscopy images (Eric Ehrsam) 4.	 DIADEM: Digital Reconstruction of Axonal and Dendritic Morphology Competition (Allen Institute for Brain Science et al) 5.	 DIARETDB1 – Standard Diabetic Retinopathy Database (Lappeenranta Univ of Technology) 6.	 DRIVE: Digital Retinal Images for Vessel Extraction (Univ of Utrecht) 7.	 MiniMammographic Database (Mammographic Image Analysis Society) 8.	 MIT CBCL Automated Mouse Behavior Recognition datasets (Nicholas Edelman) 9.	 Retinal fundus images – Ground truth of vascular bifurcations and crossovers (Univ of Groningen) 10.	 Spine and Cardiac data (Digital Imaging Group of London Ontario, Shuo Li) 11.	 Univ of Central Florida – DDSM: Digital Database for Screening Mammography (Univ of Central Florida) 12.	 VascuSynth – 120 3D vascular tree like structures with ground truth (Mengliu Zhao, Ghassan Hamarneh) 13.	 York Cardiac MRI dataset (Alexander Andreopoulos) 14.	 anatomy.tv An interactive 3-D model of human anatomy. Full functionality requires Windows Media player, QuickTime player or RealPlayer, and pop-up windows to be enabled in your browser. For more information, open Anatomy.tv and click FAQs 15.	 Dental Education in Video A video encyclopedia of dentistry and dental technique, delivering hundreds of high-definition videos featuring world-renowned clinicians and educators. It provides hundreds of hours of demonstrations, together with interviews and lectures, for students and faculty in dental surgery, medicine, oral hygiene, assisting, and nursing. Terms & conditions 16.	 JOVE: Journal of Visualized Experiments JOVE publishes visualized (video-based) biological research studies. It aims to solve some of the most difficult problems in the contemporary life science research: low transparency and reproducibility of biological experiments, time-consuming learning of experimental techniques…Video-based visualization of biological techniques and procedures provide an effective solution to the problem described.

83
Download free eBooks at bookboon.com

Management of large sets of image data

Image management and image databases

Face Databases 1.	 3D Mask Attack Database (3DMAD) – 76500 frames of 17 persons using Kinect RGBD with eye positions (Sebastien Marcel) 2.	 Audio-visual database for face and speaker recognition (Mobile Biometry MOBIO http://www.mobioproject.org/) 3.	 BANCA face and voice database (Univ of Surrey) 4.	 Binghampton Univ 3D static and dynamic facial expression database (Lijun Yin, Peter Gerhardstein and teammates) 5.	 BioID face database (BioID group) 6.	 Biwi 3D Audiovisual Corpus of Affective Communication – 1000 high quality, dynamic 3D scans of faces, recorded while pronouncing a set of English sentences. 7.	 CMU Facial Expression Database (CMU/MIT) 8.	 CMU/MIT Frontal Faces (CMU/MIT) 9.	 CMU/MIT Frontal Faces (CMU/MIT) 10.	 CMU Pose, Illumination, and Expression (PIE) Database (Simon Baker) 11.	 CSSE Frontal intensity and range images of faces (Ajmal Mian) 12.	 Face Recognition Grand Challenge datasets (FRVT – Face Recognition Vendor Test) 13.	 FaceTracer Database – 15,000 faces (Neeraj Kumar, P.N. Belhumeur, and S.K. Nayar) 14.	 FDDB: Face Detection Data set and Benchmark – studying unconstrained face detection (University of Massachusetts Computer Vision Laboratory)

Challenge the way we run

EXPERIENCE THE POWER OF FULL ENGAGEMENT… RUN FASTER. RUN LONGER.. RUN EASIER…
1349906_A6_4+0.indd 1

READ MORE & PRE-ORDER TODAY WWW.GAITEYE.COM

22-08-2014 12:56:57

84
Download free eBooks at bookboon.com

Click on the ad to read more

Management of large sets of image data

Image management and image databases

15.	 FG-Net Aging Database of faces at different ages (Face and Gesture Recognition Research Network) 16.	 Facial Recognition Technology (FERET) Database (USA National Institute of Standards and Technology) 17.	 Hannah and her sisters database – a dense audio-visual person-oriented ground-truth annotation of faces, speech segments, shot boundaries (Patrick Perez, Technicolor) 18.	 Hong Kong Face Sketch Database 19.	 Japanese Female Facial Expression (JAFFE) Database (Michael J. Lyons) 20.	 LFW: Labeled Faces in the Wild – unconstrained face recognition. Re-labeled Faces in the Wild – original images, but aligned using “deep funneling” method. (University of Massachusetts, Amherst) 21.	 Manchester Annotated Talking Face Video Dataset (Timothy Cootes) 22.	 MIT Collation of Face Databases (Ethan Meyers) 23.	 MORPH (Craniofacial Longitudinal Morphological Face Database) (University of North Carolina Wilmington) 24.	 MIT CBCL Face Recognition Database (Center for Biological and Computational Learning) 25.	 NIST mugshot identification database (USA National Institute of Standards and Technology) 26.	 ORL face database: 40 people with 10 views (ATT Cambridge Labs) 27.	 Oxford: faces, flowers, multi-view, buildings, object categories, motion segmentation, affine covariant regions, misc (Oxford Visual Geometry Group) 28.	 PubFig: Public Figures Face Database (Neeraj Kumar, Alexander C. Berg, Peter N. Belhumeur, and Shree K. Nayar) 29.	 SCface – Surveillance Cameras Face Database (Mislav Grgic, Kresimir Delac, Sonja Grgic, Bozidar Klimpak)) 30.	 Trondheim Kinect RGB-D Person Re-identification Dataset (Igor Barros Barbosa) 31.	 UB KinFace Database – University of Buffalo kinship verification and recognition database 32.	 XM2VTS Face video sequences (295): The extended M2VTS Database (XM2VTS) – (Surrey University) 33.	 Yale Face Database – 11 expressions of 10 people (A. Georghaides) 34.	 Yale Face Database B – 576 viewing conditions of 10 people (A. Georghaides) Action Databases 1.	 50 Salads – fully annotated 4.5 hour dataset of RGB-D video + accelerometer data, capturing 25 people preparing two mixed salads each (Dundee University, Sebastian Stein) 2.	 ASLAN Action similarity labeling challenge database (Orit Kliper-Gross) 3.	 Berkeley MHAD: A Comprehensive Multimodal Human Action Database (Ferda Ofli) 4.	 BEHAVE Interacting Person Video Data with markup (Scott Blunsden, Bob Fisher, Aroosha Laghaee) 5.	 Human Actions and Scenes Dataset (Marcin Marszalek, Ivan Laptev, Cordelia Schmid)

85
Download free eBooks at bookboon.com

Management of large sets of image data

Image management and image databases

6.	 HumanEva: Synchronized Video and Motion Capture Dataset for Evaluation of Articulated Human Motion (Brown University) 7.	 i3DPost Multi-View Human Action Datasets (Hansung Kim) 8.	 i-LIDS video event image dataset (Imagery library for intelligent detection systems) (Paul Hosner) 9.	 INRIA Xmas Motion Acquisition Sequences (IXMAS) (INRIA) 10.	 JPL First-Person Interaction dataset – 7 types of human activity images taken from a firstperson viewpoint (Michael S. Ryoo, JPL) 11.	 KTH human action recognition database (KTH CVAP lab) 12.	 LIRIS human activities dataset – 2 cameras, annotated, depth images (Christian Wolf, et al) 13.	 MuHAVi – Multicamera Human Action Image-Video Data (Hossein Ragheb) 14.	 Oxford TV based human interactions (Oxford Visual Geometry Group) 15.	 Rochester Activities of Daily Living Dataset (Ross Messing) 16.	 SDHA Semantic Description of Human Activities 2010 contest – aerial views (Michael S. Ryoo, J.K. Aggarwal, Amit K. Roy-Chowdhury) 17.	 SDHA Semantic Description of Human Activities 2010 contest – Human Interactions (Michael S. Ryoo, J.K. Aggarwal, Amit K. Roy-Chowdhury) 18.	 TUM Kitchen Data Set of Everyday Manipulation Activities (Moritz Tenorth, Jan Bandouch) 19.	 TV Human Interaction Dataset (Alonso Patron-Perez) 20.	 Univ of Central Florida – Feature Films Action Dataset (Univ of Central Florida) 21.	 Univ of Central Florida – YouTube Action Dataset (sports) (Univ of Central Florida) 22.	 Univ of Central Florida – 50 Action Category Recognition in Realistic Videos (3 GB) (Kishore Reddy) 23.	 UCF 101 action dataset 101 action classes, over 13k clips and 27 hours of video data (Univ of Central Florida) 24.	 Univ of Central Florida – Sports Action Dataset (Univ of Central Florida) 25.	 Univ of Central Florida – ARG Aerial camera, Rooftop camera and Ground camera (UCF Computer Vision Lab) 26.	 UCR Videoweb Multi-camera Wide-Area Activities Dataset (Amit K. Roy-Chowdhury) 27.	 Verona Social interaction dataset (Marco Cristani) 28.	 Videoweb (multicamera) Activities Dataset (B. Bhanu, G. Denina, C. Ding, A. Ivers, A. Kamal, C. Ravishankar, A. Roy-Chowdhury, B. Varda) 29.	 ViHASi: Virtual Human Action Silhouette Data (userID: VIHASI password: virtual$virtual) (Hossein Ragheb, Kingston University) 30.	 WorkoutSU-10 Kinect dataset for exercise actions (Ceyhun Akgul) 31.	 YouCook – 88 open-source YouTube cooking videos with annotations (Jason Corso) 32.	 WVU Multi-view action recognition dataset (Univ. of West Virginia)

86
Download free eBooks at bookboon.com

Management of large sets of image data

Image management and image databases

Fingerprints 1.	 FVC fingerpring verification competition 2002 dataset (University of Bologna) 2.	 FVC fingerpring verification competition 2004 dataset (University of Bologna) 3.	 FVC – a subset of FVC (Fingerprint Verification Competition) 2002 and 2004 fingerprint image databases, manually extracted minutiae data & associated documents (Umut Uludag) 4.	 NIST fingerprint databases (USA National Institute of Standards and Technology) 5.	 SPD2010 Fingerprint Singular Points Detection Competition (SPD 2010 committee) General Images 1.	 Aerial color image dataset (Swiss Federal Institute of Technology) 2.	 AMOS: Archive of Many Outdoor Scenes (20+m) (Nathan Jacobs) 3.	 Brown Univ Large Binary Image Database (Ben Kimia) 4.	 Columbia Multispectral Image Database (F. Yasuma, T. Mitsunaga, D. Iso, and S.K. Nayar) 5.	 HIPR2 Image Catalogue of different types of images (Bob Fisher et al) 6.	 Hyperspectral images of natural scenes – 2002 (David H. Foster) 7.	 Hyperspectral images of natural scenes – 2004 (David H. Foster) 8.	 ImageNet Linguistically organised (WordNet) Hierarchical Image Database – 10E7 images, 15K categories (Li Fei-Fei, Jia Deng, Hao Su, Kai Li) 9.	 ImageNet Large Scale Visual Recognition Challenge (Alex Berg, Jia Deng, Fei-Fei Li)

This e-book is made with

SetaPDF

SETASIGN

PDF components for PHP developers

www.setasign.com
87
Download free eBooks at bookboon.com

Click on the ad to read more

Management of large sets of image data

Image management and image databases

10.	 OTCBVS Thermal Imagery Benchmark Dataset Collection (Ohio State Team) 11.	 McGill Calibrated Colour Image Database (Adriana Olmos and Fred Kingdom) 12.	 Tiny Images Dataset 79 million 32×32 color images (Fergus, Torralba, Freeman) Gesture Databases 1.	 FG-Net Aging Database of faces at different ages (Face and Gesture Recognition Research Network) 2.	 Hand gesture and marine silhouettes (Euripides G.M. Petrakis) 3.	 IDIAP Hand pose/gesture datasets (Sebastien Marcel) 4.	 Sheffield gesture database – 2160 RGBD hand gesture sequences, 6 subjects, 10 gestures, 3 postures, 3 backgrounds, 2 illuminations (Ling Shao) Image, Video and Shape Database Retrieval 1.	 Brown Univ 25/99/216 Shape Databases (Ben Kimia) 2.	 IAPR TC-12 Image Benchmark (Michael Grubinger) 3.	 IAPR-TC12 Segmented and annotated image benchmark (SAIAPR TC-12): (Hugo Jair Escalante) 4.	 ImageCLEF 2010 Concept Detection and Annotation Task (Stefanie Nowak) 5.	 ImageCLEF 2011 Concept Detection and Annotation Task – multi-label classification challenge in Flickr photos 6.	 CLEF-IP 2011 evaluation on patent images 7.	 McGill 3D Shape Benchmark (Siddiqi, Zhang, Macrini, Shokoufandeh, Bouix, Dickinson) 8.	 NIST SHREC 2010 – Shape Retrieval Contest of Non-rigid 3D Models (USA National Institute of Standards and Technology) 9.	 NIST SHREC – other NIST retrieval contest databases and links (USA National Institute of Standards and Technology) 10.	 NIST TREC Video Retrieval Evaluation Database (USA National Institute of Standards and Technology) 11.	 Princeton Shape Benchmark (Princeton Shape Retrieval and Analysis Group) 12.	 Queensland cross media dataset – millions of images and text documents for “cross-media” retrieval (Yi Yang) 13.	 TOSCA 3D shape database (Bronstein, Bronstein, Kimmel) 14.	 Object Databases 15.	 2.5D/3D Datasets of various objects and scenes (Ajmal Mian) 16.	 Amsterdam Library of Object Images (ALOI): 100K views of 1K objects (University of Amsterdam/Intelligent Sensory Information Systems) 17.	 Caltech 101 (now 256) category object recognition database (Li Fei-Fei, Marco Andreeto, Marc’Aurelio Ranzato)

88
Download free eBooks at bookboon.com

Management of large sets of image data

Image management and image databases

18.	 Columbia COIL-100 3D object multiple views (Columbia University) 19.	 Densely sampled object views: 2500 views of 2 objects, eg for view-based recognition and modeling (Gabriele Peters, Universiteit Dortmund) 20.	 German Traffic Sign Detection Benchmark (Ruhr-Universitat Bochum) 21.	 GRAZ-02 Database (Bikes, cars, people) (A. Pinz) 22.	 Linkoping 3D Object Pose Estimation Database (Fredrik Viksten and Per-Erik Forssen) 23.	 Microsoft Object Class Recognition image databases (Antonio Criminisi, Pushmeet Kohli, Tom Minka, Carsten Rother, Toby Sharp, Jamie Shotton, John Winn) 24.	 Microsoft salient object databases (labeled by bounding boxes) (Liu, Sun Zheng, Tang, Shum) 25.	 MIT CBCL Car Data (Center for Biological and Computational Learning) 26.	 MIT CBCL StreetScenes Challenge Framework: (Stan Bileschi) 27.	 NEC Toy animal object recognition or categorization database (Hossein Mobahi) 28.	 NORB 50 toy image database (NYU) 29.	 PASCAL Image Database (motorbikes, cars, cows) (PASCAL Consortium) 30.	 PASCAL 2007 Challange Image Database (motorbikes, cars, cows) (PASCAL Consortium) 31.	 PASCAL 2008 Challange Image Database (PASCAL Consortium) 32.	 PASCAL 2009 Challange Image Database (PASCAL Consortium) 33.	 PASCAL 2010 Challange Image Database (PASCAL Consortium) 34.	 PASCAL 2011 Challange Image Database (PASCAL Consortium) 35.	 PASCAL 2012 Challange Image Database Category classification, detection, and segmentation, and still-image action classification (PASCAL Consortium) 36.	 UIUC Car Image Database (UIUC) 37.	 UIUC Dataset of 3D object categories (S. Savarese and L. Fei-Fei) 38.	 Venezia 3D object-in-clutter recognition and segmentation (Emanuele Rodola) People, Pedestrian, Eye/Iris, Template Detection/Tracking Databases 1.	 3D KINECT Gender Walking data base (L. Igual, A. Lapedriza, R. Borràs from UB, CVC and UOC, Spain) 2.	 Caltech Pedestrian Dataset (P. Dollar, C. Wojek, B. Schiele and P. Perona) 3.	 CASIA gait database (Chinese Academy of Sciences) 4.	 CASIA-IrisV3 (Chinese Academy of Sciences, T.N. Tan, Z. Sun) 5.	 CAVIAR project video sequences with tracking and behavior ground truth (CAVIAR team/ Edinburgh University – EC project IST-2001-37540) 6.	 Daimler Pedestrian Detection Benchmark 21790 images with 56492 pedestrians plus empty scenes (M. Enzweiler, D. M. Gavrila) 7.	 Driver Monitoring Video Dataset (RobeSafe + Jesus Nuevo-Chiquero) 8.	 Edinburgh overhead camera person tracking dataset (Bob Fisher, Bashia Majecka, Gurkirt Singh, Rowland Sillito)

89
Download free eBooks at bookboon.com

Management of large sets of image data

Image management and image databases

9.	 Eyetracking database summary (Stefan Winkler) 10.	 HAT database of 27 human attributes (Gaurav Sharma, Frederic Jurie) 11.	 INRIA Person Dataset (Navneet Dalal) 12.	 ISMAR09 ground truth video dataset for template-based (i.e. planar) tracking algorithms (Sebastian Lieberknecht) 13.	 MIT CBCL Pedestrian Data (Center for Biological and Computational Learning) 14.	 MIT eye tracking database (1003 images) (Judd et al) 15.	 Notre Dame Iris Image Dataset (Patrick J. Flynn) 16.	 PETS 2009 Crowd Challange dataset (Reading University & James Ferryman) 17.	 PETS: Performance Evaluation of Tracking and Surveillance (Reading University & James Ferryman) 18.	 PETS Winter 2009 workshop data (Reading University & James Ferryman) 19.	 UBIRIS: Noisy Visible Wavelength Iris Image Databases (University of Beira) 20.	 Univ of Central Florida – Crowd Dataset (Saad Ali) 21.	 Univ of Central Florida – Crowd Flow Segmentation datasets (Saad Ali) 22.	 York Univ Eye Tracking Dataset (120 images) (Neil Bruce)

In the past four years we have drilled

81,000 km
That’s more than twice around the world.
Who are we?
We are the world’s leading oilfield services company. Working globally—often in remote and challenging locations—we invent, design, engineer, manufacture, apply, and maintain technology to help customers find and produce oil and gas safely.

Who are we looking for?
We offer countless opportunities in the following domains: n Engineering, Research, and Operations n Geoscience and Petrotechnical n Commercial and Business If you are a self-motivated graduate looking for a dynamic career, apply to join our team.

What will you be?

careers.slb.com

90
Download free eBooks at bookboon.com

Click on the ad to read more

Management of large sets of image data

Image management and image databases

Segmentation 1.	 Alpert et al. Segmentation evaluation database (Sharon Alpert, Meirav Galun, Ronen Basri, Achi Brandt) 2.	 Berkeley Segmentation Dataset and Benchmark (David Martin and Charless Fowlkes) 3.	 GrabCut Image database (C. Rother, V. Kolmogorov, A. Blake, M. Brown) 4.	 LabelMe images database and online annotation tool (Bryan Russell, Antonio Torralba, Kevin Murphy, William Freeman) Surveillance 1.	 AVSS07: Advanced Video and Signal based Surveillance 2007 datasets (Andrea Cavallaro) 2.	 ETISEO Video Surveillance Download Datasets (INRIA Orion Team and others) 3.	 Heriot Watt Summary of datasets for human tracking and surveillance (Zsolt Husz) 4.	 SPEVI: Surveillance Performance EValuation Initiative (Queen Mary University London) 5.	 Udine Trajectory-based anomalous event detection dataset – synthetic trajectory datasets with outliers (Univ of Udine Artificial Vision and Real Time Systems Laboratory) Textures 1.	 Color texture images by category (textures.forrest.cz) 2.	 Columbia-Utrecht Reflectance and Texture Database (Columbia & Utrecht Universities) 3.	 DynTex: Dynamic texture database (Renaud Piteri, Mark Huiskes and Sandor Fazekas) 4.	 Oulu Texture Database (Oulu University) 5.	 Prague Texture Segmentation Data Generator and Benchmark (Mikes, Haindl) 6.	 Uppsala texture dataset of surfaces and materials – fabrics, grains, etc. 7.	 Vision Texture (MIT Media Lab)

91
Download free eBooks at bookboon.com

Management of large sets of image data

Image management and image databases

Web Image Repositories:
Name Location / Owner / Hoster Canada / Amazon AWS (hosting) United States Sweden / The Pirate Bay Sweden / Dayviews AB (Bilddagboken AB) Description/Focus/Registration requirements Free, registration required Registered users 1,500,000 Storage Space Allowed Per User (standard) 20 uploads/week free plan, unlimited for paid accounts Unlimited uploads / free plan Unlimited storage for 547×410 pix reduced images Unlimited storage for 547×410 pix reduced images, also unlimited resolution and size for paying BDB HD users, costing approximately equivalent to $10.5 a year Unlimited uploads with 30 MB limit per image for all account types.

500px

BeFunky Photo Editor Bayimg

Free, registration required Free image hosting, no registration required, uncensored Free image hosting, registration required

13,000,000 Unknown

Dayviews

1,291,500

DeviantART

United States

Free image hosting, registration required. Subscription option provides additional services, and unlocks hidden features. Generally allows art-specific content only, as the service is not a generic image hosting service. Open (Yahoo! login)

25,000,000

Flickr

United States / Yahoo!

87,000,000 (May 2013)

After 20 MAY 2013, 1TB free, 200MB per image, all photos display, original files downloadable. (Old limits: 300MB monthly upload limit (30MB per photo), max 200 images viewable (free account). If a free account is inactive for 90 consecutive days, it may be deleted) 50MB for free, unlimited storage for $30/year

Fotki

United States / Fotki, Inc.

Free registration photo sharing service and communication portal.

1,250,000

Fotolog

United States

Photoblogging. Popular in South America.

15,000,000

92
Download free eBooks at bookboon.com

Management of large sets of image data

Image management and image databases

GifBoom

United States / TapMojo LLC

Sharing of animated GIFs. Login through twitter or Facebook.

Unknown

The free version is ad-supported, and limits users to uploading one picture per day Paying members can upload up to 6 pictures a day Unknown

Instagram

United States / Facebook United States / Autodesk United States France Sweden

Photo sharing service. Registration is required. Free image hosting, images removed if not viewed in the last 30 days. Free image hosting, no registration required. Photo/video sharing, Groups, Blog Free hosting of galleries created with Jalbum software

100,000,000

imm.io

Unknown

Imgur Ipernity jAlbum

Unknown Unknown 340,000

Owned by Facebook No user registration. 250 visible images for free accounts. Unlimited for pro accounts. If an image does not receive even 1 view in 6 months, it may be removed to make room for new images. Images on pro accounts will never be removed. Unlimited image display/storage for paying users (200MB p/month upload, only last 1000 displayed with free account) 30MB of space given for free accounts, upgrade to 1gb space for €19/year Unlimited storage

Lafango

United States

Free Registration, customizable galleries & folders.

Unknown

Lockerz

United States

Free registration service

19,000,000

Panoramio

Headquartered in Switzerland / Google United States / Carbonite United States

Free registration service

4,700,000

Phanfare

Subscription-based photo sharing

Unlimited uploads for all users; 5MB per image 50,000,000 Unlimited provided the photos comply with the Google Earth Photo Acceptance Policy

Photobucket

Free registration service

93
Download free eBooks at bookboon.com

Management of large sets of image data

Image management and image databases

Photoshop. com

United States / Adobe Systems

Focused on for managing, editing, storing, and sharing photos online, with personal URL, Photoshop. com Mobile access, tutorials, help etc. Currently US-only, works from any Flash-enabled browser but Adobe Photoshop Elements 7 or Adobe Premiere Elements 7 users can also use it directly. Free registration service

Unknown

Unlimited storage. Subscription accounts only. Bought out by Carbonite.

Picasa Web Albums

United States / Google

500,000

Unlimited free storage, 1 MB per photo and 10 Min per video (With image size restrictions) No size restrictions with Pro account. With a free account, you can use up to 10GB of bandwidth per month and 2GB storage. Two levels of membership: the Basic level is free with 2GB space and the option to purchase additional storage starting at $19.99 per year. The Plus level costs $69.99 per year with 20 GB included storage space, and $129.99 per year with 100GB and requires either Adobe Photoshop Elements 7 or Adobe Premiere Elements 7 to use Organizer.

Pinterest

United States

Photo sharing/social networking

11,700,000

94
Download free eBooks at bookboon.com

Management of large sets of image data

Image management and image databases

Pixabay

Germany

Sharing of high-quality Public Domain photos.

Unknown

1 GB free storage for photos and videos, then plans start at $2.49/month for additional storage. Photos smaller than 2048×2048 pix (Google+ users only) or 800×800 pix and videos less than 15 minutes in length do not count toward storage limit. Photos uploaded beyond the storage limit will automatically be resized. Unknown

Pixorial

United States

Free to browse and download, registration required to contribute. Includes social networking capabilities. Personal photo and video sharing platform. Registration required. Free subscription available.

Unknown

Shutterfly SmugMug

United States United States

2,000,000 315,000

Unknown Free subscription available with 7 GB of storage for photos or videos. No limits on resolution of images or length of videos. Additional paid subscriptions available with increased storage. Unlimited uploads of photos or videos. Free, unlimited picture storage. Full resolution downloads only possible via purchase of archive DVD. “unlimited” storage, 12MB per photo (Standard /Power), 24 MB per photo (Pro) Owner control over download of original resolution image. unlimited, pay per download

Snapfish

United States / Hewlett-Packard

Free registration service

90,000,000

Streamzoo

United States

Free search, subscription hosting, $40/year (Standard); $60 (Power); $150 (Pro); too

Unknown

TinyPic

United States / PhotoBucket

Free registration service, also provides services for Costco’s online photo processing store

Unknown

95
Download free eBooks at bookboon.com

Management of large sets of image data

Image management and image databases

Trovebox

United States / Wide Angle Labs, Inc. Netherlands

Mobile photosocial game, free registration service, unlimited storage and sharing, available on Apple iOS, Android and Web. Free image hosting, no registration required.

Unknown

Unlimited

Woophy

26,410

No known limit, but largest image dimension limited to 1600 pixels 100 photo uploads per month for free users, unlimited for Pro accounts. 30MB per photo.

yfrog

United States / ImageShack

Free image hosting, registration required. Subscription option provides additional features.

Unknown

96
Download free eBooks at bookboon.com

Click on the ad to read more

Management of large sets of image data

Operating system (os) and images

12	 Operating system (os) and images
Image file size is affected by three factors: pixel dimensions, image format and bit depth. In general, reducing your pixel dimensions to only what you actually require, using the JPEG format and setting the lowest possible bit depth will give you the smallest file size. Does it matter how many files I keep in a single directory? If so, how many files in a directory is too many, and what are the impacts of having too many files? Most file systems have restrictions on the length of filenames. Many operating systems include support for more than one file system. Sometimes the OS and the file system are so tightly interwoven it is difficult to separate out file system functions. There needs to be an interface provided by the operating system software between the user and the file system. This interface can be textual (such as provided by a command line interface, such as the Unix shell, or OpenVMS DCL) or graphical (such as provided by a graphical user interface, such as file browsers). Parameters for most common files systems used for image storage are listed below: FAT32: File Allocation Table, 32-bit Introduced: August 1996 (Windows 95 OSR2) Maximum volume size: 8 TB Maximum number of files: 268,435,437 Maximum file size: 4GB Filenames: 8.3 or 255 characters, ASCII except for “’*/:<>?\| OS support: just about every OS out there. NTFS: New Technology File System Introduced: July 1993 (Windows NT 3.1) Maximum volume size: 256TB currently (16EB theoretically) Maximum number of files: 4,294,967,295 Maximum file size: 16TB currently (16EB theoretically) Filenames: 255 characters, ASCII except ‘\0’ OS support: Native support in Windows NT 3.1-4.0, Windows 2000 (NT 5.0), Windows XP (NT 5.1 and 5.2), and Windows Vista (NT 6.0.) Partial support in Linux (read support since Linux 2.2.0, rw support in 2.6.14 and newer), read support in FreeBSD, eComStation, MacOS X (10.3 and newer.)

97
Download free eBooks at bookboon.com

Management of large sets of image data

Operating system (os) and images

Ext2: Second extended file system Introduced: January 1993 Maximum volume size: 16TB Maximum number of files: 10^18 Maximum file size: 2TB Filenames: 255 characters, any byte except \0 and /. OS support: Native support under Linux and the BSDs. Read-write support in Windows 2000 and newer with a 3rd-party driver, MacOS X. Ext3: Third extended file system Introduced: November 2001 (Linux 2.4.15) Maximum volume size: 2TB (1KB block) to 16TB (4KB block) Maximum number of files: number of bytes in volume/2^13. Maximum file size: 16GB (1KB block) to 2TB (4KB block) Filenames: 255 bytes long, all bytes except \0 and /. OS support: Native support in Linux and the BSDs, read-write support in Windows 2000 and newer via a 3rd-party driver. ReiserFS 3.x Introduced: 2001 (Linux 2.4.1) Maximum volume size: 16TB Maximum number of files: 4,294,967,296 Maximum file size: 8TB Filenames: 255 bytes, all bytes except \0 and /. OS support: Linux (2.4.1 and newer, 2.6.8 or newer for block journal.) Very limited read-only access via 3rd-party programs in Windows. XFS Introduced: 1994 (SGI IRIX 3.5) Maximum volume size: 16TB (32-bit OS,) 9EB (64-bit OS) Maximum number of files: ? Maximum file size: 16TB (32-bit OS,) 9EB (64-bit OS) Filenames: 255 bytes, all bytes except /0 OS support: SGI IRIX (3.5 and newer), Linux (2.4.25), BSD. No tools to read XFS filesystems in non-UNIX OSes.

98
Download free eBooks at bookboon.com

Management of large sets of image data

Operating system (os) and images

HFS+: Hierarchical File System Plus Introduced: Jan. 19, 1998 (MacOS 8.1) Maximum volume size: 16EB Maximum number of files: Unlimited Maximum file size: 16EB Filenames: 255 characters, Unicode except :. OS support: Native support in MacOS (8.1 and newer), read-write support in Linux. ZFS: Zettabyte File System Introduced: Nov. 2005 (Solaris 10) Maximum volume size: 16EB Maximum number of files: 2^48 Maximum file size: 16EB Filenames: 255 bytes, any Unicode except /0. OS support: Native support in Solaris (10). Very limited experimental support in Linux (userspace filesystem) and BSD.

Find and follow us: http://twitter.com/bioradlscareers www.linkedin.com/groupsDirectory, search for Bio-Rad Life Sciences Careers http://bio-radlifesciencescareersblog.blogspot.com

John Randall, PhD Senior Marketing Manager, Bio-Plex Business Unit

Bio-Rad is a longtime leader in the life science research industry and has been voted one of the Best Places to Work by our employees in the San Francisco Bay Area. Bring out your best in one of our many positions in research and development, sales, marketing, operations, and software development. Opportunities await — share your passion at Bio-Rad!

www.bio-rad.com/careers

99
Download free eBooks at bookboon.com

Click on the ad to read more

Management of large sets of image data

Graphics processing unit (GPU)

13	 Graphics processing unit (GPU)
A graphics processing unit (GPU), also occasionally called visual processing unit (VPU), is a specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display. GPUs are used in embedded systems, mobile phones, personal computers, workstations, and game consoles. Modern GPUs are very efficient at manipulating computer graphics, and their highly parallel structure makes them more effective than general-purpose CPUs for algorithms where processing of large blocks of data is done in parallel. In a personal computer, a GPU can be present on a video card, or it can be on the motherboard or – in certain CPUs – on the CPU die [25]. GPU-accelerated computing offers unprecedented application performance by offloading computeintensive portions of the application to the GPU, while the remainder of the code still runs on the CPU. From a user’s perspective, applications simply run significantly faster. The first company to develop the GPU is NVIDIA Inc. Its GeForce 256 GPU is capable of billions of calculations per second, can process a minimum of 10 million polygons per second, and has over 22 million transistors, compared to the 9 million found on the Pentium III. Its workstation version called the Quadro, designed for CAD applications, can process over 200 billion operations a second and deliver up to 17 million triangles per second. CPU versus GPU A simple way to understand the difference between a CPU and GPU is to compare how they process tasks. A CPU consists of a few cores optimized for sequential serial processing while a GPU consists of thousands of smaller, more efficient cores designed for handling multiple tasks simultaneously. Usage GPU is used primarily for 3-D applications, a graphics processing unit is a single-chip processor that creates lighting effects and transforms objects every time a 3D scene is redrawn. These are mathematicallyintensive tasks, which otherwise, would put quite a strain on the CPU. Lifting this burden from the CPU frees up cycles that can be used for other jobs.

100
Download free eBooks at bookboon.com

Management of large sets of image data

Storage and archive

14	 Storage and archive
How do you store your digital image files? With digital cameras rapidly replacing film cameras, image data must have a system for archiving files. Keeping just one copy of image on computer is risky. Hard disk drives may fail, causing all the data on them to be lost. Image files are also easy to delete, from memory card or computer. The main risk associated with any archiving system is accidental data loss. This can occur in many different ways, so need some form of ‘insurance’ against it. At the very least, a secure archive will have two backup copies in separate locations in addition to the accessible copy of the images that resides on the computer. How to Back-up Backing up image files is easy. You simply copy the files from one location to another. This can involve copying pictures from a memory card to a computer, from a computer to an optical disk, or from a computer or optical disk to an external hard disk drive. An effective image archive should ensure there are several copies of images in different locations. It is important to keep one copy on computer which can access the files easily. Another copy could reside on an external hard drive that can be connected to computer and easily linked to another computer when you upgrade system or swap from desktop to laptop. Formats for Archiving If capture device can capture Raw files, the best option is to archive them as they come from the device. Although Raw files must be processed into another format (typically TIFF or JPEG) before they can be viewed or edited, the advantages of archiving images as Raw files are significant: 1.	 The original image is retained, allowing you to return to it when you have new software or hardware that can improve the output you obtain from it. No data is lost when Raw files are archived. 2.	 The original image metadata is retained. This information includes details of camera settings and photographers can add information about the date the photo was shot, the location, the subject and other descriptors. Copyright data can also be inserted in the metadata file. But suppose the files originated as JPEGs? JPEG uses lossy compression and each time you open and re-save a JPEG image, data is lost. While a first generation JPEG will offer quality comparable to any other final or ready-to-print format, it cannot offer as much latitude for correcting exposure and other shooting issues as a Raw file.

101
Download free eBooks at bookboon.com

Management of large sets of image data

Storage and archive

Both JPEG and TIFF files result from image processing, either in the device itself or when the files are produced. Certain image parameters must be set by this processing to make the files viewable and editable on a wide range of devices. When Raw or JPEG files are saved in TIFF format these adjustments are locked in, limiting opportunities for further adjustments in future editing. To minimise data loss, archive all images that originated as JPEGs in TIFF format, using lossless (or no) compression. The resulting files will be large but storage is now much more affordable than it was in the past. (Some affordable storage options are listed below.) Storage Options Users can choose from three main technologies when setting up an image archive: optical discs, magnetic tape or computer hard drives. CDs and DVDs are cheap, readily available and convenient. Almost all computers come with a compatible read/write drive, putting this option within everyone’s reach. It’s very easy to burn images to a CD or DVD and both formats can provide a high level of stability and convenience. CDs can hold up to 700MB of image data, while DVD capacities start at 4.7 GB per disk. But even a DVD will be filled quickly when you shoot with a high-resolution digital camera. CDs are best suited to short-term storage of small numbers of files. Their capacity is too small to make them viable as an archiving option. The higher capacities of DVDs make them a better choice for archiving.

678'<)25<2850$67(5©6'(*5((
&KDOPHUV 8QLYHUVLW\ RI 7HFKQRORJ\ FRQGXFWV UHVHDUFK DQG HGXFDWLRQ LQ HQJLQHHU LQJ DQG QDWXUDO VFLHQFHV DUFKLWHFWXUH WHFKQRORJ\UHODWHG PDWKHPDWLFDO VFLHQFHV DQG QDXWLFDO VFLHQFHV %HKLQG DOO WKDW &KDOPHUV DFFRPSOLVKHV WKH DLP SHUVLVWV IRU FRQWULEXWLQJ WR D VXVWDLQDEOH IXWXUH ¤ ERWK QDWLRQDOO\ DQG JOREDOO\ 9LVLW XV RQ &KDOPHUVVH RU 1H[W 6WRS &KDOPHUV RQ IDFHERRN

102
Download free eBooks at bookboon.com

Click on the ad to read more

Management of large sets of image data

Storage and archive

Additional copies of images could be burned on gold DVD media. Once the data is stored on the disk, where it cannot be erased or easily damaged, the disk should be labelled with the date and subject/ location of the images. It is recommended to use special marker pens for labelling optical disks, not regular markers. Chemicals released from normal markers can degrade optical disks. High throughput imaging units are equipped in inkjet printers may prefer to print labels for their archive disks. Specially coated CDs and DVDs are available for only a little more than regular disks. It is good to handle all optical disks carefully and avoid touching the recording surfaces as grease and other contaminants are easily transferred from your fingers. One copy of these files on disk can be kept close at hand in an ‘archive’ folder, while another copy should be stored at a different location (for example, a security box in a bank or an office). Higher storage capacities are available in Blu-ray discs, which are becoming available in computer stores, along with Blu-ray drives. Storage capacities of 25GB and 50GB are available and most drives can also read DVDs and CDs. Magnetic tape systems are still available for business back-ups but they’re seldom used these days for image archiving because individual files are slow to access and the tapes themselves become degraded by repeated use. Many users prefer hard disk drives (HDDs), which offer capacities from 60GB to many terabytes. External HDDs are the most cost-effective option for storing large quantities of digital data. Most connect to a computer via a Hi-Speed USB 2.0/3.0 cable, which offers data transfer rates of up to 480 megabits/second. Physical dimensions vary, with models designed to sit on desktops and compact, portable models that can be slipped into a notebook computer case. Larger HDDs require separate power supplies, while smaller models draw power via the USB cable. The beauty of HDDs is that they can work with any computer that has a USB port, so you can easily share them between several PCs. This makes them particularly useful for image and multimedia file storage. File Organisation User must be able to locate wanted images quickly and easily, so the hallmark of an effective archiving system is image management. Many users work with automatic cataloguing software that will create new folders each time images are uploaded to a computer and allow users to tag individual images or groups of files to make them easier to track. Folders created by such systems can also be copied to archives.

103
Download free eBooks at bookboon.com

Management of large sets of image data

Storage and archive

User should apply following guidelines to develop a file naming convention for image storage: •	 Use short names, 8 or fewer characters in the file name, whenever possible. οο Eight character file names are backwards compatible with older operating systems. οο Shorter file names are easier to read and reduce the potential for error when typing the file name. •	 Avoid unnecessary repetition and redundant words in file names and file paths. •	 Use a 3 character file extension (i.e. “.tif ”, not “.tiff ”) •	 Use only alpha-numeric characters, except for dashes, – , and underscores, _ . οο Dashes and underscores may have special uses in other operating systems or software, such as UNIX and MySQL. DPAG reviewed the contexts in which these characters are reserved and determined that dashes and underscores can be used in file names without a technical conflict. •	 Do not use special characters, such as, . \ / : * ? “ < > |, except for dashes or underscores. οο These characters are often reserved for use by the operating system. οο If using a date in the file name always state the date ‘back to front’, and use four digit years, two digit months and two digit days: YYYYMMDD or YYYYMM or YYYY or YYYY-YYYY. οο When including a personal name in a file name give the family name first followed by the initials. οο All letters should be lower case. οο Some operating systems are case sensitive. οο Using lower case consistently prevents problems, if the files are migrated to a casesensitive operating system. •	 Do not use spaces in the file name. οο Browsers and some older operating systems do not handle spaces well. •	 Abbreviate the content of elements whenever possible. •	 Avoid ambiguous and potentially redundant folders. •	 Create a “template” of empty subfolders. •	 Consider replacing extremely disorganized folder structures. •	 Use leading zeros. •	 If the file name includes numbers use zeros as placeholders. For example, a collection with 999 items should be numbered: mac001.tif, mac002.tif…mac011.tif, mac012.tif, etc. (NOT mac1.tif, mac2.tif…). This practice facilitates sorting and file management. •	 Avoid using common words such as ‘photo or ‘image’ at the start of file names, unless doing so will make it easier to retrieve the record.

104
Download free eBooks at bookboon.com

Management of large sets of image data

Storage and archive

When storing digital images, bear in mind that a failure in any of the storage devices you use could mean your pictures are irretrievably lost. It is also risky to rely on automated file management systems as some have been known to behave counter-intuitively at times. User should, therefore, develop a systematic ‘workflow’ – in other words, a standard system for taking, copying, editing and storing ALL your digital images that locks in as soon as you upload files to your computer. •	 Shoot all images at the highest resolution and quality settings. (If your camera supports Raw file capture, use it.) •	 Upload the image files to your computer, using the Copy process, which leaves the original image files on the memory card. If you’re using an automatic image downloader, it should store each batch of shots in a separate folder, labelled with the date on which the files were saved. This is a logical way to catalogue pictures and works well as long as you can remember when pictures were taken. If you don’t use automatic cataloguing you need a system that will make individual image files easy to locate and retrieve. Adding keywords, dates or times to the file name can be helpful at this stage. •	 Examine your files and delete any images that are unwanted duplicates as well as those you know you will never want again (blurred shots, pictures with serious exposure problems, shots with intervening objects that spoil pictorial composition). Some photographers like to set up sub-files of ‘keeper’ shots at this stage to make it easier to track down their best shots. •	 Make a second copy of all the image files on a CD or DVD or external hard disk drive; somewhere that is not on your computer’s hard drive. Once this has been done, you can erase the files from your memory card, knowing that if a power surge took out your computer’s hard drive, a copy of your original photos would still be available. These four steps ensure you have at least two copies of all your original shots. The most dangerous point in handling digital files is transferring them from one storage location to another. To avoid problems, NEVER delete files from the source location until you have verified them in the destination. Other ways to avoid problems when transferring image files include [48]: •	 Make sure the device (camera or card reader) has completed writing before attempting to remove the card or storage media. •	 Avoid wet or dusty places when changing memory cards or writing files to optical disks. •	 Keep spare memory cards in protective plastic containers when they are not in use and store them away from heat, electromagnetic radiation, dust and moisture.

105
Download free eBooks at bookboon.com

Management of large sets of image data

Storage and archive

Backup and archive in medicine Digital medical images are typically stored locally on picture archiving and communication system (PACS) (read below) for retrieval. It is important (and required in the USA by the Security Rule’s Administrative Safeguards section of HIPAA) that facilities have a means of recovering images in the event of an error or disaster. While each facility is different, the goal in image back-up is to make it automatic and as easy to administer as possible. The hope is that the copies won’t ever be needed, but, as with other disaster recovery and business continuity planning, they need to be available if needed. Ideally, copies of images should be streamed off-site as they are created. (If using the Internet, the Security Rule’s Technical Safeguards section of HIPAA requires that the images be encrypted during transmission.) Depending on upload bandwidth and image volume, this may not be practical if the back-up system cannot be configured to tune bandwidth usage and frequency of back-ups. Other options include removable media (hard drives, DVDs or other media that can hold many patients’ images) that is physically transferred off-site. The content of these copies must be protected via encryption from exposure to unauthorized personnel or stiff penalties can be assessed. Images may be stored both locally and remotely on off-line media such as tape or optical media, or partially or exclusively on hard disks (“spinning”) media. The latter is becoming more common. The hard drives may be configured and attached to the PACS server in various ways, either as Direct-Attached Storage (DAS), Network-attached storage (NAS), or via a Storage Area Network (SAN). However the storage is attached, the drives themselves are usually configured as a Redundant Array of Inexpensive (or Independent) Discs RAID, which may be configured to provide appropriate combination of faster disk access or protection against the failure of one (or even two) discs in the physical RAID array. Typically, failed drives may be physically replaced (hot swapping) without interruption of service. Since costs of computers has fallen, some sites opt for fully redundant Archives, rather than just protecting the drives through RAID. Further, RAIDs are fragile and can be rendered useless by one erroneous hit on the controller. Data stored on disk may also be backed up to tape or optical media or copied, in real time, to a slower, inexpensive disc in another machine at another location. Some sites make two such backups and remove them from the site on a rotating basis. In the event that it is necessary to reconstruct a PACS partially or completely from the back-up images, some means of rapidly transferring all of its images back to the PACS is required, preferably whilst the PACS continues to receive and provide current images. The back-up infrastructure may also be capable of supporting the migration of images to a new PACS. Due to the high volume of images that need to be archived many rad centers are migrating their systems to a Cloud-based PACS [47].

106
Download free eBooks at bookboon.com

Management of large sets of image data

Storage and archive

Compression Compression is a method of reducing the size of a digital image file in order to free up the storage capacity of hard drives. Compression technologies are distinguished from each other by whether they remove detail and color from the image. Lossless technologies compress image data without removing detail, while “lossy” technologies compress images by removing some detail [64]. A data-compression technique that can reduce the detail of a digital image file. Most video compression techniques utilize lossy compression. File sizes can be reduced by a number of different compression algorithms but image data may be lost depending on the type. Lossless compressions (such as Tagged Image File Format; TIFF) encode information more efficiently by identifying patterns and replacing them with short codes. These algorithms can reduce an original image by about 50 to 75 percent. This type of file compression can facilitate transfer and sharing of images and allows decompression and restoration to the original image parameters. Lossy compression algorithms, such as that used to define pre-2000 JPEG image files, are capable of reducing images to less than 1 percent of their original size. The JPEG 2000 format uses both types of compression. The large reduction is accomplished by a type of undersampling in which imperceptible grey level steps are eliminated. Thus the choice is often a compromise between image quality and manageability.

Linköping University – innovative, highly ranked, European
Interested in Engineering and its various branches? Kickstart your career with an English-taught master’s degree.

Click here!

107
Download free eBooks at bookboon.com

Click on the ad to read more

Management of large sets of image data

Storage and archive

Bit mapped or raster based images are produced by digital cameras, screen and print output devices that transfer pixel information serially. A 24 bit color (RGB) image uses 8 bits per color channel resulting in 256 values for each color for a total of 16.7 million colors. A high resolution array of 1280 × 1024 pixels representing a true color 24 bit image would require more than 3.8 megabytes of storage space. Commonly used raster based file types include GIF, TIFF, and JPEG. Vector based images are defined mathematically and used for primarily for storage of images created by drawing and animation software. Vector imaging typically requires less storage space and is amenable to transformation and resizing. Metafile formats, such as PDF, can incorporate files created by both raster and vector based images. This file format is useful when images must be consistently displayed in a variety of applications or transferred between different operating systems. As the dimensional complexity of images increases, image file sizes can become very large. For a single color, 2048 x 2048 image file size is typically about 8 megabytes. A multicolor image of the same resolution can reach 32 megabytes. For images with 3 spatial dimensions and multiple colors, a smallish image might require 120 megabytes or more of storage. In live cell imaging where time resolved, multidimensional images are collected, image files can become extremely large. For example, an experiment that uses 10 stage positions, imaged over 24 hours with 3-5 colors at one frame per minute, a 1024 × 1024 frame size, and 12 bit image could amount to 86 gigabytes per day! High speed confocal imaging with special storage arrays can produce up to 100 gigabytes per hour. Image files of this size and complexity must be organized and indexed and often require massive directories with hundreds of thousands of images saved in a single folder as they are streamed from the digital camera. Modern hard drives are capable of storing at least 500 gigabytes. The number of images that can be stored depends on the size of the image file. About 250,000 2–3 megabyte images can be stored on most modern hard drives. External storage and backup can be performed using compact disks (CDs) that hold about 650 megabytes or DVDs that have 5.2 gigabyte capacities. Image analysis typically takes longer than collection and is presently limited by computer memory and drive speed. Storage, organization, indexing, analysis and presentation will be improved as 64 bit multiprocessors with large memory cores become available. Buffer Memory A buffer memory is a temporary “holding area” for image data waiting to be processed in a camera. Buffers enable a camera to continue capturing new image files without having to shut down while previous image files are processed. Printers also make use of buffers, which allow you to queue up several pictures at a time while the printer outputs previously queued-up image files [64].

108
Download free eBooks at bookboon.com

Management of large sets of image data

Images in different disciplines

15	 Images in different disciplines
15.1	Microscopy
Molecular imaging Molecular imaging is a discipline at the intersection between molecular biology and in vivo imaging. Optical molecular imaging can be used as a powerful tool for studying the temporal and spatial dynamics of biomolecules and their interactions [18], in vitro as well as in vivo. On a purely molecular scale, imaging has for example provided an understanding of the rotational movement of F1-ATPase within ATP synthase [60]. The analysis of such highly structured macromolecular complexes of sizes and dynamics within nanometer and microsecond ranges, respectively, requires preliminary knowledge about molecular players. To observe the rotation under a microscope, Yasuda et al. [60] fixed subcomplexes of F1 on surface-bound beads and attached a fluorescently labeled actin filament to each γ subunit of ATP synthase. These structures were mounted on cover glasses. The in vitro addition of ATP finally triggered the continuous rotation of a few percentage of fluorescent actin filaments. At the time, these high-speed images obtained at single-molecule resolution were recorded on an 8-mm videotape. Since this work was published, new technologies have been developed to obtain even higher temporal and spatial data resolution [59]. However, sample preparations for such studies remain to be a manual and time-intensive endeavor [42]. Single molecule imaging in living matter provides the ability to study the molecular organization in cells and tissues by localizing specific molecules, such as RNA and proteins, in a native cellular context. However, many subcellular structures have dimensions lying below the diffraction limit of the visible light. Therefore superresolution microscopy techniques, allowing to look beyond the diffraction limit, such as PALM and STORM, are increasingly used for analyzing the organizational principles of molecular complexes and single molecules within living cells [53]. A central paradigm in systems biology is the aim for understanding biological networks including many different molecular factors. In classical fluorescence microscopy, however, the number of channels, which can be measured simultaneously, is limited by the spectral overlap between fluorophores. In this context it is important to note that recent developments have succeeded in increasing the number of molecular species that can be measured simultaneously. For example, Lubeck et al. [33] reported a method that drastically increases the number of simultaneously measurable molecular species by combining super-resolution microscopy and combinatorial labeling using mRNA barcodes with adjacent emitter/activator pairs. As a proof of concept, the authors analyzed the mRNA levels of 32 genes within a single yeast cell. Further improvements of this barcoding technology could potentially be used to perform-omics experiments at single-cell resolution, which could be a major milestone for systems biology [3].

109
Download free eBooks at bookboon.com

Management of large sets of image data

Images in different disciplines

Cellular Imaging The in vitro imaging of biophysical processes at the molecular scale requires time-intensive sample preparation, whereas the imaging of higher-scale processes is often feasible at higher throughput, which is an important advantage in terms of statistical power and network analysis. Cell-based screening for biological or chemical compounds with biological effects is at the core of modern translational systems biology. High content screening (HCS) combines high-throughput microscopy with the automated extraction of a multitude of single-cell physiological features [9]. Automated microscopes equipped with an autofocus system [54] can be used to perform high-throughput experiments, in which the effects of hundreds of thousands of compounds or genetic perturbations are analyzed. The classical readouts of such image-based high-throughput screenings are fixed endpoints that can gather data from multiple image channels. While the lack of dynamical information is a constraint of endpoint measurements, both the possible high-throughput of endpoint measurements and the possibility to use antibodies that target intracellular antigens in fixed samples are valid arguments for choosing an endpoint analysis strategy [28]. In contrast to many biochemical assays, the resulting images of cell populations circumvent the limitations of population averages [2] by analyzing image data at the single-cell level [23]. However, the large volume of images produced by such high-throughput screening requires automated image analysis, including the identification and isolation of regions or objects of interest (segmentation) as well as the extraction of numerical intensity and morphology features.

110
Download free eBooks at bookboon.com

Click on the ad to read more

Management of large sets of image data

Images in different disciplines

In addition to single-cell information, light microscopy provides a path from rough static models to more refined dynamic models. Low- and medium-throughput automated microscopy can be used to acquire sequential image series of multiple samples and analyze the resulting kinetic data. The greatest strength of live cell imaging is its potential to assess the dynamics of cellular and even subcellular events. One example is a study by Chao et al. [26], in which the local translation of specific mRNAs was analyzed in single cells. With regard to cell populations, live cell imaging enables assessment of cellular heterogeneity and synchrony, which are important for understanding cellular differentiation [24, 34, 50, 51, 63], as well as local and global control mechanisms of transcription factors. Modern live cell imaging can build upon a whole arsenal of fluorescence-based methods that can be used to quantify the subcellular distribution of proteins, dynamics of subcellular shuttling processes, and molecular binding rates. Using a highly tuned setup such as Förster resonance energy transfer (FRET) and fluorescence lifetime imaging microscopy, the dynamics of fast spatiotemporal protein-protein interactions can be observed at a molecular resolution [11]. However, the adequate interpretation of spatially resolved dynamic information requires more advanced analysis than steady-state images. In addition to segmentation, live cell imaging applications often require object tracking. Dynamic information can be highly relevant for translational research. For example, determining a correlation between hydrogel substrate elasticity and the migration of muscle stem cells could lead to the development of cell-based therapies for muscle-wasting diseases [62]. Classical tracking algorithms segment and track cells in a sequential approach by connecting neighboring time points. However, in contrast to classical tracking and cell lineage identification algorithms, improved algorithms that consider the entire image sequence, and prior knowledge (e.g., about mitosis and apoptosis) for annotating the best track and identifying the most likely cell lineage can be achieved [3]. Image processing in biology A central goal of image analysis is the conversion of microscopic images into biologically meaningful quantitative data. However, the amounts of image data produced using modern systems biology are very vast for manual analysis; hence, the development of automated image analysis tools is essential. Due to the complexity and size of modern imaging data, the computational analysis of biological imaging data has already become a vital emerging sub-discipline of bioinformatics and computer vision [22]. Research using multiparametric imaging data relies heavily on computational approaches for image acquisition, data management, visualization, and correct data interpretation [38, 56, 44]. The typical functions of dedicated computer vision systems are data pre-processing, image segmentation, feature extraction, and decision making [13, 20]. Over the past 20 years, a myriad of commercial and open-source image analysis and data management tools have evolved. We focus on open-source solutions, which facilitate community-driven efforts in the development of image analysis. Examples of microscopy developments requiring custom computational workflows for image acquisition include structured-illumination microscopy [61], super resolution microscopy [37, 52, 65], and Bessel-beam microscopy. Some modern microscopes can produce up to 30 TiB of data per day [27]. However, the volume of images generated in systems biology is growing rapidly. As a result, the scalability of storage solutions and awareness for the need of image repositories and common file formats for imaging projects are increasing.

111
Download free eBooks at bookboon.com

Management of large sets of image data

Images in different disciplines

Research on image analysis has developed an entire ecosystem of image analysis tools. ImageJ [31], formerly known as NIH image, is a role model in the landscape of open-source tools for image analysis. Since its beginnings it has always been free and it became the most popular and widespread multipurpose image analysis tool. ImageJ has become successful because the scientific community can freely use it to focus on image analysis rather than on application programming. The concept of software extensibility by adding plugins is also useful for developers and end users. Furthermore, this concept has been adopted by more recently evolved platforms such as Fiji [66] and Icy [67, 68]. The success story of ImageJ is continuing as the next-generation ImageJ2 software is currently under development. The 2 main challenges in image analysis in systems biology are the analysis of complex high-level structures such as whole organisms and the rise of experiments with ever increasing throughput. Imagery of large-scale biological systems such as embryos and brains requires state of the art algorithms for stitching, registration, and mapping to anatomical atlases. In addition to the extensible Vaa3D [45] and Fiji software packages, which are both established in this field, new tools such as TeraStitcher that can handle TiB-scale datasets have now emerged [8]. While the imaging of such high-level structures is typically conducted in a rather low throughput, partially automated workflows requiring a significant amount of user input are still quite common. In contrast, the amounts of images produced in high-throughput experiments are often increased by several orders of magnitude and cannot be manually analyzed. The challenge is to analyze data from HCS sets to a meaningful extent and in a reasonable amount of time. Several open-source packages for image analysis include functionality for machine learning-based cell classification. Some of these packages are CellProfiler [28, 29], CellClassifier [29], and the R package EBImage [43], which provide workflows for fixed cell images. CellProfiler can be used to address several application areas, including intensity and morphology measurements. In contrast to tools designed for fixed objects, CellProfiler can perform two-dimensional (2D) object tracking. Information about temporal coupling between cellular events is highly relevant for understanding the physiology of biological systems. Time-lapse imaging has emerged as a powerful tool for investigating dynamic cellular processes such as cell division or intracellular trafficking of labeled targets of interest. However, for the analysis of such high-throughput cinematography, only a few tools are currently available. CellCognition [26] is a freely available software platform that includes high-throughput batch processing and annotation of complex cellular dynamics such as the progression of single cells through distinct cell division states. In this platform, temporal hidden Markov modeling is used to reduce classification noise at state transitions and to distinguish different states with similar morphology. Briefly, CellCognition provides an analysis platform for live imaging-based screening with assays that directly score cellular dynamics [26]. BioImageXD [30], which is written in Python and C++, is leveraging the libraries VTK and ITK. As a result, BioImageXD, unlike CellProfiler and CellCognition, can offer options for 2D and 3D analyses by providing advanced batch-processing functions for multidimensional fluorescence image sets, including time series. In addition to built-in tools for visualization, colocalization analysis, segmentation, and tracking, the graphical user interface of BioImageXD facilitates the assembly of custom image analysis pipelines. The open-source design of the project, as well as the use of Python and gold standard file formats such as OME-TIFF, should further facilitate the evolution of this project for the community working on spatio-temporally resolved data.

112
Download free eBooks at bookboon.com

Management of large sets of image data

Images in different disciplines

An open-source software can foster productive collaborations between programming biologists and computer scientists interested in biology. However, an important challenge is to ensure the availability of analysis tools to the entire community of microscope users. The timely public availability of professionally programmed, easy-to-use, open-source tools for image analysis will depend on career opportunities for talented image analysis code writers, and the quality of these emerging tools will depend on good programming practices. Recently, Carpenter et al. [10] described usability criteria for image analysis software and advocated for usability as a more highly valued goal in broad-impact image analysis research. The authors emphasized that image analysis software should be user-friendly, modular, developer friendly, validated, and interoperable. Typically, the development of usable open-source software requires close collaborations between users and programmers, such that the resulting software does not suffer from the lack of software engineering expertise or real world applicability. An outstanding example of an opensource image informatics platform with very good usability is the most recently developed generalist image analysis platform Icy. The main aim of this platform is to be developer friendly and facilitate timely and efficient collaborations as well as reproducible research. The software is built on Java but can also be used with the originally C++-based VTK and ITK libraries for native 3D visualization. The modern and well-organized user interface provides access to state-of-the-art image analysis tools and μManager-based [19] microscope control for live acquisitions with feedback. Furthermore, the writing of complete protocols is facilitated by a so-called workflow design tool, which represents individual processes as graphical blocks, and does not require any Java programming knowledge.

113
Download free eBooks at bookboon.com

Click on the ad to read more

Management of large sets of image data

Images in different disciplines

The creativity of researchers asking unprecedented scientific questions will continue to present challenges in image analysis that cannot be solved with a single software tool. Due to the common use of a variety of different software tools to acquire and analyze data, the connectivity and interoperability between these tools are crucial. Fortunately, many developers already understand this, and the most successful open-source image analysis platforms are explicitly developing ways to share data and code. Finally, image analysis, with extraction of desired features, is needed but will not be sufficient for making biologically relevant conclusions. The extracted image-based features need to undergo further high-level data analysis. In turn, the analysis of extracted features and identification of relevant features can greatly improve with machine learning.

15.2	

Medical imaging

Brain imaging techniques allow doctors and researchers to view activity or problems within the human brain, without invasive neurosurgery. There are a number of accepted, safe imaging techniques in use today in research facilities and hospitals throughout the world. fMRI Functional magnetic resonance imaging, or fMRI, is a technique for measuring brain activity. It works by detecting the changes in blood oxygenation and flow that occur in response to neural activity – when a brain area is more active it consumes more oxygen and to meet this increased demand blood flow increases to the active area. fMRI can be used to produce activation maps showing which parts of the brain are involved in a particular mental process. CT Computed tomography (CT) scanning builds up a picture of the brain based on the differential absorption of X-rays. During a CT scan the subject lies on a table that slides in and out of a hollow, cylindrical apparatus. An x-ray source rides on a ring around the inside of the tube, with its beam aimed at the subjects head. After passing through the head, the beam is sampled by one of the many detectors that line the machine’s circumference. Images made using x-rays depend on the absorption of the beam by the tissue it passes through. Bone and hard tissue absorb x-rays well, air and water absorb very little and soft tissue is somewhere in between. Thus, CT scans reveal the gross features of the brain but do not resolve its structure well. PET Positron Emission Tomography (PET) uses trace amounts of short-lived radioactive material to map functional processes in the brain. When the material undergoes radioactive decay a positron is emitted, which can be picked up be the detector. Areas of high radioactivity are associated with brain activity.

114
Download free eBooks at bookboon.com

Management of large sets of image data

Images in different disciplines

EEG Electroencephalography (EEG) is the measurement of the electrical activity of the brain by recording from electrodes placed on the scalp. The resulting traces are known as an electroencephalogram (EEG) and represent an electrical signal from a large number of neurons. EEGs are frequently used in experimentation because the process is non-invasive to the research subject. The EEG is capable of detecting changes in electrical activity in the brain on a millisecond-level. It is one of the few techniques available that has such high temporal resolution. MEG Magnetoencephalography (MEG) is an imaging technique used to measure the magnetic fields produced by electrical activity in the brain via extremely sensitive devices known as SQUIDs. These measurements are commonly used in both research and clinical settings. There are many uses for the MEG, including assisting surgeons in localizing a pathology, assisting researchers in determining the function of various parts of the brain, neurofeedback, and others. NIRS Near infrared spectroscopy is an optical technique for measuring blood oxygenation in the brain. It works by shining light in the near infrared part of the spectrum (700–900nm) through the skull and detecting how much the remerging light is attenuated. How much the light is attenuated depends on blood oxygenation and thus NIRS can provide an indirect measure of brain activity. DICOM Digital Imaging and Communications in Medicine (DICOM) is a standard for handling, storing, printing, and transmitting information in medical imaging. It includes a file format definition and a network communications protocol. The communication protocol is an application protocol that uses TCP/IP to communicate between systems. DICOM files can be exchanged between two entities that are capable of receiving image and patient data in DICOM format. The National Electrical Manufacturers Association (NEMA) holds the copyright to this standard. It was developed by the DICOM Standards Committee, whose members are also partly members of NEMA. DICOM enables the integration of scanners, servers, workstations, printers, and network hardware from multiple manufacturers into a picture archiving and communication system (PACS). The different devices come with DICOM conformance statements which clearly state which DICOM classes they support. DICOM has been widely adopted by hospitals and is making inroads in smaller applications like dentists’ and doctors’ offices.

115
Download free eBooks at bookboon.com

Management of large sets of image data

Images in different disciplines

A picture archiving and communication system (PACS) is a medical imaging technology which provides economical storage of and convenient access to, images from multiple modalities (source machine types). Electronic images and reports are transmitted digitally via PACS; this eliminates the need to manually file, retrieve, or transport film jackets. The universal format for PACS image storage and transfer is DICOM. Non-image data, such as scanned documents, may be incorporated using consumer industry standard formats like PDF (Portable Document Format), once encapsulated in DICOM. A PACS consists of four major components: The imaging modalities such as X-ray plain film (PF), computed tomography (CT) and magnetic resonance imaging (MRI), a secured network for the transmission of patient information, workstations for interpreting and reviewing images, and archives for the storage and retrieval of images and reports. Combined with available and emerging web technology, PACS has the ability to deliver timely and efficient access to images, interpretations, and related data. PACS breaks down the physical and time barriers associated with traditional film-based image retrieval, distribution, and display [47].

26 destinations 4 continents
Bartending is your ticket to the world

GET STARTED

116
Download free eBooks at bookboon.com

Click on the ad to read more

Management of large sets of image data

Images in different disciplines

PACS has four main uses: 1.	 Hard copy replacement: PACS replaces hard-copy based means of managing medical images, such as film archives. With the decreasing price of digital storage, PACSs provide a growing cost and space advantage over film archives in addition to the instant access to prior images at the same institution. Digital copies are referred to as Soft-copy. 2.	 Remote access: It expands on the possibilities of conventional systems by providing capabilities of off-site viewing and reporting (distance education, telediagnosis). It enables practitioners in different physical locations to access the same information simultaneously for teleradiology. 3.	 Electronic image integration platform: PACS provides the electronic platform for radiology images interfacing with other medical automation systems such as Hospital Information System (HIS), Electronic Medical Record (EMR), Practice Management Software, and Radiology Information System (RIS). 4.	 Radiology Workflow Management: PACS is used by radiology personnel to manage the workflow of patient exams.

15.3	

Astronomical images

Images of astronomical objects are usually taken with electronic detectors such as a CCD (Charge Coupled Device). Similar detectors are found in normal digital cameras. Telescope images are nearly always greyscale, but nevertheless contain some colour information. An astronomical image may be taken through a colour filter. Different detectors and telescopes also usually have different sensitivities to different colours (wavelengths). Filters Filters can either be broad-band (Wide) or narrow-band (Narrow). A broad-band filter lets a wide range of colours through, for instance the entire green or red area of the spectrum. A narrow-band filter typically only lets a small wavelength span through, thus effectively restricting the transmitted radiation to that coming from a given atomic transition, allowing astronomers to investigate individual atomic processes in the object. A filename such as 502nmos .fits indicates that the filter used has a peak at 502 nm. In the table below, you can see that this filter is a narrow bandwidth filter, i.e. it only lets radiation with wavelengths within a few nm of 502 nm through. Galaxies are often studied through broad-band filters as they allow more light to get through. Also the processes in a galaxy are more ‘mixed’ or complicated, result from the outputs of billions of stars and so narrow-band filters give less ‘specific’ information about the processes there.

117
Download free eBooks at bookboon.com

Management of large sets of image data

Images in different disciplines

15.4	

Industrial imaging

Automated optical inspection (AOI) is an automated visual inspection of a wide range of products, such as printed circuit boards (PCBs), LCDs, transistors, automotive parts, lids and labels on product packages or agricultural products (seed corn or fruits). In case of PCB-inspection, a camera autonomously scans the device under test (DUT) for variety of surface feature defects such as scratches and stains, open circuits, short circuits, thinning of the solder as well as missing components, incorrect components, and incorrectly placed components. Agricultural inspections might check for variations in part color, perhaps to find ripe fruit. AOI is a type of white box testing. It is commonly used in the manufacturing process because it is a non-contact test method. AOI is able to perform most of the visual checks performed previously by manual operators, and far more swiftly and accurately. AOI systems are implemented at many stages through the manufacturing process. They are used for inspecting parts that have limited and known variations. For defect or flaw detection, the AOI system looks for differences from a perfect part. There are systems capable of bare board inspection, solder paste inspection (SPI), as well as inspecting the component placement prior to reflow, the post-reflow component conditions, and post-reflow solder joints. These inspection devices all have some common attributes that affect capability, accuracy, and reliability. Low costs and programming efforts make AOI a practical and powerful quality tool for both prototypes and high-volume assembles. It is often paired with the testing provided by boundary scan test, in-circuit test, x-ray test, and functional test. In many cases, smaller circuit board designs are driving up the demand for AOI versus in-circuit test. A machine vision or an AOI system can acquire millions of data points (pixels) in a fraction of a second. These data points are used for visual inspection and precision measurement. AOI visually scans the surface of the PCB. The board is lit by several light sources and observed by a scanner or by a number of high definition cameras. This enables the monitoring of all areas of the board, even those hidden in one direction by other components. It should be noted that each manufacturer of AOI systems utilizes different inspection algorithms and lighting techniques, each of these systems may have varying strengths and weaknesses depending upon the item/product it is inspecting. If a scanner is used it has to scan the surface of the PCB from above only once. If image cameras are used, one must first determine the number of cameras needed. There are systems with only one camera which scans the DUT from above and systems with a couple of cameras from all sides. To be able to scan the device from all points of view, the cameras should be able to move in both X- and Y-direction controlled by software. To program this software the test engineer must have the CAD data.

118
Download free eBooks at bookboon.com

Management of large sets of image data

Images in different disciplines

Then the type of cameras must be chosen. There are several types in use today. Streaming video 2D frame grabbers are common. They utilize a motion capture video camera that extracts one frame from a streaming video and creates a still image. However, the system sacrifices image quality for speed and efficiency. A second type of camera imaging system is the Line Scan Still Image Camera. In this system, a still camera is placed relatively close to the target. Because of this, this system requires a very good lighting system. Unfortunately, the image can be distorted by subsystem imperfections such as transporter movement. This makes obtaining precise positioning and measurements difficult when compared to other types of systems. A benefit of the Line Scan Still Image Camera is the image acquisition speed, which is faster than a CCD camera. Another camera imaging system is the 2D Charge-coupled device or CCD. The CCD is used for highend and special applications such as space and military technologies. This system creates high precision still images in color that are more accurate than other systems. Each industry is different in how image acquisition signals are transferred to the camera. It can be hardware driven via a mechanical signal such as a proximity sensor, laser interruption, drive system encoder position, or software. Regardless of the signal the AOI system interprets the signal which triggers the vision assembly (which could be a single frame grabber and camera combination or more advanced as already discussed) that the object is in a known location and to begin image acquisition. The vision computer then triggers the camera/cameras to simultaneously acquire images of the device [4].

.

119
Download free eBooks at bookboon.com

Click on the ad to read more

Management of large sets of image data

Selection of best digital images

16	 Selection of best digital images

Figure 15. This confocal micrograph shows the tissue structures within the leaf of an Arabidopsis thaliana seedling, the model organism in plant genetics (image source [49]).

120
Download free eBooks at bookboon.com

Management of large sets of image data

Selection of best digital images

Figure 16. Astronomy – top 1. The Very Large Telescope Snaps a Stellar Nursery and Celebrates Fifteen Years of Operations (image source [21]).

121
Download free eBooks at bookboon.com

Management of large sets of image data

Selection of best digital images

Figure 17. Antennae Galaxies reloaded (image source [55])

122
Download free eBooks at bookboon.com

Management of large sets of image data

Selection of best digital images

Figure 18. Best Microscope Photos of the Year Bring Tiny, Amazing Worlds to Light: Chaetoceros debilis (marine diatom), a colonial plankton organism (image source [58]).

Think Umeå. Get a Master’s degree!
• modern campus • world class research • 31 000 students • top class teachers • ranked nr 1 by international students Master’s programmes: • Architecture • Industrial Design • Science • Engineering

Sweden www.teknat.umu.se/english

123
Download free eBooks at bookboon.com

Click on the ad to read more

Management of large sets of image data

References

References:
1.	 1000 Memories: http://blog.1000memories.com 2.	 Ankers JM, Spiller DG, White MR, Harper CV: Spatio-temporal protein dynamics in single living cells. Curr Opin Biotechnol 2008, 19:375–80. 3.	 Antony PM1, Trefois C, Stojanovic A, Baumuratov AS, Kozak K. Light microscopy applications in systems biology: opportunities and challenges. Cell Commun Signal. 2013 Apr 11;11(1):24. 4.	 “Automated optical inspection.” Wikipedia: The Free Encyclopedia. Wikimedia Foundation, Inc., 7 Nov. 2012. Web. 25 Nov. 2012. <http://en.wikipedia.org/wiki/Automated_optical_inspection> 5.	 Berthold MR, Cebron N, Dill F, Gabriel TR, Kötter T, Meinl T, Ohl P, Thiel K, Wiswedel B: KNIME – the Konstanz information miner. ACM SIGKDD Explorations Newsletter 2009, 11:26. 6.	 BH Photo Video: http://www.bhphotovideo.com/indepth 7.	 BioImage: http://www.bioimage.ucsb.edu/bisque 8.	 Bria A, Iannello G: TeraStitcher – A Tool for Fast Automatic 3D-Stitching of Teravoxel-Sized Microscopy Images. BMC Bioinformatics 2012, 13:316. 9.	 Buchser W: Assay Development Guidelines for Image-Based High Content Screening, High Content Analysis and High Content Imaging. In Assay Guidance Manual. Bethesda, MD: National Center for Advancing Translational Sciences (NCATS), 2012:1–69. 10.	 Carpenter AE, Kamentsky L, Eliceiri KW: A call for bioimaging software usability. Nat Methods 2012, 9:666–70. 11.	 Chao JA, Yoon YJ, Singer RH: Imaging Translation in Single Cells Using Fluorescent Microscopy. Cold Spring Harb Perspect Biol 2012, 4:1–12. 12.	 “Database.” Wikipedia: The Free Encyclopedia. Wikimedia Foundation, Inc., 7 Nov. 2012. Web. 25 Nov. 2012. <http://en.wikipedia.org/wiki/Database> 13.	 Davies ER: Computer and Machine Vision: Theory, Algorithms, Practicalities. Academic Press, New York, NY; 2012:1–912. 14.	 “Digital camera.” Wikipedia: The Free Encyclopedia. Wikimedia Foundation, Inc., 7 Nov. 2012. Web. 25 Nov. 2012. <http://en.wikipedia.org/wiki/Digital_camera> 15.	 “Digital image.” Wikipedia: The Free Encyclopedia. Wikimedia Foundation, Inc., 7 Nov. 2012. Web. 25 Nov. 2012. <http://en.wikipedia.org/wiki/Digital_image> 16.	 “Digital image processing.” Wikipedia: The Free Encyclopedia. Wikimedia Foundation, Inc., 7 Nov. 2012. Web. 25 Nov. 2012.

124
Download free eBooks at bookboon.com

Management of large sets of image data

References

< http://en.wikipedia.org/wiki/Digital_image_processing> 17.	 Digital Sir Guide: http://www.digital-slr-guide.com 18.	 Du W, Wang Y, Luo Q, Liu B-F: Optical molecular imaging for systems biology: from molecule to organism. Anal Bioanal Chem 2006, 386:444–57. 19.	 Edelstein A, Amodaj N, Hoover K, Vale R, Stuurman N, et al: Computer control of microscopes using μManager. In Current protocols in molecular biology. Edited by Ausubel FM.; 2010. Chapter 14:Unit14.20. 20.	 Eliceiri KW, Berthold MR, Goldberg IG, Ibáñez L, Manjunath BS, Martone ME, Murphy RF, Peng H, Plant AL, Roysam B, Stuurmann N, Swedlow JR, Tomancak P, Carpenter AE: Biological imaging software tools. Nat Methods 2012, 9:697–710. 21.	 ESO: http://www.eso.org/public/ 22.	 Flusberg BA, Nimmerjahn A, Cocker ED, Mukamel EA, Barretto RPJ, Ko TH, Burns LD, Jung JC, Schnitzer MJ: High-speed, miniaturized fluorescence microscopy in freely moving mice. Nat Methods 2008, 5:935–938. 23.	 Fuchs F, Pau G, Kranz D, Sklyar O, Budjan C, Steinbrink S, Horn T, Pedal A, Huber W, Boutros M: Clustering phenotype populations by genome-wide RNAi and multiparametric imaging. Mol Syst Biol 2010, 6:370. 24.	 Ghosh KK, Burns LD, Cocker ED, Nimmerjahn A, Ziv Y, Gamal AE, Schnitzer MJ: Miniaturized integration of a fluorescence microscope. Nat Methods 2011, 8:871–8. 25.	 “Graphics processing unit.” Wikipedia: The Free Encyclopedia. Wikimedia Foundation, Inc., 7 Nov. 2012. Web. 25 Nov. 2011 <http://en.wikipedia.org/wiki/Graphics_processing_unit> 26.	 Held M, Schmitz MHA, Fischer B, Walter T, Neumann B, Olma MH, Peter M, Ellenberg J, Gerlich DW: Cell Cognition: time-resolved phenotype annotation in high-throughput live cell imaging. Nat Methods 2010, 7:747–54. 27.	 Horvath P, Wild T, Kutay U, Csucs G: Machine Learning Improves the Precision and Robustness of High-Content Screens: Using Nonlinear Multiparametric Methods to Analyze Screening Results. J Biomol Screen 2011, 16:1059–1067. 28.	 Jones TR, Carpenter AE, Lamprecht MR, Moffat J, Silver SJ, Grenier JK, Castoreno AB, Eggert US, Root DE, Golland P, Sabatini DM: Scoring diverse cellular morphologies in image-based screens with iterative feedback and machine learning. Proc Natl Acad Sci U S A 2009, 106:1826–31. 29.	 Jones TR, Kang IH, Wheeler DB, Lindquist RA, Papallo A, Sabatini DM, Golland P, Carpenter AE: CellProfiler Analyst: data exploration and analysis software for complex image-based screens. BMC bioinformatics 2008, 9:482. 30.	 Kankaanpää P, Paavolainen L, Tiitta S, Karjalainen M, Päivärinne J, Nieminen J, Marjomäki V, Heino J, White DJ: BioImageXD: an open, general-purpose and high-throughput imageprocessing platform. Nat Methods 2012, 9:683–9.

125
Download free eBooks at bookboon.com

Management of large sets of image data

References

31.	 Kvilekval K, Fedorov D, Obara B, Singh A, Manjunath BS: B: A platform for bioimage analysis and management. Bioinformatics (Oxford, England) 2010, 26:544–52. 32.	 “List of photo-sharing websites.” Wikipedia: The Free Encyclopedia. Wikimedia Foundation, Inc., 7 Nov. 2012. Web. 25 Nov. 2012. <http://en.wikipedia.org/wiki/List_of_photo-sharing_websites> 33.	 Lubeck E, Cai L: Single-cell systems biology by super-resolution imaging and combinatorial labeling. Nat Methods 2012, 9:743–8. 34.	 Long F, Peng H, Liu X, Kim SK, Myers E: A 3D digital atlas of C. elegans and its application to single-cell analyses. Nat Methods 2009, 6:667–72. 35.	 “Median Filter.” Wikipedia: The Free Encyclopedia. Wikimedia Foundation, Inc., 7 Nov. 2012. Web. 25 Nov. 2012. < http://en.wikipedia.org/wiki/Median_filter > 36.	 Metadata Working Group: http://www.metadataworkinggroup.com 37.	 Mosaliganti KR, Noche RR, Xiong F, Swinburne IA, Megason SG: ACME: Automated Cell Morphology Extractor for Comprehensive Reconstruction of Cell Membranes. PLoS Comput Biol 2012, 8:e1002780. 38.	 Myers G: Why bioimage informatics matters. Nat Methods 2012, 9:659–60. 39.	 Nature Focused: http://www.naturefocused.com/articles/image-protection.html 40.	 Nauru: http:// www.nauruit.com 41.	 Open Microscopy: www.openmicroscopy.org

How could you take your studies to new heights?
By thinking about things that nobody has ever thought about before By writing a dissertation about the highest building on earth With an internship about natural hazards at popular tourist destinations By discussing with doctors, engineers and seismologists By all of the above

From climate change to space travel – as one of the leading reinsurers, we examine risks of all kinds and insure against them. Learn with us how you can drive projects of global significance forwards. Profit from the know-how and network of our staff. Lay the foundation stone for your professional career, while still at university. Find out how you can get involved at Munich Re as a student at munichre.com/career.

126
Download free eBooks at bookboon.com

Click on the ad to read more

Management of large sets of image data

References

42.	 Okuno D, Iino R, Noji H: Rotation and structure of FoF1-ATP synthase. J Biochem 2011, 149:655–64. 43.	 Pau G, Fuchs F, Sklyar O, Boutros M, Huber W: EBImage – an R package for image processing with applications to cellular phenotypes. Bioinformatics 2010, 26:979–981. 44.	 Peng H: Bioimage informatics: a new area of engineering biology. Bioinformatics 2008, 24:1827–1836. 45.	 Peng H, Ruan Z, Long F, Simpson JH, Myers EW: V3D enables real-time 3D visualization and quantitative analysis of large-scale biological image data sets. Nat Biotechnol 2010, 28:348–53. 46.	 Peter Jsucy: http://www.peterjsucy.com/DigHist.htm 47.	 “Picture archiving and communication system.” Wikipedia: The Free Encyclopedia. Wikimedia Foundation, Inc., 7 Nov. 2012. Web. 25 Nov. 2012. <http://en.wikipedia.org/wiki/Picture_archiving_and_communication_system> 48.	 Photo Review: http://www.photoreview.com.au 49.	 Popsci: http://www.popsci.com 50.	 Ronneberger O, Liu K, Rath M, Rueβ D, Mueller T, Skibbe H, Drayer B, Schmidt T, Filippi A, Nitschke R, Brox T, Burkhardt H, Driever W: ViBE-Z: a framework for 3D virtual colocalization analysis in zebrafish larval brains. Nat Methods 2012, 9:735–42. 51.	 Savidge TC, Newman P, Pothoulakis C, Ruhl A, Neunlist M, Bourreille A, Hurst R, Sofroniew MV: Enteric glia regulate intestinal barrier function and inflammation via release of S-nitrosoglutathione. Gastroenterology 2007, 132:1344–58. 52.	 Schmid B, Schindelin J, Cardona A, Longair M, Heisenberg M: A high-level 3D visualization API for Java and ImageJ. BMC Bioinformatics 2010, 11:274. 53.	 Sengupta P, Van Engelenburg S, Lippincott-Schwartz J: Visualizing cell structure and function with point-localization superresolution imaging. Dev Cell 2012, 23:1092–102. 54.	 Shen F, Hodgson L, Hahn K: Digital autofocus methods for automated microscopy. Methods Enzymol 2006, 414:620–32. 55.	 Spacetelescope ESA/Hubble & NASA: http://www.spacetelescope.org 56.	 Swedlow JR, Eliceiri KW: Open source bioimage informatics for cell biology. Trends Cell Biol 2009, 19:656–60. 57.	 Techlore: http://www.techlore.com 58.	 Tychyonemc2. Micropolitan Museum, Berkel en Rodenrijs, Zuid Holland, The Netherlands: http://tachyonemc2.wordpress.com 59.	 Ueno H, Nishikawa S, Iino R, Tabata KV, Sakakihara S, Yanagida T, Noji H: Simple darkfield microscopy with nanometer spatial precision and microsecond temporal resolution. Biophys J 2010, 98:2014–23. 60.	 Yasuda R, Noji H, Kinosita K, Yoshida M: F1-ATPase is a highly efficient molecular motor that rotates with discrete 120 degree steps. Cell 1998, 93:1117–24.

127
Download free eBooks at bookboon.com

Management of large sets of image data

References

61.	 Walter T, Shattuck DW, Baldock R, Bastin ME, Carpenter AE, Duce S, Ellenberg J, Fraser A, Hamilton N, Pieper S, Ragan MA, Schneider JE, Tomancak P, Hériché J-K: Visualization of image data from cells to organisms. Nat methods 2010, 7:S26–41. 62.	 Watmuff B, Pouton CW, Haynes JM: In vitro maturation of dopaminergic neurons derived from mouse embryonic stem cells: implications for transplantation. PLoS One 2012, 7:e31999. 63.	 Wählby C, Kamentsky L, Liu ZH, Riklin-Raviv T, Conery AL, O’Rourke EJ, Sokolnicki KL, Visvikis O, Ljosa V, Irazoqui JE, Golland P, Ruvkun G, Ausubel FM, Carpenter AE: An image analysis toolbox for high-throughput C. elegans assays. Nat methods 2012, 9:714–6. 64.	 Wikipedia: http://en.wikipedia.org/wiki 65.	 Zwolinski L, Kozak M, Kozak K: 1Click1View: Interactive Visualization Methodology for RNAi Cell-Based Microscopic Screening. BioMed Res Int 2013, 2013:1–11. 66.	 Schindelin J, Arganda-Carreras I, Frise E, Kaynig V, Longair M, Pietzsch T, Preibisch S, Rueden C, Saalfeld S, Schmid B, Tinevez J-Y, White DJ, Hartenstein V, Eliceiri K, Tomancak P, Cardona A: Fiji: an open-source platform for biological-image analysis. Nature methods 2012, 9:676–82. 67.	 De Chaumont F, Dallongeville S, Olivo-Marin J-C: ICY: A new open-source community image processing software. In 2011 IEEE International Symposium on Biomedical Imaging: From Nano to Macro. IEEE; 2011:234–237. 68.	 De Chaumont F, Dallongeville S, Chenouard N, Hervé N, Pop S, Provoost T, Meas-Yedid V, Pankajakshan P, Lecomte T, Le Montagner Y, Lagache T, Dufour A, Olivo-Marin J-C: Icy: an open bioimage informatics platform for extended reproducible research. Nature methods 2012, 9:690–6.

128
Download free eBooks at bookboon.com

